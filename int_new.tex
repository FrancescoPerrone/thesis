\chapter{Introduction}
\label{chap:introduction}
Think of moral decision-making as the full mental sequence we go through when we’re choosing between competing ideas of what the ‘right thing’ might be. It starts with what we notice: certain details stand out, others fade into the background. Those initial impressions shape what we care about, which in turn shapes what we treat as relevant. Only then does our reasoning step in to organise all of that into a sense of, ‘This is what I should do.’ In a way, it’s the process that turns a handful of moral impressions into a genuine commitment to act.

And most of the time, this isn’t a slow, deliberate calculation. It’s closer to an immediate sense of something feeling right or wrong, which we then test against the situation and the social world around us. We respond to small cues—a shift in tone, a facial expression, the atmosphere of a room—and they quietly push us toward one reaction rather than another long before we begin to articulate reasons.

After that early, intuitive pull, we start to refine it. We call to mind similar situations. We notice details we missed at first glance. We talk it through, sometimes out loud, sometimes just internally. And we develop reasons that make sense of the direction we’re already leaning toward. The decision is still real, but it grows out of these quick, socially shaped impressions that guide us well before any careful reflection begins.

This is precisely why the idea of creating a ‘moral’ machine by embedding a single ethical theory—utilitarianism, deontology, or any other framework—is so misguided. Those theories are helpful tools for analysing moral arguments after they’ve happened, but they’re not the engines we rely on when we actually navigate a situation. They’re abstractions, not working models of human judgment.

Yet in the technology world, you still encounter the view that if you program a system to follow a specific theory, you’ve solved the moral problem. That assumption is, at best, overly optimistic. A machine following a tidy rulebook bears little resemblance to what humans do when we sense tension in a room, register someone’s discomfort, or feel the pull of how our actions will land with others. Real moral life is textured, social, emotional, and deeply dependent on context. There isn’t a clean set of instructions that captures all that.

So when somebody claims to have built an algorithm that ‘acts ethically,’ it often reflects an academic game of who can produce the most polished theoretical model rather than a meaningful engagement with how moral decisions actually work. 

The theory may look elegant on paper, but it doesn’t map onto the realities of human moral experience.

And this, is exactly the space where our work begins. We know that our moral reactions are shaped by tiny cues—someone’s expression, the tension in their posture, the energy in a room, even things as subtle as the smell of someone who’s had a long day. These details don’t just colour the moment; they steer our judgment before we’re even aware of it.

So the real question for us is this: what happens when the agent in front of you isn’t a person at all, but a humanoid robot? How do we respond when the timing of a gaze is algorithmic, and the emotional tone is produced by design rather than by experience?

We still react. We can’t help it. Our perceptual systems are tuned to pick up anything that looks or behaves like a person. But the meaning of those reactions becomes murkier. Are we responding to genuine social cues, or to clever mimicry? And if a robot can reliably trigger the same moral intuitions that another human does, what does that say about the foundations of our own judgments?

For us, that’s the critical challenge. Not whether a machine can follow a rulebook, but \textit{but how our deeply human, automatic moral instincts adapt—or fail to adapt—when something built rather than born is standing in front of us. And not just in a lab, but in our rooms, in our kitchens, woven into the background of daily life.}

\bigskip
\noindent
Moral decision-making is:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{The cognitive process through which agents select between competing moral judgments—mutually exclusive evaluations of what is right or wrong, good or bad—that provide the motive, direction, and justificatory structure of their practical behaviour. It is a composite operation: perceptual encoding, affective appraisal, memory, attentional orientation, and interpretive reasoning jointly determine how morally salient cues are registered, weighted, and transformed into a behavioural commitment.}
	\end{leftbar}
\end{center}
\bigskip
\noindent

The work we present here developes withing this framework, which we applied to a concrete and experimentally tractable setting within Human--Robot Interaction and Social Signal Processing. 

We conducted a study in which participants enter a small room and encounter a simple but meaningful moral choice: they may donate part of their participation payment to a real charity, or keep the full amount for themselves. This setting does not claim to capture moral cognition in its entirety; instead, it offers a minimal, controlled environment in which the elements of its definition become empirically observable. 
 

%\noindent
%Upon entering the room, participants first engage in \textbf{perceptual encoding}: they register the coins on the table, the bright charity charity, the instruction sheet, and the charity poster overhead depicting a child in need with big expressive eyes. These serve as the \emph{morally salient cues} structuring the situation. Almost immediately, \textbf{affective appraisal} is recruited: the charitable context evokes a mild empathic pull; the Watching-Eye cue introduces a sense of being implicitly observed by the child; and the act of giving up one’s own money elicits a familiar tension between prosocial motivation and self-interest. Alongside this, \textbf{memory and normative expectations} quietly shape interpretation—past experiences with giving, internalised cultural norms about generosity, and associations between being watched and acting prosocially.


\noindent
Upon entering the room, participants first engage in \textbf{perceptual encoding}: they register the coins on the table, the charity materials, and the child-poster overhead with its large, expressive eyes. These elements constitute the \emph{morally salient cues} structuring the situation, consistent with work showing that minimal observational cues and child-like eyes heighten perceived social relevance and implicit monitoring \cite{Haley2005, Bateson2006, ErnestJones2011, Nettle2013, Dear2019, Conty2016}. 

Almost immediately, \textbf{affective appraisal} is recruited. The charitable context elicits a mild empathic pull in line with established findings on affective resonance and empathetic sensitivity \cite{BaronCohenWheelwright2004_EmpathyQuotient, Gleichgerrcht2013, Haidt2001EmotionalDog}. Simultaneously, the watching-eye cue introduces an implicit sense of being observed, activating reputational and attentional systems documented in observational-cue research \cite{Haley2005, Bateson2006, Dear2019}. The prospect of giving up one’s own money further evokes the familiar tension between prosocial motivation and self-interest captured in dual-process and motivational models of moral decision-making \cite{Greene2004, Cushman2013DualSystem, Crockett2016Models}.

Alongside these immediate appraisals, \textbf{memory and normative expectations} shape interpretation: past experiences with charitable giving, internalised cultural norms of generosity, and well-established associations between being watched and acting prosocially influence how the evaluative field is instantiated in the moment \cite{Nettle2013, Bateson2006, Haidt2001EmotionalDog}.

\noindent
At the same time, \textbf{attentional orientation} determines which elements dominate the evaluative landscape: is the participant more attuned to the need expressed by the charity? to the coins that could be kept? 

\noindent
To describe moral decision-making in this sense is to recognise its fundamentally 
\emph{teleological} character, a view rooted in classical action-centred accounts of 
ethics \cite{Aristotle_nicomachean,Foot2001,Korsgaard1996}.  
Moral cognition unfolds toward action: it organises the evaluative conditions under 
which an agent adopts one course rather than another, consistent with empirical models 
linking appraisal to action selection \cite{Haidt2001,Greene2004,Cushman2013}.  
The transition from moral judgment to behaviour is not an optional addendum to the 
process—it is its natural terminus.  
A moral evaluation that does not shape the field of possible actions has not yet 
completed its function; a moral action, conversely, is the crystallised endpoint of 
evaluative dynamics that have been unfolding long before reflection makes them explicit 
\cite{Decety2004,Moll2002,Haidt2001}.

\medskip
\noindent
The participant’s eventual choice to donate or not is the behavioural crystallisation of this entire evaluative process. This thesis examines how the silent co-presence of a humanoid robot modulates that transformation. The robot does not request, instruct, or communicate, yet its ambiguous social ontology—perceptually agentic, normatively indeterminate—reshapes the conditions under which moral judgments are formed and resolved. In this way, the experiment offers a precise instantiation of the definition of moral decision-making introduced above: a setting in which perceptual cues, affective resonance, attentional dynamics, and implicit social meaning combine to produce a practical moral commitment, and in which that process can be systematically perturbed.

\medskip
\noindent
Moral cognition thus operates within a social environment dense with cues—gaze, posture, interpersonal distance, implicit accountability signals—that modulate the affective and attentional components of evaluation. These modulations occur upstream of explicit reasoning: they determine \emph{what becomes salient} well before agents deliberate on what \emph{ought} to be done.

\medskip
\noindent
The introduction of synthetic agents into this environment raises a conceptual and empirical challenge. Humanoid robots occupy a liminal ontological space: perceptually social yet not persons, agent-shaped yet not agents. Their presence recruits perceptual and affective systems that evolved for human–human interaction, while simultaneously withholding the ordinary resources through which social meaning stabilises. This thesis examines the possibility that \emph{such entities reshape the evaluative conditions of moral cognition not by acting, but simply by being present}.

\bigskip
\noindent
One may picture the problem in concrete terms of our example above.Imagine the participant in the experimental room. On a table: the charity box, a few pound coins, and a simple instruction inviting a donation. The child in need, with big expressive eyes—an established prime of perceived accountability—looks down from a poster.
Alone, the participant might experience a mild empathic pull, a subtle sense of being expected to act prosocially.

Now place a NAO robot on the same table.
It does nothing. It does not speak, gesture, or request. Yet its humanoid shape, its forward posture, its apparent capacity for attention, reframes the scene. The participant hesitates: the social field has changed. Something in the evaluative machinery has shifted—an attenuation of empathic pull, a dilution of accountability, a re-weighting of salience.

We started by looking at something very simple: what happens when a humanoid robot is present in the room while someone is making a moral decision. The robot doesn’t talk, it doesn’t give instructions, it doesn’t ask for anything. It just shares the space—quietly, almost like another person waiting their turn.

But that quiet presence turns out to matter. A robot like that sits in an odd position: it looks and moves in ways that make us treat it as an agent, yet we don’t quite know what kind of ‘being’ it is or what norms apply to it. That ambiguity changes the atmosphere. It shifts how people interpret the situation, what they take to be appropriate, and how comfortable they feel committing to one judgment over another.

So even without speaking, the robot reshapes the background against which moral choices are made. It nudges the whole process—not by argument or instruction, but simply by being there, hovering between the familiar category of a person and the familiar category of a machine. That’s where we see the transformation beginning.

This modest behavioural moment is the phenomenon under investigation.What has changed? And why?

\noindent
The central question that follows from this observation frames the entire research
programme:

\bigskip
\noindent
\begin{center}
\begin{leftbar}
	\textit{Can the mere presence of a synthetic, non-agentic entity perturb the inferential 
		transformation through which morally salient cues are converted into observable moral 
		behaviour?}
\end{leftbar}
\end{center}

\bigskip
\noindent
This question is motivated by the theoretical claim that synthetic agents may function
as \emph{operators on the evaluative field} in which moral decisions are formed.
If their perceptual salience or ambiguous social ontology alters the distribution of
attention, empathy, or accountability, then the evaluative trajectory that links
perception to action may shift accordingly.
In such a case, moral behaviour would not be changed by explicit influence but by
modulation of the cognitive–affective machinery upstream of conscious judgment.

In that case, moral behaviour wouldn’t be shifting because the robot told anyone what to do. It would be shifting because the upstream machinery—the mix of perception, emotion, and expectation that feeds into conscious judgment—has been quietly modulated. The influence is silent, indirect, and deeply embedded in the way we make sense of the world. That’s why this moment, small as it looks, matters.

\section{From Research Question to Hypotheses: Framing the Investigative Architecture}

Our question comes from a broader theoretical idea: that synthetic agents might operate on the moral landscape in which our decisions take shape. Not by persuasion, not by argument, but by subtly altering the conditions under which those judgments form. If a robot’s visual presence, or the uncertainty about what kind of ‘being’ it is, changes where people direct their attention, or how much empathy they feel, or who they think is accountable, then the whole path from perception to action can start to bend.

\medskip
\noindent
If the simple presence of a synthetic agent shifts that chain of inferences,\noindent
then the traditional approach in machine ethics—starting with abstract principles and trying to code them directly into a system \cite{Anderson2011,Bringsjord2006,Arkin2009,Wallach2008,Guarini2006}—can’t explain what’s going on. Those models operate at the reflective level, the level where we articulate reasons and moral rules. But the effects we’re observing happen earlier, in the pre-reflective machinery that sets the stage for those reasons.

So we need a different way of thinking about moral behaviour. A framework that treats it as the outcome of a field shaped by attention, emotion, and the way certain cues stand out or fade away. In that view, moral action isn’t just a conclusion drawn from a principle; it’s the end point of a landscape structured by what feels salient, what draws concern, and what seems to matter in the moment. That’s the level at which synthetic presence exerts its influence—and the level we have to model if we want to understand it.

\noindent
One way to make sense of this is by borrowing a notion from Luciano Floridi: the Level of Abstraction \cite{Floridi2008,Floridi2011}. It’s a simple idea with a lot of power behind it. Whenever we study a system—whether it’s a computer, a person, a society—\textit{we have to decide the level at which we’re describing it}. Are we talking about the underlying code? The behaviour? The motivations? The social context? Each level reveals some things and hides others.

\noindent
Most classical work in machine ethics starts at a very high, reflective level of abstraction. It focuses on principles—rules about what the system should or shouldn’t do—and tries to formalise those rules so they can be implemented~\cite{Allen2005,Arkoudas2005,Winfield2019,Anderson2010}. That’s useful if your goal is to build a system that behaves consistently with a particular ethical theory. But it tells you almost nothing about what happens at the cognitive level, where perception and emotion begin shaping the decision long before anyone appeals to a principle.


Our work sits at a different level of abstraction. We’re looking at the machinery that turns raw perception into a sense of what matters, and then into action. At that level, the presence of a humanoid robot isn’t a question about the robot’s rights or intentions; it’s a question about how its appearance and behaviour reshape the informational landscape the human is navigating.

\medskip
\noindent
Once we fix the Level of Abstraction—the cognitive level where perception, concern, and action are linked—we can be precise about what we’re testing. The thesis proposes three hypotheses, each tied to a different kind of perturbation at that level. They’re not rivals. They’re three structurally distinct ways in which the presence of a synthetic agent might reshape the evaluative process itself. Each one captures a different mechanism through which the perceptual and affective landscape can shift before conscious judgment begins. The thesis therefore develops three hypotheses, each mapped onto a different kind of perturbation within the cognitive–affective system that generates moral judgment. They’re not competing explanations; each one isolates a distinct structural route through which the simple presence of a synthetic agent might influence the transformation from perception to action.

Taken together, these hypotheses define the theoretical space of the project. They mark out the possibilities that become visible once we commit to the correct Level of Abstraction—the level where shifts in salience, attention, and affect reorganise the evaluative field long before a person arrives at a conscious moral conclusion.

The first hypothesis says that the robot changes the function that maps what you perceive to how you evaluate it. 
\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 1: Evaluative Deformation]
		Synthetic presence alters the evaluative function 
		$f : \mathcal{X} \rightarrow \mathcal{A}$ by reshaping salience 
		gradients, affective weights, or attentional trajectories. 
		In this model, the robot acts as a \emph{field operator}: 
		its perceptual salience deforms the topology through which moral cues acquire 
		behavioural force.
	\end{tcolorbox}
\end{center}

\bigskip
\noindent
The mathematical notation—$f : \mathcal{X} \rightarrow \mathcal{A}$—just means:
given some input from the world, how do you turn it into a sense of what matters? What we test here is very simple: does having a humanoid robot in the room subtly shift what stands out to the subject, what feels important, or what pulls their attention?

If the robot is visually or socially salient—even without speaking—it might ‘bend’ the landscape you’re navigating. Think of it like a small gravitational field: it doesn’t tell you what to do, but it changes the shape of the space you’re moving through. This hypothesis asks:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{ does the robot’s presence deforms that evaluative landscape just enough to change how moral cues gain their force?}
	\end{leftbar}
\end{center}

\bigskip
\noindent
The second hypothesis is about how people interpret responsibility and expectations in the presence of a humanoid robot. Here the claim is not that the robot has moral status or intentions. It’s that its human-like appearance gives it certain practical effects in how people interpret the situation. 

\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 2: Synthetic Normativity of Moral Displacement]
		A humanoid robot acquires \emph{normative affordances} through its 
		ambiguous social ontology. 
		Without communicating or expressing intention, it may refract perceived 
		accountability relations, modifying how agents interpret morally salient cues 
		within the situation.
	\end{tcolorbox}
\end{center}

People may unconsciously treat it as if it participates in the moral scene, even though it hasn’t said or done anything. So this hypothesis asks:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Does the robot shift who people feel accountable to, or who they think is paying attention, or what they think ‘counts’ in that moment?}
	\end{leftbar}
\end{center}

\bigskip
\noindent
The robot’s ambiguous status—something between a person and a tool—may subtly redirect moral attention. It’s not giving orders; it’s reframing the situation just by being there.

The third hypothesis looks at what happens in the transition from noticing something morally important to actually doing something about it.

Humans don’t move straight from perception to action. There’s a whole middle layer: empathy, emotional resonance, a sense of alignment with others. This hypothesis asks whether the robot interferes with that middle layer.

Does its presence dampen empathy? Does it redirect attention? Does it change how strongly certain cues ‘tag’ the situation as requiring action?

\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 3: Synthetic Perturbation of Moral Inference]
		Synthetic presence interferes with the transition from moral salience to 
		prosocial action by modulating empathic resonance, affective tagging, or 
		attentional alignment. 
		This mechanism predicts differential perturbation across dispositional 
		ecologies, precisely as observed in the experimental results.
	\end{tcolorbox}
\end{center}

So this final hypothesis says:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{the robot doesn’t change the rule you apply—it changes the internal bridge that links your moral perception to your moral behaviour}
	\end{leftbar}
\end{center}

And importantly, this hypothesis predicts that people with different dispositions—different personalities, sensitivities, backgrounds—will be affected differently. That’s exactly what the experiments showed: the effect isn’t uniform; it varies depending on the person.

\medskip
\noindent
These hypotheses structure the theoretical and empirical work that follows. 
They operationalise the core research question---whether synthetic presence can 
perturb the inferential machinery that links moral perception to moral action---and 
provide the conceptual scaffolding through which the experiment in 
Chapter~\ref{chap:experimental_methods} is interpreted.

\noindent
Together, these three hypotheses outline the whole space in which synthetic presence might influence moral judgment. Each captures a different mechanism, and all of them operate at the cognitive level—the level where perception and affect set the stage for what we later call ‘a moral decision.’

\section{The Need for a New Theoretical Orientation}

All three hypotheses point to the same structural insight: the traditional tools of Machine Ethics operate at the wrong level of explanation. Classic Machine Ethics starts with high-level principles—rules, utilities, virtues—and then tries to engineer machines that follow them. That’s perfectly coherent if your aim is to design a system that behaves consistently with an ethical theory.

But that framework doesn’t touch the kind of phenomenon our research question is targeting. We’re asking whether the presence of a synthetic agent reshapes the process by which humans move from perception to moral action. And that process unfolds long before anyone appeals to principles or reasons.

In other words, the phenomenon we’re investigating doesn’t live at the reflective Level of Abstraction. It shows up upstream, in the cognitive–affective machinery that makes moral reasoning possible in the first place. When a humanoid robot is in the room, it can alter what draws attention, how empathy is allocated, and what feels socially significant. That isn’t a change in moral reasoning—it’s a change in the conditions under which moral reasoning forms.

And if that’s where the modulation happens, then a principle-first approach to moral AI can’t explain it. We cannot start with abstract theories and work downward. You have to start with the architecture of moral cognition and work upward. Moral behaviour isn’t just the outcome of applying a rule; it is the emergent trajectory of a system sculpted by perceptual salience, affective appraisal, and socially mediated cues—processes that moral psychology has shown to precede and shape explicit judgment \cite{Haidt2001,Greene2001,Decety2004,Prinz2007}. These evaluative dynamics are deeply sensitive to contextual modulation: shifts in attention, affective resonance, or perceived social presence can reconfigure the very pathway through which an agent moves from appraisal to action \cite{Conty2016}. Artificial agents, even without agency or intention, participate in this structure by perturbing the field of salience and social meaning~\cite{Kuchenbrandt2011,Malle2016,Zlotowski2015}.

So the shift we’re proposing isn’t just methodological; it’s conceptual. It reframes the core task of moral AI. Instead of asking, How can machines apply moral principles? we have to ask:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{How do artificial agents alter the environment in which humans experience, interpret, and act on moral cues?}
	\end{leftbar}
\end{center}

That’s the question that anchors the thesis. And later, when we look at the experimental results, we’ll see why a principle-driven account simply can’t capture the effects we observe.

% -------------------------------------------------------
% Section VII — Structure of the Thesis
% -------------------------------------------------------

The argument developed so far brings us to a decisive shift. The question that will guide the remainder of the thesis is no longer whether artificial agents can execute or approximate moral principles, but how their presence reshapes the very field in which humans perceive, interpret, and respond to moral cues. This reframing closes the introduction and opens the path to the theoretical and empirical work that follows.

\medskip

\section{Structure of the Thesis}

The chapters that follow are arranged to make the implications of this shift increasingly explicit. The progression is cumulative. Each chapter establishes the conditions under which the next can be understood, and together they build a unified account of machine–mediated, machine-detactable moral cognition.

Chapter \ref{chap:lit_rev} establishes the philosophical and methodological ground of the thesis. It disentangles the two projects often grouped under Machine Ethics—Human–Machine Ethics and Computational Machine Ethics—and shows why neither operates at the cognitive Level of Abstraction required to explain synthetic moral perturbation. Drawing on normative ethics, moral psychology, and Social Signal Processing, the chapter argues that moral behaviour arises from a salience-weighted evaluative process rather than from the application of encoded principles. Its central conclusion introduces the core tension that motivates the thesis:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Classical Machine Ethics works at the reflective LoA, while the phenomenon under investigation unfolds at the cognitive LoA, upstream of explicit moral reasoning.}
	\end{leftbar}
\end{center}

\bigskip
\noindent
Chapter \ref{chap:moral_primer} provides the conceptual architecture needed to understand moral cognition empirically. It introduces dual-process theories, the Social Intuitionist Model, affective tagging, attentional capture, and accountability structures, illustrating how these mechanisms shape the path from moral perception to action. The chapter identifies the inferential gap: \textit{the transformation from moral appraisal to moral behaviour.} This gap motivates the thesis’s central question—whether synthetic presence can perturb that transformation—and prepares the reader for a systematic account of the evaluative processes at stake.

\bigskip
\noindent
Chapter \ref{chap:tools_new} specifies the methodological infrastructure through which the thesis renders evaluative cognition empirically tractable. Whereas the previous chapters developed the theoretical topology of moral appraisal, the present chapter introduces the instruments—psychometric, dispositional, and perturbational—that operationalise that topology in experimental form. It clarifies how established constructs from moral psychology, cognitive science, social signal processing, and HRI serve not as neutral measurement devices but as theoretically motivated probes into the latent dispositional manifold modelled as $\beta_C$.

By situating the Empathizing Quotient, the Systemizing Quotient, the Big Five Inventory, and the Watching–Eye paradigm within the evaluative–topological framework, the chapter demonstrates that each tool targets a distinct dimension of the architecture through which moral salience is encoded, transformed, and expressed in behaviour. Their role is therefore conceptual rather than merely procedural: these instruments define the coordinate system in which the perturbation introduced by synthetic presence becomes detectable as a deformation of the evaluative field rather than as a trait-driven behavioural fluctuation.

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{The tools introduced here provide the empirical interface between theoretical topology and behavioural data: they operationalise the dispositional term $\beta_C$ and supply the salience baselines against which synthetic perturbation can be identified.}
	\end{leftbar}
\end{center}

\bigskip
\noindent

This chapter therefore establishes the measurement logic of the thesis. It shows why these specific instruments are required to distinguish dispositional variation from field-level modulation, and how they allow the experiment to test whether humanoid robotic presence alters not who participants are, but the evaluative topology within which their moral trajectories unfold.

Chapter \ref{chap:exp_methods} constitutes the empirical core of the thesis. It operationalises the evaluative–topological model developed in the earlier chapters into a full experimental framework, integrating design, measurement, and statistical inference into a single methodological architecture. The chapter introduces the controlled observational conditions, reconstructs the Watching–Eye paradigm, and justifies the use of the NAO platform as a parametrically stable source of synthetic presence. It specifies all behavioural measures, psychometric instruments, and salience manipulations, and it details the complete analytical pipeline—from preprocessing and cluster formation to non-parametric tests, regression modelling, and Bayesian estimation.

Its function is foundational: this is the chapter in which the three central hypotheses of the thesis—Evaluative Deformation, Synthetic Normativity, and Synthetic Perturbation of Moral Inference—are formally operationalised and subjected to empirical test. By consolidating the full experimental architecture with the statistical logic required to evaluate deformation in the evaluative field, the chapter provides the decisive evidence for the thesis’ central claim: that synthetic co-presence induces a measurable, structured alteration in the mapping from moral salience to action that cannot be reduced to trait-level variation or noise.

Chapter \ref{chap:ethics_s} reconstructs the major normative traditions—deontology, consequentialism, virtue ethics, sentimentalism, contractualism, particularism, and hybrid views—at the appropriate Level of Abstraction for the thesis. Instead of treating them as implementable rule systems, the chapter interprets their normative structures as patterns that constrain or guide evaluation within human moral cognition. Floridi’s Level-of-Abstraction discipline is introduced here as a methodological tool for locating where an explanation must live. The chapter concludes by synthesising these perspectives into a coherent view of moral behaviour as a field-sensitive process shaped by both normative expectations and cognitive–affective dynamics. This synthesis provides the philosophical infrastructure that makes the subsequent hypotheses meaningful.

Chapter \ref{chap:general_discussion} provides the structural integration of the thesis. It unifies the cognitive–affective architecture, the normative analyses, and the experimental findings into a single theoretical account of how synthetic presence perturbs moral cognition. Building on the experimental result—uniform attenuation of prosocial donation under humanoid co-presence—the chapter shows that the effect cannot be understood as a trait-level phenomenon, a local behavioural anomaly, or a deficit of explicit reasoning. Instead, it requires a field-level interpretation: synthetic presence deforms the evaluative topology that ordinarily carries moral salience into action. By bringing together the three dispositional ecologies, the topological formalism, the reconstructed normative frameworks, and Floridi’s Level-of-Abstraction analysis, the chapter argues that the humanoid robot operates as a perturbation operator on the moral field, not as an ethical agent. Its role is therefore decisive: it offers a general theoretical synthesis through which the empirical signature revealed by the data becomes a window into the structure of moral cognition and the methodological limits of Machine Ethics.

Taken together, these chapters form a cumulative argumentative trajectory. Each chapter establishes the conditions of intelligibility for the next, guiding the reader from conceptual reframing to cognitive mechanism, from mechanism to experimental design, from empirical outcome to theoretical explanation. The result is a systematic account of how synthetic presence perturbs human moral cognition and what this means for the future of moral AI.