\chapter{Introduction}
\label{chap:introduction}
Think of moral decision-making as the full mental sequence we go through when we’re choosing between competing ideas of what the ‘right thing’ might be. It starts with what we notice: certain details stand out, others fade into the background. Those initial impressions shape what we care about, which in turn shapes what we treat as relevant. Only then does our reasoning step in to organise all of that into a sense of, ‘This is what I should do.’ In a way, it’s the process that turns a handful of moral impressions into a genuine commitment to act.

And most of the time, this isn’t a slow, deliberate calculation. It’s closer to an immediate sense of something feeling right or wrong, which we then test against the situation and the social world around us. We respond to small cues—a shift in tone, a facial expression, the atmosphere of a room—and they quietly push us toward one reaction rather than another long before we begin to articulate reasons.

After that early, intuitive pull, we start to refine it. We call to mind similar situations. We notice details we missed at first glance. We talk it through, sometimes out loud, sometimes just internally. And we develop reasons that make sense of the direction we’re already leaning toward. The decision is still real, but it grows out of these quick, socially shaped impressions that guide us well before any careful reflection begins.

This is precisely why the idea of creating a ‘moral’ machine by embedding a single ethical theory—utilitarianism, deontology, or any other framework—is so misguided. Those theories are helpful tools for analysing moral arguments after they’ve happened, but they’re not the engines we rely on when we actually navigate a situation. They’re abstractions, not working models of human judgment.

Yet in the technology world, you still encounter the view that if you program a system to follow a specific theory, you’ve solved the moral problem. That assumption is, at best, overly optimistic. A machine following a tidy rulebook bears little resemblance to what humans do when we sense tension in a room, register someone’s discomfort, or feel the pull of how our actions will land with others. Real moral life is textured, social, emotional, and deeply dependent on context. There isn’t a clean set of instructions that captures all that.

So when somebody claims to have built an algorithm that ‘acts ethically,’ it often reflects an academic game of who can produce the most polished theoretical model rather than a meaningful engagement with how moral decisions actually work. 

The theory may look elegant on paper, but it doesn’t map onto the realities of human moral experience.

And this, is exactly the space where our work begins. We know that our moral reactions are shaped by tiny cues—someone’s expression, the tension in their posture, the energy in a room, even things as subtle as the smell of someone who’s had a long day. These details don’t just colour the moment; they steer our judgment before we’re even aware of it.

So the real question for us is this: what happens when the agent in front of you isn’t a person at all, but a humanoid robot? How do we respond when the timing of a gaze is algorithmic, and the emotional tone is produced by design rather than by experience?

We still react. We can’t help it. Our perceptual systems are tuned to pick up anything that looks or behaves like a person. But the meaning of those reactions becomes murkier. Are we responding to genuine social cues, or to clever mimicry? And if a robot can reliably trigger the same moral intuitions that another human does, what does that say about the foundations of our own judgments?

For us, that’s the critical challenge. Not whether a machine can follow a rulebook, but \textit{but how our deeply human, automatic moral instincts adapt—or fail to adapt—when something built rather than born is standing in front of us. And not just in a lab, but in our rooms, in our kitchens, woven into the background of daily life.}

\bigskip
\noindent
Moral decision-making is:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{The cognitive process through which agents select between competing moral judgments—mutually exclusive evaluations of what is right or wrong, good or bad—that provide the motive, direction, and justificatory structure of their practical behaviour. It is a composite operation: perceptual encoding, affective appraisal, memory, attentional orientation, and interpretive reasoning jointly determine how morally salient cues are registered, weighted, and transformed into a behavioural commitment.}
	\end{leftbar}
\end{center}
\bigskip
\noindent

The work we present here developes withing this framework, which we applied to a concrete and experimentally tractable setting within Human--Robot Interaction and Social Signal Processing. 

We conducted a study in which participants enter a small room and encounter a simple but meaningful moral choice: they may donate part of their participation payment to a real charity, or keep the full amount for themselves. This setting does not claim to capture moral cognition in its entirety; instead, it offers a minimal, controlled environment in which the elements of its definition become empirically observable. 
 

%\noindent
%Upon entering the room, participants first engage in \textbf{perceptual encoding}: they register the coins on the table, the bright charity charity, the instruction sheet, and the charity poster overhead depicting a child in need with big expressive eyes. These serve as the \emph{morally salient cues} structuring the situation. Almost immediately, \textbf{affective appraisal} is recruited: the charitable context evokes a mild empathic pull; the Watching-Eye cue introduces a sense of being implicitly observed by the child; and the act of giving up one’s own money elicits a familiar tension between prosocial motivation and self-interest. Alongside this, \textbf{memory and normative expectations} quietly shape interpretation—past experiences with giving, internalised cultural norms about generosity, and associations between being watched and acting prosocially.


\noindent
Upon entering the room, participants first engage in \textbf{perceptual encoding}: they register the coins on the table, the charity materials, and the child-poster overhead with its large, expressive eyes. These elements constitute the \emph{morally salient cues} structuring the situation, consistent with work showing that minimal observational cues and child-like eyes heighten perceived social relevance and implicit monitoring \cite{Haley2005, Bateson2006, ErnestJones2011, Nettle2013, Dear2019, Conty2016}. 

Almost immediately, \textbf{affective appraisal} is recruited. The charitable context elicits a mild empathic pull in line with established findings on affective resonance and empathetic sensitivity \cite{BaronCohenWheelwright2004_EmpathyQuotient, Gleichgerrcht2013, Haidt2001EmotionalDog}. Simultaneously, the watching-eye cue introduces an implicit sense of being observed, activating reputational and attentional systems documented in observational-cue research \cite{Haley2005, Bateson2006, Dear2019}. The prospect of giving up one’s own money further evokes the familiar tension between prosocial motivation and self-interest captured in dual-process and motivational models of moral decision-making \cite{Greene2004, Cushman2013DualSystem, Crockett2016Models}.

Alongside these immediate appraisals, \textbf{memory and normative expectations} shape interpretation: past experiences with charitable giving, internalised cultural norms of generosity, and well-established associations between being watched and acting prosocially influence how the evaluative field is instantiated in the moment \cite{Nettle2013, Bateson2006, Haidt2001EmotionalDog}.

\noindent
At the same time, \textbf{attentional orientation} determines which elements dominate the evaluative landscape: is the participant more attuned to the need expressed by the charity? to the coins that could be kept? 

\noindent
To describe moral decision-making in this sense is to recognise its fundamentally 
\emph{teleological} character, a view rooted in classical action-centred accounts of 
ethics \cite{Aristotle_nicomachean,Foot2001,Korsgaard1996}.  
Moral cognition unfolds toward action: it organises the evaluative conditions under 
which an agent adopts one course rather than another, consistent with empirical models 
linking appraisal to action selection \cite{Haidt2001,Greene2004,Cushman2013}.  
The transition from moral judgment to behaviour is not an optional addendum to the 
process—it is its natural terminus.  
A moral evaluation that does not shape the field of possible actions has not yet 
completed its function; a moral action, conversely, is the crystallised endpoint of 
evaluative dynamics that have been unfolding long before reflection makes them explicit 
\cite{Decety2004,Moll2002,Haidt2001}.

\medskip
\noindent
The participant’s eventual choice to donate or not is the behavioural crystallisation of this entire evaluative process. This thesis examines how the silent co-presence of a humanoid robot modulates that transformation. The robot does not request, instruct, or communicate, yet its ambiguous social ontology—perceptually agentic, normatively indeterminate—reshapes the conditions under which moral judgments are formed and resolved. In this way, the experiment offers a precise instantiation of the definition of moral decision-making introduced above: a setting in which perceptual cues, affective resonance, attentional dynamics, and implicit social meaning combine to produce a practical moral commitment, and in which that process can be systematically perturbed.

\medskip
\noindent
Moral cognition thus operates within a social environment dense with cues—gaze, posture, interpersonal distance, implicit accountability signals—that modulate the affective and attentional components of evaluation. These modulations occur upstream of explicit reasoning: they determine \emph{what becomes salient} well before agents deliberate on what \emph{ought} to be done.

\medskip
\noindent
The introduction of synthetic agents into this environment raises a conceptual and empirical challenge. Humanoid robots occupy a liminal ontological space: perceptually social yet not persons, agent-shaped yet not agents. Their presence recruits perceptual and affective systems that evolved for human–human interaction, while simultaneously withholding the ordinary resources through which social meaning stabilises. This thesis examines the possibility that \emph{such entities reshape the evaluative conditions of moral cognition not by acting, but simply by being present}.

\bigskip
\noindent
One may picture the problem in concrete terms of our example above.Imagine the participant in the experimental room. On a table: the charity box, a few pound coins, and a simple instruction inviting a donation. The child in need, with big expressive eyes—an established prime of perceived accountability—looks down from a poster.
Alone, the participant might experience a mild empathic pull, a subtle sense of being expected to act prosocially.

Now place a NAO robot on the same table.
It does nothing. It does not speak, gesture, or request. Yet its humanoid shape, its forward posture, its apparent capacity for attention, reframes the scene. The participant hesitates: the social field has changed. Something in the evaluative machinery has shifted—an attenuation of empathic pull, a dilution of accountability, a re-weighting of salience.

We started by looking at something very simple: what happens when a humanoid robot is present in the room while someone is making a moral decision. The robot doesn’t talk, it doesn’t give instructions, it doesn’t ask for anything. It just shares the space—quietly, almost like another person waiting their turn.

But that quiet presence turns out to matter. A robot like that sits in an odd position: it looks and moves in ways that make us treat it as an agent, yet we don’t quite know what kind of ‘being’ it is or what norms apply to it. That ambiguity changes the atmosphere. It shifts how people interpret the situation, what they take to be appropriate, and how comfortable they feel committing to one judgment over another.

So even without speaking, the robot reshapes the background against which moral choices are made. It nudges the whole process—not by argument or instruction, but simply by being there, hovering between the familiar category of a person and the familiar category of a machine. That’s where we see the transformation beginning.

This modest behavioural moment is the phenomenon under investigation.What has changed? And why?

\noindent
The central question that follows from this observation frames the entire research
programme:

\bigskip
\noindent
\begin{center}
\begin{leftbar}
	\textit{Can the mere presence of a synthetic, non-agentic entity perturb the inferential 
		transformation through which morally salient cues are converted into observable moral 
		behaviour?}
\end{leftbar}
\end{center}

\bigskip
\noindent
This question is motivated by the theoretical claim that synthetic agents may function
as \emph{operators on the evaluative field} in which moral decisions are formed.
If their perceptual salience or ambiguous social ontology alters the distribution of
attention, empathy, or accountability, then the evaluative trajectory that links
perception to action may shift accordingly.
In such a case, moral behaviour would not be changed by explicit influence but by
modulation of the cognitive–affective machinery upstream of conscious judgment.

In that case, moral behaviour wouldn’t be shifting because the robot told anyone what to do. It would be shifting because the upstream machinery—the mix of perception, emotion, and expectation that feeds into conscious judgment—has been quietly modulated. The influence is silent, indirect, and deeply embedded in the way we make sense of the world. That’s why this moment, small as it looks, matters.

\section{From Research Question to Hypotheses: Framing the Investigative Architecture}

Our question comes from a broader theoretical idea: that synthetic agents might operate on the moral landscape in which our decisions take shape. Not by persuasion, not by argument, but by subtly altering the conditions under which those judgments form. If a robot’s visual presence, or the uncertainty about what kind of ‘being’ it is, changes where people direct their attention, or how much empathy they feel, or who they think is accountable, then the whole path from perception to action can start to bend.

\medskip
\noindent
If the simple presence of a synthetic agent shifts that chain of inferences,\noindent
then the traditional approach in machine ethics—starting with abstract principles and trying to code them directly into a system \cite{Anderson2011,Bringsjord2006,Arkin2009,Wallach2008,Guarini2006}—can’t explain what’s going on. Those models operate at the reflective level, the level where we articulate reasons and moral rules. But the effects we’re observing happen earlier, in the pre-reflective machinery that sets the stage for those reasons.

So we need a different way of thinking about moral behaviour. A framework that treats it as the outcome of a field shaped by attention, emotion, and the way certain cues stand out or fade away. In that view, moral action isn’t just a conclusion drawn from a principle; it’s the end point of a landscape structured by what feels salient, what draws concern, and what seems to matter in the moment. That’s the level at which synthetic presence exerts its influence—and the level we have to model if we want to understand it.

\noindent
One way to make sense of this is by borrowing a notion from Luciano Floridi: the Level of Abstraction \cite{Floridi2008,Floridi2011}. It’s a simple idea with a lot of power behind it. Whenever we study a system—whether it’s a computer, a person, a society—\textit{we have to decide the level at which we’re describing it}. Are we talking about the underlying code? The behaviour? The motivations? The social context? Each level reveals some things and hides others.

\noindent
Most classical work in machine ethics starts at a very high, reflective level of abstraction. It focuses on principles—rules about what the system should or shouldn’t do—and tries to formalise those rules so they can be implemented~\cite{Allen2005,Arkoudas2005,Winfield2019,Anderson2010}. That’s useful if your goal is to build a system that behaves consistently with a particular ethical theory. But it tells you almost nothing about what happens at the cognitive level, where perception and emotion begin shaping the decision long before anyone appeals to a principle.


Our work sits at a different level of abstraction. We’re looking at the machinery that turns raw perception into a sense of what matters, and then into action. At that level, the presence of a humanoid robot isn’t a question about the robot’s rights or intentions; it’s a question about how its appearance and behaviour reshape the informational landscape the human is navigating.

\medskip
\noindent
Once we fix the Level of Abstraction—the cognitive level where perception, concern, and action are linked—we can be precise about what we’re testing. The thesis proposes three hypotheses, each tied to a different kind of perturbation at that level. They’re not rivals. They’re three structurally distinct ways in which the presence of a synthetic agent might reshape the evaluative process itself. Each one captures a different mechanism through which the perceptual and affective landscape can shift before conscious judgment begins. The thesis therefore develops three hypotheses, each mapped onto a different kind of perturbation within the cognitive–affective system that generates moral judgment. They’re not competing explanations; each one isolates a distinct structural route through which the simple presence of a synthetic agent might influence the transformation from perception to action.

Taken together, these hypotheses define the theoretical space of the project. They mark out the possibilities that become visible once we commit to the correct Level of Abstraction—the level where shifts in salience, attention, and affect reorganise the evaluative field long before a person arrives at a conscious moral conclusion.

The first hypothesis says that the robot changes the function that maps what you perceive to how you evaluate it. 
\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 1: Evaluative Deformation]
		Synthetic presence alters the evaluative function 
		$f : \mathcal{X} \rightarrow \mathcal{A}$ by reshaping salience 
		gradients, affective weights, or attentional trajectories. 
		In this model, the robot acts as a \emph{field operator}: 
		its perceptual salience deforms the topology through which moral cues acquire 
		behavioural force.
	\end{tcolorbox}
\end{center}

\bigskip
\noindent
The mathematical notation—$f : \mathcal{X} \rightarrow \mathcal{A}$—just means:
given some input from the world, how do you turn it into a sense of what matters? What we test here is very simple: does having a humanoid robot in the room subtly shift what stands out to the subject, what feels important, or what pulls their attention?

If the robot is visually or socially salient—even without speaking—it might ‘bend’ the landscape you’re navigating. Think of it like a small gravitational field: it doesn’t tell you what to do, but it changes the shape of the space you’re moving through. This hypothesis asks:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{ does the robot’s presence deforms that evaluative landscape just enough to change how moral cues gain their force?}
	\end{leftbar}
\end{center}

\bigskip
\noindent
The second hypothesis is about how people interpret responsibility and expectations in the presence of a humanoid robot. Here the claim is not that the robot has moral status or intentions. It’s that its human-like appearance gives it certain practical effects in how people interpret the situation. 

\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 2: Synthetic Normativity of Moral Displacement]
		A humanoid robot acquires \emph{normative affordances} through its 
		ambiguous social ontology. 
		Without communicating or expressing intention, it may refract perceived 
		accountability relations, modifying how agents interpret morally salient cues 
		within the situation.
	\end{tcolorbox}
\end{center}

People may unconsciously treat it as if it participates in the moral scene, even though it hasn’t said or done anything. So this hypothesis asks:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Does the robot shift who people feel accountable to, or who they think is paying attention, or what they think ‘counts’ in that moment?}
	\end{leftbar}
\end{center}

\bigskip
\noindent
The robot’s ambiguous status—something between a person and a tool—may subtly redirect moral attention. It’s not giving orders; it’s reframing the situation just by being there.

The third hypothesis looks at what happens in the transition from noticing something morally important to actually doing something about it.

Humans don’t move straight from perception to action. There’s a whole middle layer: empathy, emotional resonance, a sense of alignment with others. This hypothesis asks whether the robot interferes with that middle layer.

Does its presence dampen empathy? Does it redirect attention? Does it change how strongly certain cues ‘tag’ the situation as requiring action?

\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 3: Synthetic Perturbation of Moral Inference]
		Synthetic presence interferes with the transition from moral salience to 
		prosocial action by modulating empathic resonance, affective tagging, or 
		attentional alignment. 
		This mechanism predicts differential perturbation across dispositional 
		ecologies, precisely as observed in the experimental results.
	\end{tcolorbox}
\end{center}

So this final hypothesis says:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{the robot doesn’t change the rule you apply—it changes the internal bridge that links your moral perception to your moral behaviour}
	\end{leftbar}
\end{center}

And importantly, this hypothesis predicts that people with different dispositions—different personalities, sensitivities, backgrounds—will be affected differently. That’s exactly what the experiments showed: the effect isn’t uniform; it varies depending on the person.

\medskip
\noindent
These hypotheses structure the theoretical and empirical work that follows. 
They operationalise the core research question---whether synthetic presence can 
perturb the inferential machinery that links moral perception to moral action---and 
provide the conceptual scaffolding through which the experiment in 
Chapter~\ref{chap:experimental_methods} is interpreted.

\noindent
Together, these three hypotheses outline the whole space in which synthetic presence might influence moral judgment. Each captures a different mechanism, and all of them operate at the cognitive level—the level where perception and affect set the stage for what we later call ‘a moral decision.’
% -------------------------------------------------------
% Section VI — The Need for a New Theoretical Orientation
% -------------------------------------------------------

\section{The Need for a New Theoretical Orientation}

\noindent
The three hypotheses formulated above express a single structural insight: 
the standard conceptual resources of Machine Ethics are insufficient to account for 
how synthetic agents modulate human moral behaviour. 
The field traditionally begins from reflective-level principles—rules, utilities, 
virtues, deontic operators—and attempts to build ethical machines by encoding 
these principles into computational architectures. 
But the phenomenon at the centre of this thesis does not occur at the reflective 
Level of Abstraction. 
It emerges upstream, within the cognitive--affective substrate that transforms 
moral salience into action.

\medskip

\noindent
The attenuation observed in the experiment cannot be explained as a failure of 
reasoning, a misapplication of a rule, or a re-evaluation of moral principle. 
Its locus is neither propositional nor deliberative. 
It arises at the level where perceptual cues acquire motivational relevance: 
where gaze, co-presence, empathy, and accountability shape the evaluative field  
in which a moral judgment becomes behaviourally operative. 
In short, the empirical effect is a deformation of the machinery that \emph{enables} 
moral reasoning, not a modification of reasoning itself.

\medskip

\noindent
This requires a theoretical reorientation. 
A scientifically credible approach to moral AI cannot begin with ethical theory and 
work downward; it must begin with the architecture of moral cognition and work 
upward. 
Such an approach treats moral behaviour as the emergent trajectory of an 
affectively structured, salience-weighted evaluative system—one that can be 
perturbed by social cues, modified by attentional dynamics, and reshaped by the 
presence of synthetic bodies.

\medskip

\noindent
This shift is not merely methodological; it is conceptual. 
It reframes the task of moral AI from one of principle implementation to one of 
evaluative-field analysis. 
Instead of asking how machines can apply moral rules, the relevant question 
becomes: \textit{How do artificial systems alter the conditions under which human 
	agents experience, interpret, and act upon moral cues?} 
The answer, as the hypotheses suggest, lies not in robot agency but in robot 
ontology: in the way synthetic presence occupies, disrupts, or refracts the social 
perceptual channels through which moral meaning is ordinarily stabilised.

\medskip

\noindent
The remainder of the thesis proceeds from this orientation. 
It develops the empirical, philosophical, and computational tools needed to model moral behaviour at the appropriate Level of Abstraction; it analyses synthetic presence as a perturbation operator on the evaluative field; and it situates the experimental findings within a broader account of machine-mediated moral cognition. This reorientation is not an auxiliary interpretive choice—it is the condition of possibility for understanding the phenomenon under investigation.

% -------------------------------------------------------
% Section VII — Structure of the Thesis
% -------------------------------------------------------

\section{Structure of the Thesis}

\noindent
The thesis unfolds through a sequence of conceptual stages designed to guide the 
reader from the foundational question of inferential displacement to a 
fully articulated model of machine–mediated moral cognition. 
Each chapter contributes a necessary component of this trajectory, both 
theoretically and empirically, and prepares the conditions under which the 
subsequent chapters become intelligible. 
What follows is a roadmap of this structure.

\medskip

\noindent
\textbf{Chapter~\ref{chap:lit_rev} (Reframing Machine Ethics)}  
establishes the philosophical and methodological ground on which the thesis stands. 
It reconstructs the two projects traditionally conflated under the name 
\emph{Machine Ethics}—Human--Machine Ethics and Computational Machine Ethics—while 
demonstrating why neither can explain the phenomenon of synthetic moral 
perturbation.  
Through the combined lenses of normative theory, moral psychology, and Social 
Signal Processing, it articulates the central claim that moral behaviour emerges 
from a salience-weighted evaluative field rather than from encoded ethical 
principles.  
Its conclusions introduce the conceptual tension—between reflective ethical 
theories and the cognitive LoA of moral behaviour—that prepares the reader for the 
formal reconstruction that follows.

\medskip

\noindent
\textbf{Chapter~\ref{chap:moral_primer} (A Moral Primer for Computer Scientists)}  
provides the conceptual architecture necessary for engaging with moral cognition 
from a scientifically disciplined perspective.  
It introduces dual-process models, the Social Intuitionist Model, affective 
tagging, attentional capture, and accountability structures, showing how these 
mechanisms generate the trajectories through which moral salience becomes action.  
The chapter culminates in the identification of the \emph{inferential gap}—the 
transformation from moral appraisal to moral behaviour—and motivates the research 
question of whether synthetic presence can perturb this transformation.  
The conclusion of this chapter thus sets the stage for a more systematic treatment 
of moral cognition within an evaluative-topological framework.

\medskip

\noindent
\textbf{Chapter~\ref{chap:ethics_s} (Ethical Cognition and Normative Foundations)}  
reconstructs the major normative traditions—deontology, consequentialism, virtue 
ethics, sentimentalism, contractualism, particularism, and hybrid models—through a 
topological reinterpretation.  
Here, normative structures are treated as constraints, gradients, attractors, or 
justificatory equilibria within an evaluative manifold.  
This chapter also introduces Floridi’s Level-of-Abstraction discipline, which 
provides the methodological boundary conditions that classical Machine Ethics 
violates.  
The chapter closes by synthesising these frameworks into a unified model in which 
moral behaviour is understood as a field-sensitive process.  
This synthesis provides the philosophical infrastructure that makes the 
experimental hypotheses both meaningful and testable.

\medskip

\noindent
\textbf{Chapter~\ref{chap:experimental_methods} (Experimental Architecture)}  
translates the conceptual and philosophical commitments of the preceding chapters 
into an empirical framework.  
It reconstructs the Watching-Eye paradigm, justifies the selection of NAO as a 
synthetic perturbation operator, and formalises the measurement logic through the 
EQ, SQ, and BFI instruments.  
This chapter also operationalises the three hypotheses—Evaluative Deformation, 
Synthetic Normativity, and Synthetic Perturbation of Moral Inference—within a 
controlled behavioural environment.  
Its function within the thesis is architectural: it builds the experimental world 
in which the theoretical claims of the prior chapters can be empirically tested.

\medskip

\noindent
\textbf{Chapter~\ref{chap:results} (Results)}  
presents the empirical outcome of the experimental study.  
Across dispositional profiles, the presence of the humanoid robot produces a 
uniform attenuation of prosocial donation.  
This attenuation is not random; it is structured, directional, and aligned with 
the perturbation mechanisms predicted by the evaluative-topological framework.  
The chapter does not offer interpretation—only evidence.  
Its role is to provide the behavioural fact-pattern that the subsequent chapter 
must explain.

\medskip

\noindent
\textbf{Chapter~\ref{chap:discussion} (Discussion: A Topological Account of Synthetic Moral Perturbation)}  
provides the integrative explanation toward which the thesis has been moving.  
It interprets the empirical attenuation as a deformation of the evaluative field, 
a suppression of salience gradients, and a displacement of empathic–attentional 
pathways.  
The chapter synthesises the normative, psychological, and topological analyses 
into a coherent model in which robots act not as moral agents but as operators 
that modulate the cognitive–affective substrate of moral behaviour.  
Its conclusion articulates the thesis’s central claim: \textit{machines reshape the 
	conditions under which humans experience and enact moral meaning}.  
This chapter completes the arc initiated in the Introduction.

\medskip

\noindent
\textbf{Chapter~\ref{chap:conclusion} (General Conclusion)}  
draws together the conceptual, empirical, and methodological contributions of the 
thesis.  
It reflects on the implications of synthetic moral perturbation for the future of 
HRI, moral psychology, and Machine Ethics, and it articulates the sense in which 
a scientifically credible programme for moral AI must begin with the architecture 
of human moral cognition rather than the codification of ethical principles.  
Finally, it outlines the broader conceptual contribution: a field-theoretic model 
of moral behaviour that integrates normative constraints, cognitive–affective 
mechanisms, and machine-mediated modulation into a unified philosophical and 
scientific framework.

\medskip

\noindent
Taken together, these chapters form a single argumentative trajectory.  
Each chapter furnishes the conceptual or empirical conditions of intelligibility 
for the next; each step advances the central question from abstraction to 
mechanism, from mechanism to experiment, from experiment to interpretation.  
The cumulative result is a comprehensive theory of how synthetic presence 
perturbs human moral cognition and what this means for the future of moral AI.

