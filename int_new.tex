\chapter{Introduction}
\label{chap:introduction}
Think of moral decision-making as the full mental sequence we go through when we’re choosing between competing ideas of what the ‘right thing’ might be. It starts with what we notice: certain details stand out, others fade into the background. Those initial impressions shape what we care about, which in turn shapes what we treat as relevant. Only then does our reasoning step in to organise all of that into a sense of, ‘This is what I should do.’ In a way, it’s the process that turns a handful of moral impressions into a genuine commitment to act.

And most of the time, this isn’t a slow, deliberate calculation. It’s closer to an immediate sense of something feeling right or wrong, which we then test against the situation and the social world around us. We respond to small cues—a shift in tone, a facial expression, the atmosphere of a room—and they quietly push us toward one reaction rather than another long before we begin to articulate reasons.

After that early, intuitive pull, we start to refine it. We call to mind similar situations. We notice details we missed at first glance. We talk it through, sometimes out loud, sometimes just internally. And we develop reasons that make sense of the direction we’re already leaning toward. The decision is still real, but it grows out of these quick, socially shaped impressions that guide us well before any careful reflection begins.

This is precisely why the idea of creating a ‘moral’ machine by embedding a single ethical theory—utilitarianism, deontology, or any other framework—is so misguided. Those theories are helpful tools for analysing moral arguments after they’ve happened, but they’re not the engines we rely on when we actually navigate a situation. They’re abstractions, not working models of human judgment.

Yet in the technology world, you still encounter the view that if you program a system to follow a specific theory, you’ve solved the moral problem. That assumption is, at best, overly optimistic. A machine following a tidy rulebook bears little resemblance to what humans do when we sense tension in a room, register someone’s discomfort, or feel the pull of how our actions will land with others. Real moral life is textured, social, emotional, and deeply dependent on context. There isn’t a clean set of instructions that captures all that.

Assertions that an algorithm “acts ethically” typically reflect confidence in the coherence of a formal model rather than an examination of how moral behaviour actually arises in human agents. Such models can be elegant and internally consistent, yet their structure often mirrors the assumptions required for formal tractability rather than the dynamics that govern intuitive moral cognition. The difficulty is not that these approaches are misguided, but that they describe a different kind of object: a rule-based construction designed to yield determinate outputs, rather than the perceptual, affective, and context-sensitive processes through which moral considerations gain relevance in human action. Recognising this distinction clarifies why achievements at the level of formal specification do not straightforwardly illuminate the mechanisms by which people navigate morally charged situations.

The theory may look elegant on paper, but it doesn’t map onto the realities of human moral experience.

And this, is exactly the space where our work begins. We know that our moral reactions are shaped by tiny cues—someone’s expression, the tension in their posture, the energy in a room, even things as subtle as the smell of someone who’s had a long day. These details don’t just colour the moment; they steer our judgment before we’re even aware of it.

So the real question for us is this: what happens when the agent in front of you isn’t a person at all, but a humanoid robot? How do we respond when the timing of a gaze is algorithmic, and the emotional tone is produced by design rather than by experience?

We still react. We can’t help it. Our perceptual systems are tuned to pick up anything that looks or behaves like a person. But the meaning of those reactions becomes murkier. Are we responding to genuine social cues, or to clever mimicry? And if a robot can reliably trigger the same moral intuitions that another human does, what does that say about the foundations of our own judgments?

For us, that’s the critical challenge. Not whether a machine can follow a rulebook, but \textit{how our deeply human, automatic moral instincts adapt—or fail to adapt—when something built rather than born is standing in front of us}. And not just in a lab, but in our rooms, in our kitchens, and—more quietly—in our phones, woven into the background of daily life.

And it is here that we must pause, abruptly, to ask what exactly is being altered. 

Moral decision-making is:

\noindent
\begin{center}
	\begin{leftbar}
		\textit{The cognitive process through which agents select between competing moral judgments—mutually exclusive evaluations of what is right or wrong, good or bad—that provide the motive, direction, and justificatory structure of their practical behaviour. It is a composite operation: perceptual encoding, affective appraisal, memory, attentional orientation, and interpretive reasoning jointly determine how morally salient cues are registered, weighted, and transformed into a behavioural commitment.}
	\end{leftbar}
\end{center}
\bigskip

This move-- from a lived encounter to an analytic definition-- is deliberate. We cannot understand how artificial agents affect our moral thinking unless we consider both the concrete situations in which people act, and the underlying processes that give their moral behaviour its structure. With this framework, we designed a study in which participants enter a small room and face a simple moral choice. They may give part of their participation payment to a real charity, or keep the full amount for themselves. This task is not intended to represent moral cognition in all its complexity. Its aim is different. By providing a minimal and controlled setting, it allows some of the elements in our definition of moral decision-making to become empirically observable.

This setting does not claim to capture moral cognition in its entirety; instead, it offers a minimal, controlled environment in which the elements of its definition become empirically observable. 
 
Upon entering the room, participants first engage in \textbf{perceptual encoding}: they register the coins on the table, the charity materials, and the child-poster overhead with its large, expressive eyes. These elements constitute the \emph{morally salient cues} structuring the situation, consistent with work showing that minimal observational cues and child-like eyes heighten perceived social relevance and implicit monitoring \cite{Haley2005, Bateson2006, ErnestJones2011, Nettle2013, Dear2019, Conty2016}. 

Almost immediately, \textbf{affective appraisal} is recruited. The charitable context elicits a mild empathic pull in line with established findings on affective resonance and empathetic sensitivity \cite{BaronCohenWheelwright2004_EmpathyQuotient, Gleichgerrcht2013, Haidt2001EmotionalDog}. Simultaneously, the watching-eye cue introduces an implicit sense of being observed, activating reputational and attentional systems documented in observational-cue research \cite{Haley2005, Bateson2006, Dear2019}. The prospect of giving up one’s own money further evokes the familiar tension between prosocial motivation and self-interest captured in dual-process and motivational models of moral decision-making \cite{Greene2004, Cushman2013DualSystem, Crockett2016Models}.

Alongside these immediate appraisals, \textbf{memory and normative expectations} shape interpretation: past experiences with charitable giving, internalised cultural norms of generosity, and well-established associations between being watched and acting prosocially influence how the evaluative field is instantiated in the moment \cite{Nettle2013, Bateson2006, Haidt2001EmotionalDog}. At the same time, \textbf{attentional orientation} determines which elements dominate the evaluative landscape: is the participant more attuned to the need expressed by the charity, or to the coins that could be kept?

\noindent
Moral decision-making is, at its core, a fundamentally \emph{teleological} process. It unfolds toward action: its purpose is to organise the evaluative conditions under which an agent adopts one course rather than another, consistent with classical action-centred accounts of ethics~\cite{Aristotle_nicomachean,Foot2001,Korsgaard1996}. When we describe moral decision-making in terms of the perceptual, affective, mnemonic, and attentional operations outlined above, we are recognising the teleological structure through which these elements converge on a practical commitment. This view also aligns with empirical models linking appraisal to action selection~\cite{Haidt2001,Greene2004,Cushman2013}. 

The transition from moral judgment to behaviour is not an optional addendum to the process—it is its natural terminus. A moral evaluation that does not shape the field of possible actions has not yet completed its function; a moral action, conversely, is the crystallised endpoint of evaluative dynamics that have been unfolding long before reflection makes them explicit~\cite{Decety2004,Moll2002,Haidt2001}. The participant’s eventual choice to donate or not is the behavioural crystallisation of this entire evaluative process. 

This thesis examines how the silent co-presence of a humanoid robot modulates that transformation. The robot does not request, instruct, or communicate, yet its ambiguous social ontology—perceptually agentic, normatively indeterminate—reshapes the conditions under which moral judgments are formed and resolved. In this way, the experiment offers a precise instantiation of the definition of moral decision-making introduced above: a setting in which perceptual cues, affective resonance, attentional dynamics, and implicit social meaning combine to produce a practical moral commitment, and in which that process can be systematically perturbed.

\medskip
\noindent
Moral cognition thus operates within a social environment dense with cues—gaze, posture, interpersonal distance, implicit accountability signals—that modulate the affective and attentional components of evaluation. These modulations occur upstream of explicit reasoning: they determine \emph{what becomes salient} well before agents deliberate on what \emph{ought} to be done.

\noindent
The introduction of synthetic agents into this environment raises a conceptual and empirical challenge. Humanoid robots occupy a liminal ontological space: perceptually social yet not persons, agent-shaped yet not agents. Their presence recruits perceptual and affective systems that evolved for human–human interaction, while simultaneously withholding the ordinary resources through which social meaning stabilises. 

This thesis examines the possibility that \emph{such entities reshape the evaluative conditions of moral cognition not by acting, but simply by being present}.

\noindent
One may picture the problem in concrete terms of our example above. Imagine the participant in the experimental room. On a table: the charity box, a few pound coins, and a simple instruction inviting a donation. The child in need, with big expressive eyes—an established prime of perceived accountability—looks down from a poster.
Alone, the participant might experience a mild empathic pull, a subtle sense of being expected to act prosocially.

Now \textit{place a NAO robot on the same table}. It does nothing. It does not speak, gesture, or request. Yet its humanoid shape, its forward posture, its apparent capacity for attention, reframes the scene. The participant hesitates: the social field has changed. Something in the evaluative machinery has shifted—an attenuation of empathic pull, a dilution of accountability, a re-weighting of salience. We started by looking at something very simple: what happens when a humanoid robot is present in the room while someone is making a moral decision. The robot doesn’t talk, it doesn’t give instructions, it doesn’t ask for anything. It just shares the space—quietly, almost like another person waiting their turn. But that quiet presence turns out to matter. A robot like that sits in an odd position: it looks and moves in ways that make us treat it as an agent, yet we don’t quite know what kind of ‘being’ it is or what norms apply to it. That ambiguity changes the atmosphere. It shifts how people interpret the situation, what they take to be appropriate, and how comfortable they feel committing to one judgment over another. So even without speaking, the robot reshapes the background against which moral choices are made. It nudges the whole process—not by argument or instruction, but simply by being there, hovering between the familiar category of a person and the familiar category of a machine. That’s where we see the transformation beginning. This modest behavioural moment is the phenomenon under investigation.What has changed? And why?

\noindent
The central question that follows from this observation frames the entire research
programme:

\noindent
\begin{center}
\begin{leftbar}
	\textit{Can the mere presence of a synthetic, non-agentic entity perturb the inferential 
		transformation through which morally salient cues are converted into observable moral 
		behaviour?}
\end{leftbar}
\end{center}

\noindent
This question is motivated by the theoretical claim that synthetic agents may function
as \emph{operators on the evaluative field} in which moral decisions are formed.
If their perceptual salience or ambiguous social ontology alters the distribution of
attention, empathy, or accountability, then the evaluative trajectory that links
perception to action may shift accordingly.
In such a case, moral behaviour would not be changed by explicit influence but by
modulation of the cognitive–affective machinery upstream of conscious judgment.

In that case, moral behaviour wouldn’t be shifting because the robot told anyone what to do. It would be shifting because the upstream machinery—the mix of perception, emotion, and expectation that feeds into conscious judgment—has been quietly modulated. The influence is silent, indirect, and deeply embedded in the way we make sense of the world. That’s why this moment, small as it looks, matters.

\section{From Research Question to Hypotheses: Framing the Investigative Architecture}

Our question comes from a broader theoretical idea: that synthetic agents might operate on the moral landscape in which our decisions take shape. Not by persuasion, not by argument, but by subtly altering the conditions under which those judgments form. If a robot’s visual presence, or the uncertainty about what kind of ‘being’ it is, changes where people direct their attention, or how much empathy they feel, or who they think is accountable, then the whole path from perception to action can start to bend.

\noindent
If the simple presence of a synthetic agent shifts that chain of inferences,\noindent
then the traditional approach in machine ethics—starting with abstract principles and trying to code them directly into a system \cite{Anderson2011,Bringsjord2006,Arkin2009,Wallach2008,Guarini2006}—can’t explain what’s going on. Those models operate at the reflective level, the level where we articulate reasons and moral rules. But the effects we’re observing happen earlier, in the pre-reflective machinery that sets the stage for those reasons.

So we need a different way of thinking about moral behaviour. A framework that treats it as the outcome of a field shaped by attention, emotion, and the way certain cues stand out or fade away. In that view, moral action isn’t just a conclusion drawn from a principle; it’s the end point of a landscape structured by what feels salient, what draws concern, and what seems to matter in the moment. That’s the level at which synthetic presence exerts its influence—and the level we have to model if we want to understand it.

\noindent
One way to make sense of this is by borrowing a notion from Luciano Floridi: the Level of Abstraction \cite{Floridi2008,Floridi2011}. It’s a simple idea with a lot of power behind it. Whenever we study a system—whether it’s a computer, a person, a society—\textit{we have to decide the level at which we’re describing it}. Are we talking about the underlying code? The behaviour? The motivations? The social context? Each level reveals some things and hides others.

\noindent
Most classical work in Machine Ethics starts at a very high, reflective level of abstraction. It focuses on principles—rules about what the system should or shouldn’t do—and tries to formalise those rules so they can be implemented~\cite{Allen2005,Arkoudas2005,Winfield2019,Anderson2010}. That’s useful if your goal is to build a system that behaves consistently with a particular ethical theory. But it tells you almost nothing about what happens at the cognitive level, where perception and emotion begin shaping the decision long before anyone appeals to a principle.


Our work sits at a different level of abstraction. We’re looking at the machinery that turns raw perception into a sense of what matters, and then into action. At that level, the presence of a humanoid robot isn’t a question about the robot’s rights or intentions; it’s a question about how its appearance and behaviour reshape the informational landscape the human is navigating.

\medskip
\noindent
Once we fix the Level of Abstraction—the cognitive level where perception, concern, and action are linked—we can be precise about what we’re testing. The thesis proposes three hypotheses, each tied to a different kind of perturbation at that level. They’re not rivals. They’re three structurally distinct ways in which the presence of a synthetic agent might reshape the evaluative process itself. Each one captures a different mechanism through which the perceptual and affective landscape can shift before conscious judgment begins. The thesis therefore develops three hypotheses, each mapped onto a different kind of perturbation within the cognitive–affective system that generates moral judgment. They’re not competing explanations; each one isolates a distinct structural route through which the simple presence of a synthetic agent might influence the transformation from perception to action.

Taken together, these hypotheses define the theoretical space of the project. They mark out the possibilities that become visible once we commit to the correct Level of Abstraction—the level where shifts in salience, attention, and affect reorganise the evaluative field long before a person arrives at a conscious moral conclusion.

The first hypothesis says that the robot changes the function that maps what you perceive to how you evaluate it. 
\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 1: Evaluative Deformation]
		Synthetic presence alters the evaluative function 
		$f : \mathcal{X} \rightarrow \mathcal{A}$ by reshaping salience 
		gradients, affective weights, or attentional trajectories. 
		In this model, the robot acts as a \emph{field operator}: 
		its perceptual salience deforms the topology through which moral cues acquire 
		behavioural force.
	\end{tcolorbox}
\end{center}

\bigskip
\noindent
The mathematical notation—$f : \mathcal{X} \rightarrow \mathcal{A}$—just means:
given some input from the world, how do you turn it into a sense of what matters? What we test here is very simple: does having a humanoid robot in the room subtly shift what stands out to the subject, what feels important, or what pulls their attention?

If the robot is visually or socially salient—even without speaking—it might ‘bend’ the landscape you’re navigating. Think of it like a small gravitational field: it doesn’t tell you what to do, but it changes the shape of the space you’re moving through. This hypothesis asks:

\noindent
\begin{center}
	\begin{leftbar}
		\textit{Does the robot’s presence deforms that evaluative landscape just enough to change how moral cues gain their force?}
	\end{leftbar}
\end{center}

\noindent
The second hypothesis is about how people interpret responsibility and expectations in the presence of a humanoid robot. Here the claim is not that the robot has moral status or intentions. It’s that its human-like appearance gives it certain practical effects in how people interpret the situation. 

\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 2: Synthetic Normativity of Moral Displacement]
		A humanoid robot acquires \emph{normative affordances} through its 
		ambiguous social ontology. 
		Without communicating or expressing intention, it may refract perceived 
		accountability relations, modifying how agents interpret morally salient cues 
		within the situation.
	\end{tcolorbox}
\end{center}

\medskip
People may unconsciously treat a robot as if it participates in the moral scene, even though it hasn’t said or done anything. So this hypothesis asks:
\medskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Does the robot shift who people feel accountable to, or who they think is paying attention, or what they think ‘counts’ in that moment?}
	\end{leftbar}
\end{center}
\medskip
\noindent
The robot’s ambiguous status—something between a person and a tool—may subtly redirect moral attention. It’s not giving orders; it’s reframing the situation just by being there.

The third hypothesis looks at what happens in the transition from noticing something morally important to actually doing something about it.

Humans don’t move straight from perception to action. There’s a whole middle layer: empathy, emotional resonance, a sense of alignment with others. This hypothesis asks whether the robot interferes with that middle layer.

Does its presence dampen empathy? Does it redirect attention? Does it change how strongly certain cues ‘tag’ the situation as requiring action?

\bigskip
\noindent
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Hypothesis 3: Synthetic Perturbation of Moral Inference]
		Synthetic presence interferes with the transition from moral salience to 
		prosocial action by modulating empathic resonance, affective tagging, or 
		attentional alignment. 
		This mechanism predicts differential perturbation across dispositional 
		ecologies, precisely as observed in the experimental results.
	\end{tcolorbox}
\end{center}
\medskip
So this final hypothesis says:

\medskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{The robot doesn’t change the rule you apply—it changes the internal bridge that links your moral perception to your moral behaviour}
	\end{leftbar}
\end{center}
\medskip

And importantly, this hypothesis predicts that people with different dispositions—different personalities, sensitivities, backgrounds—will be affected differently. That’s exactly what the experiments showed: the effect isn’t uniform; it varies depending on the person.

\medskip
\noindent
These hypotheses structure the theoretical and empirical work that follows. They operationalise the core research question---whether synthetic presence can perturb the inferential machinery that links moral perception to moral action---and provide the conceptual scaffolding through which the experiment in Chapter~\ref{chap:exp_methods} is interpreted. Together, these three hypotheses outline the whole space in which synthetic presence might influence moral judgment. Each captures a different mechanism, and all of them operate at the cognitive level—the level where perception and affect set the stage for what we later call ‘a moral decision.’

\section{The Need for a New Theoretical Orientation}

All three hypotheses converge on a structural point: the research question in this thesis concerns the same broad domain that motivates Machine Ethics—how humans and artificial systems interact in morally relevant contexts—but approaches it from a different explanatory level. Traditional Machine Ethics begins with articulated normative frameworks—rules, utilities, virtues—and develops methods for ensuring that artificial systems behave in ways consistent with those frameworks. This is a coherent and valuable aim when the goal is to design systems whose actions can be aligned with explicit ethical models.

The present work, however, investigates a phenomenon that arises upstream of such normative articulation. Our question is whether the mere presence of a synthetic agent alters the process through which humans move from perceptual encounter to moral action. That transition is shaped by intuitive appraisal, affective resonance, and attentional dynamics long before principles or reasons are invoked. In this sense, Machine Ethics and the approach developed here examine related human–machine moral interactions, but at different explanatory levels: one focuses on the normative specification of artificial behaviour, while the other examines how artificial presence perturbs the evaluative processes through which human moral behaviour is formed.

In other words, the phenomenon we’re investigating doesn’t live at the reflective Level of Abstraction. It shows up upstream, in the cognitive–affective machinery that makes moral reasoning possible in the first place. When a humanoid robot is in the room, it can alter what draws attention, how empathy is allocated, and what feels socially significant. That isn’t a change in moral reasoning—it’s a change in the conditions under which moral reasoning forms.

And if that’s where the modulation happens, then a principle-first approach to moral AI can’t explain it. We cannot start with abstract theories and work downward. You have to start with the architecture of moral cognition and work upward. Moral behaviour isn’t just the outcome of applying a rule; it is the emergent trajectory of a system sculpted by perceptual salience, affective appraisal, and socially mediated cues—processes that moral psychology has shown to precede and shape explicit judgment \cite{Haidt2001,Greene2001,Decety2004,Prinz2007}. These evaluative dynamics are deeply sensitive to contextual modulation: shifts in attention, affective resonance, or perceived social presence can reconfigure the very pathway through which an agent moves from appraisal to action \cite{Conty2016}. Artificial agents, even without agency or intention, participate in this structure by perturbing the field of salience and social meaning~\cite{Kuchenbrandt2011,Malle2016,Zlotowski2015}.

So the shift we’re proposing isn’t just methodological; it’s conceptual. It marks a move away from the agenda that has shaped much of Machine Ethics over the past fifteen years—an agenda that addresses the same broad question we do, but at a Level of Abstraction concerned with how machines apply moral principles. Our aim is different. Instead of asking, 'How can machines apply moral principles?' we have to ask:

\noindent
\begin{center}
	\begin{leftbar}
		\textit{How do artificial agents alter the environment in which humans experience, interpret, and act on moral cues?}
	\end{leftbar}
\end{center}

\medskip
\noindent
That’s the question that anchors the thesis. And later, when we look at the experimental results, we’ll see why a principle-driven account simply can’t capture the effects we observe.

% -------------------------------------------------------
% Section VII — Structure of the Thesis
% -------------------------------------------------------

The argument developed so far brings us to a decisive shift. The question that will guide the remainder of the thesis is no longer whether artificial agents can execute or approximate moral principles, but how their presence reshapes the very field in which humans perceive, interpret, and respond to moral cues. This reframing closes the introduction and opens the path to the theoretical and empirical work that follows.

\medskip

\section{Structure of the Thesis}

The chapters that follow are arranged to make the implications of this shift increasingly explicit. The progression is cumulative. Each chapter establishes the conditions under which the next can be understood, and together they build a unified account of machine–mediated, machine-detactable moral cognition.

Chapter \ref{chap:lit_rev} establishes the philosophical and methodological ground of the thesis. It disentangles the two projects often grouped under Machine Ethics—Human–Machine Ethics and Computational Machine Ethics—and shows why neither operates at the cognitive Level of Abstraction (LoA) required to explain synthetic moral perturbation. Drawing on Normative Ethics, Moral Psychology, and Social Signal Processing, the chapter argues that moral behaviour arises from a salience-weighted evaluative process rather than from the application of encoded principles. Its central conclusion introduces the core tension that motivates the thesis:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Classical Machine Ethics works at the reflective LoA, while the phenomenon under investigation unfolds at the cognitive LoA, upstream of explicit moral reasoning.}
	\end{leftbar}
\end{center}

\bigskip
\noindent
Chapter \ref{chap:moral_primer} provides the conceptual architecture needed to understand moral cognition empirically. It introduces dual-process theories, the Social Intuitionist Model, affective tagging, attentional capture, and accountability structures, illustrating how these mechanisms shape the path from moral perception to action. The chapter identifies the inferential gap: \textit{the transformation from moral appraisal to moral behaviour.} This gap motivates the thesis’s central question—whether synthetic presence can perturb that transformation—and prepares the reader for a systematic account of the evaluative processes at stake.

\bigskip
\noindent
Chapter \ref{chap:tools_new} specifies the methodological infrastructure through which the thesis renders evaluative cognition empirically tractable. Whereas the previous chapters developed the theoretical topology of moral appraisal, the present chapter introduces the instruments—psychometric, dispositional, and perturbational—that operationalise that topology in experimental form. It clarifies how established constructs from Moral Psychology, Cognitive Science, Social Signal Processing, and Human-Robot Interation (HRI) serve not as neutral measurement devices but as theoretically motivated probes into the latent dispositional manifold (let us call it $\beta_C$).

By situating the Empathizing Quotient, the Systemizing Quotient, the Big Five Inventory, and the Watching–Eye paradigm within the evaluative–topological framework, the chapter attempts to show that each tool targets a distinct dimension of the architecture through which moral salience is encoded, transformed, and expressed in behaviour. Their role is therefore conceptual rather than merely procedural: these instruments define the coordinate system in which the perturbation introduced by synthetic presence becomes detectable as a deformation of the evaluative field rather than as a trait-driven behavioural fluctuation.

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{The tools introduced here provide the empirical interface between theoretical topology and behavioural data: they operationalise the dispositional term $\beta_C$ and supply the salience baselines against which synthetic perturbation can be identified.}
	\end{leftbar}
\end{center}

\bigskip
\noindent

This chapter therefore establishes the measurement logic of the thesis. It shows why these specific instruments are required to distinguish dispositional variation from field-level modulation, and how they allow the experiment to test whether humanoid robotic presence alters not who participants are, but the evaluative topology within which their moral trajectories unfold.

Chapter \ref{chap:exp_methods} constitutes the empirical core of the thesis. It operationalises the evaluative–topological model developed in the earlier chapters into a full experimental framework, integrating design, measurement, and statistical inference into a single methodological architecture. The chapter introduces the controlled observational conditions, reconstructs the Watching–Eye paradigm, and justifies the use of the NAO platform as a parametrically stable source of synthetic presence. It specifies all behavioural measures, psychometric instruments, and salience manipulations, and it details the complete analytical pipeline—from preprocessing and cluster formation to non-parametric tests, regression modelling, and Bayesian estimation.

Its function is foundational: this is the chapter in which the three central hypotheses of the thesis—Evaluative Deformation, Synthetic Normativity, and Synthetic Perturbation of Moral Inference—are formally operationalised and subjected to empirical test. By consolidating the full experimental architecture with the statistical logic required to evaluate deformation in the evaluative field, the chapter provides the decisive evidence for the thesis’ central claim: that synthetic co-presence induces a measurable, structured alteration in the mapping from moral salience to action that cannot be reduced to trait-level variation or noise.

Chapter \ref{chap:dis} interprets the experimental findings within the cognitive–topological framework developed in the earlier chapters. It examines how the robot’s silent co-presence attenuates prosocial donation and argues that this modulation is best understood as a subtle reshaping of the pre-reflective pathways through which moral salience is registered and transformed into action. The chapter situates this effect within intuitionist Moral Psychology, integrates it with Floridi’s Levels of 
Abstraction, and contrasts the resulting ecological account of moral influence with agent-centred approaches in Machine Ethics. It concludes by outlining the conceptual, empirical, and governance implications of treating synthetic presence as a perturbation of human moral environments rather than as a candidate locus of 
moral agency.

Chapter \ref{chap:ethics_s} reconstructs the major normative traditions— deontological, consequentialist, virtue-theoretic, sentimentalist, contractualist, particularist, and pluralist—at the Level of Abstraction (LoA) appropriate to the aims of the thesis. The chapter distinguishes their reflective and justificatory role from the cognitive–affective mechanisms that generate everyday moral behaviour, and uses Floridi’s LoA framework together with the evaluative–topological model developed earlier to interpret these theories not as implementable rule systems but as structural patterns—constraints, value gradients, dispositional tendencies, and 
affective vectors—that organise how moral evaluation is formed and directed.  

This reconstruction serves a methodological purpose: it provides the normative coordinates required to assess the experimental perturbation ethically rather than merely descriptively, and it establishes the conceptual infrastructure that renders 
the thesis’s hypotheses and their interpretation coherent within both normative theory and cognitive science.


Chapter \ref{chap:general_discussion} provides the structural integration of the thesis. It unifies the cognitive–affective architecture, the normative analyses, and the experimental findings into a single theoretical account of how synthetic presence perturbs moral cognition. Building on the experimental result—uniform attenuation of prosocial donation under humanoid co-presence—the chapter attempts to show that the effect cannot be understood as a trait-level phenomenon, a local behavioural anomaly, or a deficit of explicit reasoning. Instead, it requires a field-level interpretation: synthetic presence deforms the evaluative topology that ordinarily carries moral salience into action. By bringing together the three dispositional ecologies, the topological formalism, the reconstructed normative frameworks, and Floridi’s LoA analysis, the chapter argues that the humanoid robot operates as a perturbation operator on the moral field, not as an ethical agent. Its role is therefore interpretive: it provides a theoretical lens through which the behavioural pattern observed in the data can be read as one possible indication of underlying evaluative dynamics, while also clarifying the kinds of questions that exceed the methodological reach of Machine Ethics. In this sense, the framework suggests how synthetic presence might be understood as a perturbation of moral appraisal without implying that the experiment discloses the full structure of moral cognition.

Taken together, these chapters form a cumulative argumentative trajectory. Each chapter establishes the conditions of intelligibility for the next, guiding the reader from conceptual reframing to cognitive mechanism, from mechanism to experimental design, from empirical outcome to theoretical explanation. The result is a systematic account of how synthetic presence perturbs human moral cognition and what this means for the future of moral AI.