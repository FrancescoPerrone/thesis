\chapter{Discussions}
\label{chap:discussions}
\thispagestyle{pprintTitle}

\section{Reframing the Findings: Moral Cognition Under Synthetic Co-Presence}

The experimental results developed in the previous chapter permit a substantive 
reassessment of how moral behaviour is shaped by the structure of the environment in 
which it unfolds. Throughout this thesis, I have treated moral decision-making not as a 
single cognitive act but as a sequence of evaluative transitions: from perceptual 
salience, to affectively weighted appraisal, to the formation of a behaviourally 
actionable moral judgment. This sequence, as argued in Chapter~\ref{chap:introduction}, is 
sensitive to small variations in context---to subtle shifts in tone, posture, and the 
broader semiotic landscape within which an agent finds themselves. What appears to be a 
stable moral conclusion is often the endpoint of a dynamic evaluative pathway in which 
social and perceptual cues act as modulators long before explicit reasoning enters the 
scene.

The present experiment was designed to probe one particular form of modulation: the 
influence of a \emph{synthetic presence} whose behavioural expressivity is minimal yet 
perceptually salient. The NAO robot, in its silent and non-interactive configuration, 
does not present reasons, issue commands, or initiate social exchange. Instead, it 
occupies an ambiguous ontological position---a position that is neither that of a 
conventional object nor that of a recognisable social agent. In Floridi's terms, it 
constitutes a \emph{semantic body} situated at a specific Level of Abstraction (LoA), one 
that carries informational affordances precisely because of how it appears and is 
interpreted, not because of any intrinsic mental properties it might possess.

The central question of this Discussion is: \emph{what do the observed behavioural 
	perturbations reveal about the evaluative architecture through which moral salience is 
	converted into action?} The attenuation detected in the Robot condition, while modest in 
absolute terms, is statistically meaningful and theoretically instructive. Its presence 
demonstrates that moral cognition is structurally permeable to synthetic forms of social 
salience. The fact that this influence emerges in the absence of interaction suggests that 
its origin is not motivational suppression or empathic fatigue, but a deformation in the 
topology of the evaluative field itself.

In what follows, I weave together the empirical findings, the mathematical formalism, and 
the philosophical commitments articulated in the earlier chapters to provide a unified 
interpretive account of this deformation. The aim is not merely to summarise the results 
but to situate them within a broader explanatory framework that respects the complexities 
of moral psychology while avoiding anthropomorphic assumptions about artificial systems. 
The discussion proceeds from the structural---how the evaluative mapping was perturbed---to 
the dispositional---how this perturbation varied across latent cognitive--affective 
profiles---and finally to the ontological, where I examine what these findings imply for 
synthetic normativity and the boundaries of moral agency in human--robot interaction.

Throughout, I adopt a stance of empirical restraint. The modest effect sizes, the 
heterogeneity observed across individuals, and the inferential limits of the design are 
treated not as weaknesses but as constraints within which a more precise and philosophically 
disciplined interpretation can be articulated. Within those constraints, the results offer 
a rare opportunity: they reveal how the moral field can be deformed not by explicit 
manipulation or argument, but by the presence of a synthetic entity whose normative force 
arises from its perceived ontology alone.

This chapter therefore takes as its starting point the notion that the robot functions as a 
\emph{perturbative operator} within the cognitive ecology of moral appraisal. It does not 
override moral reasoning; it refracts it. It does not diminish empathy; it redistributes 
evaluative weight across competing sources of salience. And it does not introduce new 
norms; it modifies the topology through which pre-existing moral cues are interpreted. The 
discussion that follows builds upon this conceptual foundation to make sense of the 
empirical signature observed in the experiment and to draw out its implications for moral 
psychology, human--robot interaction, and the broader landscape of machine ethics.
