\chapter{Literature Review: Existing Approaches and the Level-of-Abstraction Problem}
\label{chap:lit_rev}

% Section 1
\section{Introduction: Scope, Objectives, and Theoretical Commitments}

Machine Ethics is often described as the branch of Artificial Intelligence concerned with building machines that can “act ethically.” But that description, while technically correct, misses the more interesting question—the one that quietly motivates the field~\cite{Moor2006,Wallach2008,Anderson2011}.

At its core, Machine Ethics tries to understand what happens when artificial systems begin to participate in the moral situations humans inhabit—not in the abstract, not in the controlled space
of philosophical thought experiments, but in the ordinary, textured environments where people make decisions, respond to one another, and register the subtle pressures of social life~\cite{Coeckelbergh2010,Floridi2013,Gunkel2012}.

What has changed in the past decade is not merely the level of attention the field receives, but the kind of attention. Researchers in Machine Ethics now draw on Psychology, Cognitive Science, Philosophy, HRI, Affective Computing, and Behavioural 
Economics—not as optional glosses, but as integral sources of evidence about how moral judgment actually works~\cite{Malle2016,VanStraten2020,Banks2020,Vinciarelli2009,Picard1997,FehrGachter2002,Andreoni1990}. A parallel shift has been driven by the rise of large language models, which make it possible to examine moral cognition not only through human behaviour but also through models trained on large-scale moral and social corpora~\cite{Jiang2021,Noothigattu2021,Ramirez2023,Bender2021}.
 While these systems do not replicate human judgment, they provide a 
complementary lens for testing how moral cues are represented, transformed, or misrepresented in artificial agents. For the first time, the field can move beyond purely theoretical constructions and test its assumptions against empirical data that 
reveal how humans negotiate moral space in the presence of artificial systems~\cite{Zlotowski2015,Groom2010,Breazeal2003,Fischer2011}.


In this sense, Machine Ethics has become something like a laboratory for Moral Philosophy. Ideas that once lived comfortably as small-scale thought experiments—trolley problems, dilemmas of
autonomy, abstract accounts of agency—\textit{must now} be examined under conditions where artificial systems interact with real people, in real environments, with real consequences~\cite{Doris2015,Greene2014,SinnottArmstrong2008,Malle2016}. The theories that survive this transition do so
not because they are elegant, but because they remain intact when exposed to the messiness of human moral cognition~\cite{Haidt2001,Cushman2013,Greene2001}. And this is precisely where the field finds both its opportunity and its challenge. 

The question is no longer only how to encode moral principles into machines \cite{Moor2006,Anderson2011}. It is how to
understand the ways in which artificial systems reshape the moral landscape itself—the cues people attend to, the expectations they carry, and the background sense of what is appropriate or
permissible~\cite{FloridiSanders2004,Floridi2013,Coeckelbergh2020}. Machine Ethics, in this broader and more mature form, is not simply about building ethical machines. It is about understanding how machines and humans co-construct the environments in which moral meaning is formed~\cite{Rahwan2019,Banks2020,Shank2019}.

Together, these developments mark a turning point. Machine Ethics has expanded from a largely theoretical inquiry into a domain where philosophical models, behavioural evidence, and computational systems now interact. Yet precisely because the field has grown in scope, it has also grown in complexity: the assumptions embedded in its frameworks, the \textit{Levels of Abstraction (LoA)} at which its explanations operate, and the mechanisms they presuppose are no longer uniform. Before we can interpret the experiment that follows in Chapter~\ref{chap:exp_methods}, we must therefore clarify the conceptual terrain on which such interpretations depend.

This chapter establishes that terrain. It examines the frameworks that claim to explain how artificial systems acquire moral relevance and identifies the points at which they illuminate—or obscure—the phenomenon under investigation. This review is not background filler; it is the first test of the assumptions on which 
the later analysis relies: the LoAs they occupy, the cognitive processes they take for granted, and the gaps they leave unexplained. It lets us ask:

\noindent
\begin{center}
	\begin{leftbar}
		\textit{Whether synthetic presence really does modulate the path from perception to action, which existing frameworks can even register that phenomenon—and which ones are structurally blind to it?}
	\end{leftbar}
\end{center}

\noindent
By examining the published work through that lens, we start to see an emerging pattern: almost all of classical Machine Ethics operates at the reflective level—principles, rules, deliberation—while the phenomenon we are studying unfolds at the cognitive level, upstream of reasoning. That mismatch isn’t an opinion; it’s a structural finding that the literature itself reveals.

\bigskip
\noindent
The aim here is therefore to reposition the study of moral behaviour under artificial co-presence---and the design of artificial moral systems more broadly---within a theoretically unified space at the intersection of \emph{Machine Ethics}, \emph{Computational Morality}, and \emph{Social Signal Processing} (SSP). Although these fields emerged from distinct disciplinary lineages, the experimental results presented in Chapter~\ref{chap:exp_methods} show that they now converge around a single problem: artificial agents, even when silent, passive, and non-interactive, \emph{modulate the evaluative conditions under which moral judgment and action unfold}. Understanding this phenomenon requires an integration of normative philosophy, moral psychology, computational modelling, and HRI.

Hence, the project takes root here. The literature review is the first piece of evidence. It shows that if we stay at the reflective level, we can’t even formulate the right kind of question, let alone explain the modulation we later observe experimentally (Chapter~\ref{chap:exp_methods}). That’s why the review matters so much—it’s the tool that tells us where the explanation has to live before you collect single data points.

\noindent
One of the core findings of the literature is that classical Machine Ethics starts from the wrong end of the problem. The whole tradition begins by taking high-level ethical theories—Kantian tests, utilitarian calculations, virtue templates, deontic logics—and trying to encode them as if they were models of moral agency~\cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008,Guarini2006,Allen2005}.

\medskip
\noindent
But if we look closely at what those theories actually do, they are not descriptions of how humans produce moral behaviour. They are descriptions of how humans \emph{justify} moral behaviour after the fact. This distinction is explicit in modern moral philosophy: Kantian universalisability, utilitarian aggregation, and contractualist justification articulate reflective standards for assessing reasons, not cognitive processes for generating action \cite{SidgwickMethods,Scanlon1998,Korsgaard1996}. They operate at a very high Level of Abstraction: they tell you what counts as a good reason, \textit{not how a person comes to act in the first place} \cite{Floridi2008,Floridi2011}.

It should be noted that while most of what traditionally falls under Machine Ethics—Computational Morality, formal deontic systems, encoded utility functions—belongs to the “\textit{pre-LLM}’’ era, the limitation identified here does not evaporate with the advent of large language models. If anything, the arrival of LLMs makes the limitation more sharply visible.

\medskip
\noindent
Recent work demonstrates that LLMs can perform exceptionally well on reflective moral tasks: they generate sophisticated reasoning, balance competing principles, and provide normatively articulate justifications that map cleanly onto established ethical frameworks \cite{Jin2022Moral,Scherrer2023,Nguyen2023,Aher2023,Charlton2023}. They also exhibit high performance on benchmarked moral analogy tasks and moral classification challenges \cite{Hendrycks2021,Emelin2023}. But all of this ability is situated at the reflective Level of Abstraction: the linguistic, justificatory, post-hoc LoA.

And humans do not act morally at that level. On every empirically supported account of moral cognition—from social intuitionism \cite{Haidt2001,Haidt2007}, to dual-process theory \cite{Greene2001,Greene2014,Cushman2013}, to affective neuroscience \cite{Phelps2006,Decety2008,Zaki2012}, to embodied and socially embedded models \cite{Buon2016,Malle2016,Bremner2022}—moral behaviour is driven by salience, affect, perceptual appraisal, social cues, and attentional orientation, not by the explicit application of normative principles. These processes sit one LoA below the linguistic–justificatory space in which LLMs operate.

Thus, although we now live in a “post-LLM’’ era, the fundamental issue is not that pre-LLM Machine Ethics was technically limited or symbolically brittle. The deeper problem is that both pre-LLM Machine Ethics and modern LLMs operate at the wrong Level of Abstraction if the goal is to model, predict, or understand human moral behaviour. This is precisely the mismatch Floridi’s LoA discipline is designed to diagnose \cite{Floridi2008,Floridi2011}: moral justification and moral production belong to different descriptive orders. LLMs amplify the upper order; they leave the generative order untouched.

Chronologically, the pattern is straightforward:

\begin{itemize}
	\item \textbf{Pre-LLM Machine Ethics} attempted to encode normative principles directly—deontic rules, utility functions, virtue schemas—and encountered the reflective/cognitive mismatch documented extensively in the literature \cite{Moor2006,Wallach2008,Cervantes2020,Coeckelbergh2023}.
	\item \textbf{Post-LLM models} generate better principles, better explanations, and more articulate moral rhetoric, but they encounter the same mismatch, now at a higher level of linguistic sophistication \cite{Bender2021,Mittelstadt2019,Whittlestone2019,Andrus2023,Kasirzadeh2023}.
\end{itemize}

The chronology therefore does not mark a methodological revolution; it exposes the persistence of a category error. The assumption that moral behaviour is fundamentally a matter of reasoning or principle-application has survived unchallenged into the LLM era. But contemporary empirical evidence shows that humans rarely deploy such reasoning in the production of moral action \cite{Cushman2013,Haidt2001,Young2012}.

As several recent critical analyses emphasise, LLMs produce moral reasoning without moral cognition \cite{Kasirzadeh2023,Andrus2023,Gardner2024}. They resolve dilemmas fluently~\footnote{The distinction between reflective and generative Levels of Abstraction (LoAs) is crucial here. Moral justification, principle-balancing, and linguistic explanation occur at a reflective LoA \cite{Floridi2008,Floridi2011}. Human moral behaviour, by contrast, arises from perceptual, affective, and socially embedded processes documented across moral psychology and social neuroscience \cite{Haidt2001,Greene2001,Zaki2012,Decety2008}. Recent analyses of LLM-based moral reasoning confirm that these models excel at reflective justification but do not reproduce the generative cognitive–affective mechanisms that produce moral action \cite{Kasirzadeh2023,Andrus2023,Gardner2024}. The arrival of LLMs therefore intensifies—rather than resolves—the LoA mismatch at the core of Machine Ethics.}; they do not reproduce the cognitive–affective processes by which humans come to feel that something is a dilemma in the first place. Moral language is not moral experience. Reflective justification is not perceptual-affective appraisal.

If there is a chronological lesson, it is this: the technologies have changed, but the Level-of-Abstraction mismatch has not. The surface of Machine Ethics has shifted, yet the underlying category error remains fixed. And this is the hinge of the present work: the decisive starting question is not how well artificial systems can perform reflective moral reasoning, but how their presence intervenes in the pre-reflective, perceptual–affective processes that carry moral salience into action. It is this generative layer—not the reflective one—that the experiment 
must speak to. And the decisive shift is this: artificial systems no longer occupy only the reflective space in which their moral language lives. They enter the very environments that scaffold human moral perception—phones, rooms, shared spaces—altering what is noticed, how situations feel, and how moral salience flows~\cite{WaytzHeafnerEpley2014,Eyssel2012,Fischer2011,Kim2023,Wirtz2018,Banks2020}. Presence, not reasoning, becomes the point of intervention~\cite{Rahwan2019,Shank2019}.

So the shift isn’t from ‘pre-LLM Machine Ethics’ to ‘post-LLM Machine Ethics.’ The shift is from seeing AI as an agent that reasons to seeing AI as an element in the cognitive ecology—something that reshapes the conditions in which human moral behaviour unfolds. Whether the system speaks like Kant or Shakespeare or your best friend is irrelevant if its presence still modulates the way people notice, feel, and act. That’s the axis that matters. That is the core of this work. And this is where the category error comes in. Machine Ethics often proceeds by treating the principles of an ethical theory as if they could stand in for the cognitive machinery of a moral agent—\textit{as though human behaviour were governed by internal procedures resembling Kantian tests or utilitarian calculations}. But we know that isn’t how moral action is produced. Human behaviour comes from a much lower level: from what captures our attention, what feels salient, how we read a face or a tone, how empathy gets triggered, how the context shifts our sense of what matters. These processes are fast, intuitive, emotional, and deeply social~\cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}.

Decades of work in moral psychology and neuroscience demonstrate that intuitive, affectively laden processes precede and shape explicit moral judgement \cite{Greene2004,Haidt2001,Moll2002}. The intuitive, affectively charged processes come first \cite{Haidt2001,Greene2001,Decety2004,Cushman2013}. They shape the space in which explicit reasoning even becomes possible: before reflection begins, appraisal mechanisms, empathic resonance, salience attribution, and motivational tagging have already constrained the field of viable responses \cite{Moll2002,Crockett2016,Prinz2007}. The reflective story we tell afterwards might be coherent, but it is downstream of the machinery that actually drives behaviour \cite{Greene2004,Haidt2001}.

So when Machine Ethics takes ethical principles and treats them as if they were the generator of moral action, it is working at the wrong level entirely. It is replacing the justification of moral behaviour with the mechanism of moral behaviour, and those are not the same thing \cite{Korsgaard1996,Scanlon1998,Foot2001}. High-level principles articulate normative standards, but the processes that produce moral action operate at a far more fundamental cognitive--affective level \cite{Floridi2008,Floridi2011}.

\noindent
So when one tries to design a “moral machine” by encoding Kant or utilitarianism, one collapses these two levels of abstraction. One is treating reflective principles as if they were psychological mechanisms. And the literature shows very clearly that they are not. Ethical theories explain why an action can be defended; they do not explain how moral behaviour is formed \cite{Prinz2007,Foot2001}. 

\noindent
The literature review therefore brings a particular limitation into focus. Classical Machine Ethics is methodologically refined and theoretically coherent, but it is oriented toward a Level of Abstraction that does not readily register the phenomenon 
examined here. As work in Moral Psychology, Affective Neuroscience, Social Signal Processing (SSP), and HRI indicates, the dynamics most relevant to our question arise within the evaluative substrate of salience, affect, and social interpretation—not primarily within the reflective principles articulated after the fact ~\cite{Vinciarelli2009,Kuchenbrandt2011,Malle2016,Zlotowski2015}.

\noindent
Classical Machine Ethics remains an elegant and logically disciplined project, yet it approaches moral agency through principles, rules, and reflective argumentation ~\cite{Bringsjord2006,Anderson2011,Wallach2008,Arkin2009}. By contrast, the processes that shape immediate moral behaviour appear to operate at an earlier level of appraisal, where salience, emotion, attention, and social interpretation structure what becomes behaviourally relevant ~\cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}. In this sense, the approaches are not in conflict but address different layers of moral cognition, and only one of them engages the generative processes at issue in this thesis.

\noindent
If we attend to the wrong layer of analysis, the phenomenon simply remains invisible. This is why the same difficulty appears in what is now called Computational Morality. Whether implemented through logic engines, preference aggregators, or LLM-based moral modelling, these approaches treat moral judgment as a problem of symbolic inference—as if the structure of behaviour could be captured by procedures for generating reasons \cite{Guarini2006,Allen2005,
	Arkoudas2005,Mittelstadt2019,Bender2021}.

\noindent
Empirical research in Moral Psychology, Affective Neuroscience, SSP, and HRI offers a different picture. Moral judgments typically arise from fast, intuitive, and affectively charged appraisal \cite{Haidt2001,Greene2004,Cushman2013}. They are shaped by presence, gaze, tone, perceived stake, and the social and affective 
cues embedded in the environment~\cite{Decety2004,Conty2016,Vinciarelli2009,Zlotowski2015}. These processes are context-sensitive and dynamically assembled 
\cite{Moll2002,Prinz2007}. When morality is modelled as a chain of propositions—if A then B, if C then D—the generative machinery that produces behaviour is abstracted away. And it is precisely this machinery that our experiment is designed to probe, given that even silent humanoid co-presence can shift the evaluative conditions in which action is selected \cite{Kuchenbrandt2011, Malle2016,Bremner2022}.

In other words, the classical computational models of Machine Ethics are not limited by deficiencies in their logical structure; rather, they address a different component of the moral process than the one examined here. They aim to model explicit reasoning, while much of the behaviourally consequential activity appears to unfold within the evaluative landscape that precedes reflective judgment. It is within this substrate of salience, affect, and social interpretation that synthetic presence is most plausibly understood to exert its influence.


In Chapter~\ref{chap:moral_primer} we make very explicit that: \textit{any model of moral behaviour that leaves out the cognitive–affective machinery and the social-signalling dynamics behind moral judgment is simply not describing human beings.} It becomes unstable both scientifically and philosophically. This is where the Level of Abstraction issue gets predominant. If we would take high-level moral theories—the reflective content, the principles, the rules—and treat them as if they were the psychological mechanism that produces moral behaviour, we would end up with theories that look elegant but don’t actually predict what people do. They explain justification, not behaviour. We would develop artefacts; models that fail not because the logic is wrong, but because they’re modelling the wrong layer of the system. This becomes plainly clear in the experiment in Chapter~\ref{chap:exp_methods}.

\noindent
As we will see that the robot employed in this study has no beliefs, goals, intentions, or communicative acts; it is not reasoning or attempting to influence participants. Prior work in HRI and social psychology indicates that even minimally expressive 
robots can modulate certain aspects of human social behaviour 
\cite{Kuchenbrandt2011,Bremner2022,Bainbridge2011,Malle2016}. What remains less well understood—and what the present thesis investigates—is how such synthetic presence interacts with the evaluative processes specific to moral judgement. These dynamics involve shifts in salience, attention, and perceived social 
presence rather than changes in explicit reasoning 
\cite{HaleyFessler2005,Bateson2006,Phelps2006,Zaki2012}. Because such processes operate upstream of deliberation, they fall outside the scope of rule-based, utility-theoretic, or propositional models of morality, which focus on reflective justification rather than the intuitive, affective systems documented across moral psychology \cite{Haidt2001,Greene2014,Cushman2013,Dancy2004,Cervantes2020}.

\noindent
At this stage, the literature points to a difficulty that classical Machine Ethics has not fully resolved. If the aim is to understand how humans behave morally in the presence of artificial agents—and to model those behaviours in a form that artificial systems could eventually operationalise—then the foundational assumptions of principle-first approaches require re-examination. Deontic, 
Utilitarian, and Virtue-theoretic models presuppose that moral norms can be rendered as explicit rules or evaluative operators ~\cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Yet empirical research in Moral Psychology and Affective Neuroscience indicates that morally relevant behaviour typically emerges from cognitively embedded processes of appraisal, salience detection, affective resonance, and social interpretation ~\cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}. These findings do not contradict normative theories, but they suggest that reflective principles capture only the justificatory layer of moral cognition, not the generative processes on which the present thesis focuses.

\noindent
From this perspective, moral norms are not best understood as rules to be encoded, but as structural patterns that organise evaluation. They shape constraints, emphases, and trajectories within the space of moral appraisal, operating at a reflective Level of Abstraction that specifies justificatory standards rather than the cognitive mechanisms that produce behaviour ~\cite{Korsgaard1996,Scanlon1998,Floridi2008,Floridi2011}. Their influence depends on how they interact with, and are realised through, lower-level processes of perception, affect, and social interpretation~\cite{Foot2001,Aristotle_nicomachean,Prinz2007}.

\noindent
For similar reasons, moral judgement cannot be modelled as pure reasoning or symbolic inference. Dual-process and intuitionist models suggest that intuitive, affectively charged appraisals precede reflective deliberation and constrain the space of admissible reasons \cite{Greene2004,Haidt2001,Cushman2013}. Attention, empathic resonance, perceptual salience, and social–contextual modulation shape the evaluative landscape long before propositional reasoning becomes active ~\cite{Moll2002,Decety2004,Vinciarelli2009}. These dynamics provide the psychological foundation for the account developed in Chapter~\ref{chap:moral_primer}.

\noindent
Nor can artificial agents be understood as carriers or executors of moral values. Work in HRI and Social Signal Processing suggests that artificial systems often function as \emph{modulators} within the environment, shaping patterns of 
salience, perceived social presence, accountability cues, and evaluative expectations~\cite{Kuchenbrandt2011,Malle2016,Zlotowski2015,Bremner2022}. These influences appear to operate upstream of explicit judgment, within the same cognitive–affective processes that organise the evaluative conditions under which moral decisions are formed.

\noindent
Through this reframing, the literature review achieves a clear result: it exposes a fundamental LoA mismatch at the heart of Machine Ethics and shows that no principle-first, rule-codification framework can access the phenomena under investigation. Moral norms operate at a reflective LoA, specifying justificatory relations \cite{Korsgaard1996,Scanlon1998}, whereas moral behaviour is produced at the cognitive LoA through the dynamic interplay of affect, salience, and social interpretation. By bringing these strands together, the review establishes an integrated conceptual framework in which \emph{synthetic presence} becomes intelligible as a perturbation of the evaluative field itself—a theoretical insight that classical Machine Ethics could not formulate, and a necessary foundation for interpreting the empirical results of this thesis.




\section{Two Levels of Abstraction in Machine Ethics}

What the literature refers to as “Machine Ethics” encompasses two distinct research programmes that share a name but operate at different Levels of Abstraction \cite{Floridi2008,Floridi2011}. One approaches moral behaviour as a computational problem and asks how ethical principles might be implemented in artificial agents. The other investigates how artificial systems participate in, 
and sometimes perturb, the situations in which human moral behaviour unfolds. Their conflation is understandable, but it obscures the distinct explanatory aims of each approach and makes it less clear which framework can address the phenomenon investigated in this thesis.

\noindent
To make this distinction explicit, the section introduces each programme in turn, before clarifying why the present work aligns with one rather than the other.

\noindent
The first research programme is what may be called \emph{Human--Machine Ethics}. This strand examines how humans think, feel, and behave in the presence of artificial agents. It encompasses questions of accountability, agency displacement, social influence, norm perception, and moral risk. Its empirical 
foundation comes from Human--Robot Interaction, Media Psychology, and Social Signal Processing. Work in these areas suggests that artificial systems---whether humanoid robots, embodied agents, or even minimally interactive media---can modulate attention, empathy, prosociality, and interpersonal expectations through 
their mere presence~\cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,
	Malle2016,Bremner2022,Zlotowski2015}. This programme therefore aligns closely with the phenomenon examined in this thesis: the possibility that a robot’s silent co-presence perturbs the evaluative processes through which moral behaviour is generated.

\noindent
The second programme is \emph{Computational Machine Ethics}. This project seeks to design systems that make ethically adequate decisions by embedding moral theories into computational architectures. Deontic logics, Utilitarian optimisation engines, rule-based governors, and virtue-theoretic templates fall 
under this category~\cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008,Guarini2006,Allen2005}. Here, moral judgement is commonly treated as a reasoning problem: behaviour is modelled as the outcome of applying principles, performing symbolic inference, or satisfying constraints. In this respect, Computational Machine Ethics addresses a different explanatory target than the one motivating the present study. Although the literature often treats these two strands as if progress in one should inform the other, they operate at different Levels of Abstraction and address different questions.

Human--Machine Ethics investigates how artificial 
systems modulate human evaluative processes, whereas Computational Machine Ethics aims to formalise normative content in ways that could guide artificial agents. Both domains contribute to the broader landscape, but they do so from distinct conceptual vantage points.

\noindent
The empirical findings of this thesis highlight why maintaining this distinction is important. Research in Psychology and HRI indicates that artificial systems can influence patterns of attention, salience, and social interpretation~\cite{Conty2016,Decety2004,Haidt2001}. Such influences operate within the pre-reflective evaluative processes that shape moral behaviour. By contrast, Computational Machine Ethics focuses on reflective, principle-driven models of 
ethical decision-making, and therefore does not readily accommodate phenomena that arise upstream of deliberation~\cite{Greene2001,Greene2004,Crockett2016,
	Prinz2007}. The approaches are thus not incompatible, but they illuminate different components of moral cognition.

\noindent
Seen in this light, the apparent lack of unity in “Machine Ethics” reflects the field’s underlying conceptual structure rather than a terminological accident. One strand is empirically grounded, concerned with how humans behave in sociotechnical environments; the other is formally oriented, concerned with how ethical principles might be encoded into artificial agents. The present work belongs to the former: it examines how artificial systems, even when passive, may \emph{modulate the evaluative field} within which human moral decisions take shape.

\section{Clarifying the Explanatory Level of the Present Work}

\noindent
It is natural to ask how this research should be situated. It touches Affective Computing, with its emphasis on modelling emotion computationally~\cite{Picard1997}; Human--Robot Interaction, where the behavioural consequences of artificial social agents are examined \cite{Kuchenbrandt2011,Malle2016,Bremner2022}; and Moral Psychology, which analyses the cognitive and affective substrates of moral behaviour~\cite{Haidt2001,Greene2001,Decety2004,Crockett2016}. Each contributes something essential, yet none, on its own, provides the conceptual resources needed to address the phenomenon at stake. For the purposes of this thesis, the disciplinary label is secondary; the priority is to clarify the explanatory level at which the question must be framed.

\noindent
The central confusion this thesis confronts is therefore not empirical but conceptual. Work collected under the name “Machine Ethics” has often blurred two distinct enterprises: understanding how humans behave morally in sociotechnical settings, and designing machines that behave in accordance with encoded ethical theories. These projects operate at different Levels of Abstraction~\cite{Floridi2008,Floridi2011}, draw on different forms of evidence, and address different explanatory aims. Treating them as a single field has created a methodological entanglement in which normative elegance can obscure the generative processes that empirical research suggests are crucial for understanding moral behaviour in human--robot contexts.

\noindent
Applying the LoA distinction clarifies the source of the confusion. Human moral behaviour unfolds at the cognitive LoA: it is shaped by perceptual salience, affective resonance, attentional dynamics, and social-cue interpretation~\cite{Haidt2001,Greene2004,Decety2004,Conty2016}. Normative theories—deontological, utilitarian, contractualist—operate at a reflective LoA concerned with justification rather than generation~\cite{Korsgaard1996,Scanlon1998}. Treating high-LoA principles as if they functioned as low-LoA psychological mechanisms 
tends to obscure the evaluative processes from which behaviour emerges.

\noindent
The experimental findings of this thesis are consistent with this distinction. A humanoid robot with no beliefs, goals, or communicative acts nonetheless appears to influence the evaluative conditions under which participants move from moral perception to prosocial action. Such modulation is more plausibly associated with 
changes in salience, affective alignment, and attentional orientation than with reflective reasoning~\cite{Greene2001,Decety2004,Conty2016}. Frameworks that 
model moral action primarily as rule retrieval, utility computation, or principle execution therefore address a different LoA and do not readily accommodate these upstream dynamics.

\noindent
This is why the disciplinary categorisation of the work is secondary. The issue is not where the research should be filed, but what becomes visible once the LoA distinction is applied. Through this lens, the field divides into two legitimate 
but distinct activities: \emph{Human--Machine Ethics}, an empirically grounded inquiry into how artificial agents modulate human evaluative processes~\cite{Vinciarelli2009,Bremner2022,Zlotowski2015}; and \emph{Computational Machine Ethics}, a reflective programme concerned with principled design, drawing on formalisms such as Deontic Logic \cite{Bringsjord2006,Anderson2011}, Utility 
Optimisation \cite{Arkin2009}, and Virtue-theoretic modelling \cite{Wallach2008}. Maintaining this distinction clarifies the explanatory space in which the present work is situated: it investigates how artificial systems, even when passive, may modulate the evaluative field within which human moral decisions take shape.

\noindent
Clarifying the Levels of Abstraction is only a first step. Once the distinction is restored, a different research agenda comes into view. Moral behaviour is not well understood as the output of rule application or principle execution; it is better characterised as emerging from a dynamic evaluative field shaped by affective gradients, perceptual cues, attentional flows, and socially mediated 
expectations \cite{Haidt2001,Greene2004,Decety2004,Vinciarelli2009}. Artificial agents—robots, avatars, conversational AIs—can interact with this field simply by being present. From this perspective, a productive programme for moral AI begins not with ethics construed as a set of principles, but with the cognitive and affective architecture through which moral perception becomes action.

\noindent
Several methodological implications follow from this shift in perspective. \textbf{First}, empirical grounding becomes essential. Any account of moral behaviour must engage with findings from Moral Psychology, Affective Neuroscience, Developmental Research, HRI, and Social Signal Processing. A model that cannot accommodate the influence of gaze, posture, co-presence, or anthropomorphic cues risks omitting mechanisms that empirical work suggests are 
behaviourally significant \cite{Haley2005,Bateson2006,Conty2016}. \textbf{Second}, artificial agents are more usefully modelled as operators within the evaluative environment rather than as reasoners applying rules: their influence appears to arise from how they modulate the conditions under which humans act~\cite{Kuchenbrandt2011,Zlotowski2015,Malle2016}. \textbf{Third}, normative theory is best interpreted in structural rather than procedural terms: norms describe constraints, gradients, and dispositions within the evaluative space through 
which behaviour unfolds~\cite{Foot2001,Aristotle_nicomachean,Prinz2007}. 

\noindent
This reframing also clarifies a question that often arises in engineering contexts: what, if anything, is the practical implication? The implication is not a new ethical theory to encode, nor a list of principles to implement. Rather, it is the recognition that artificial agents influence human moral behaviour primarily through their presence—by shaping salience, attention, and social 
interpretation—rather than through argument or explicit reasoning. Ignoring these evaluative processes risks overlooking the very mechanisms through which artificial systems can come to matter morally.

\noindent
From this perspective, the future development of moral AI is less about constructing machines that reason like moral philosophers and more about understanding how artificial systems participate in the evaluative conditions that guide human action. A coherent research programme must therefore attend to the perceptual, affective, and social dynamics through which moral behaviour is generated. In bringing the LoA distinction to bear on the literature, this 
section has identified the conceptual terrain on which such a programme can be built and has clarified why the generative processes of moral cognition—not their reflectivejustifications—form the appropriate locus of explanation for the phenomenon investigated in this thesis.



% Section 3
\section{The Cognitive--Affective Foundations of Moral Judgment}

\noindent
Once the conceptual landscape is clarified, the next step is to start considering the machinery from which moral behaviour is generated. Here the empirical picture is not merely extensive but strikingly consistent. Across moral psychology, affective neuroscience, and behavioural science, evidence suggests that moral 
judgment is shaped primarily by fast, intuitive, emotionally charged processes~\cite{Haidt2001,Greene2001,Greene2004}. These processes respond to perceptual salience, attentional capture, empathic resonance, and situational cues long before reflective reasoning is engaged. Slower, deliberative processes do play a role, but typically by refining, justifying, or overriding an appraisal that has already been formed \cite{Cushman2013,Decety2004,Crockett2016}. From a cognitive LoA perspective, it is this intuitive–affective layer that performs the primary generative work in moral cognition.

\noindent
The major theoretical frameworks in the field converge on this interpretation. Haidt’s Social Intuitionist Model \cite{Haidt2001}, Greene’s neurocognitive dual-process account~\cite{Greene2001,Greene2004,Greene2014}, and Cushman’s 
action-based inference models \cite{Cushman2013} each propose that moral evaluation begins with rapid, affectively valenced appraisals, with reflective reasoning entering later and often retrospectively. Neuroscientific work supports this view: affective tagging, motivational relevance, empathy circuitry, and social-interpretive processes are recruited early in the perceptual stream~\cite{Moll2002,Decety2004,Conty2016}. These findings locate 
the generative locus of moral cognition firmly within the evaluative processes that precede explicit judgment.

\noindent
This cognitive--affective picture stands in contrast to the philosophical traditions on which classical Machine Ethics has relied. Kantian ethics, utilitarian frameworks, and contractualism articulate \emph{justificatory} structures—conditions of universalisability, procedures for aggregating value, 
or standards of interpersonal reason-giving~\cite{Korsgaard1996,Scanlon1998}. These theories operate at a reflective Level of Abstraction: they describe how 
actions can be defended, not the psychological mechanisms through which moral judgments arise.

\noindent
Classical Machine Ethics, however, adopted only this reflective dimension and treated it as if it described the generative architecture of moral agency. It presumed that humans behave by applying principles and that artificial systems could do likewise by encoding those principles into computational structures ~\cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Empirical research suggests a different picture: moral behaviour appears to emerge from a cognitive--affective substrate shaped by salience, emotion, attention, embodiment, and social interpretation rather than from the direct application of explicit rules.


\noindent
This perspective helps explain why studies of human moral behaviour in context—across HRI, Media Psychology, and Social Signal Processing—consistently identify patterns governed by attentional capture, affective resonance, perceived 
monitoring, and contextual meaning~\cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,Malle2016}. A clear illustration is the Watching--Eye effect: minimal cues of observation, even stylised eyes, can modulate prosocial behaviour \cite{Haley2005,Bateson2006,Dear2019}. Such shifts do not reflect the endorsement of principles but subtle environmental modulation of the evaluative posture from which moral action is selected.

\noindent
The cognitive level—the terrain of salience, empathy, vigilance, and contextual modulation—is where much of moral behaviour appears to be shaped. It is also the level at which the attenuation effect observed in our experiment is most plausibly situated. The humanoid robot neither reasons nor requests anything; yet its silent co-presence seems sufficient to shift the evaluative field from 
which prosocial action is selected. This is the cognitive--affective layer at work—the layer classical Machine Ethics did not attempt to model.

\noindent
From this perspective, a structural implication follows. If moral behaviour emerges from processes of perceptual salience, affective pull, attentional alignment, and social interpretation, then computational models that cast moral agency as rule retrieval or propositional inference address a different 
phenomenon. They remain coherent at the reflective LoA, but they leave the generative cognitive LoA unrepresented, and thus offer an incomplete account of the mechanisms through which behaviour arises.

\noindent
This brings the discussion back to Levels of Abstraction. Once the mismatch between reflective theories and generative processes is recognised, it becomes clear why many debates in Machine Ethics have struggled to engage the kinds of phenomena examined here. The remainder of the thesis develops a framework in which moral cognition, evaluative topology, and synthetic presence can be 
understood in principled alignment. In this sense, the present section has established the conceptual ground on which the subsequent analysis rests.

\noindent
With this distinction established, the path forward becomes clearer. The present section has identified why reflective, principle-based models do not address the generative processes through which moral behaviour emerges. The next step is to develop a positive framework suited to that task: one that locates moral cognition at the appropriate Level of Abstraction and explains how synthetic presence can perturb the evaluative structures linking perception to action.

\section{Levels of Abstraction and the Structure of Machine Ethics}

A central conceptual tool for clarifying the landscape of Machine Ethics has been Floridi’s notion of a \emph{Level of Abstraction} (LoA) \cite{Floridi2008,Floridi2011}. The idea is structurally simple but analytically powerful: any explanation depends on specifying the level at which a system is being described. The LoA determines which variables are observable, the appropriate grain of 
description, and the kinds of explanations that can be meaningfully offered. Ethical theories operate at a high, reflective LoA: they articulate justificatory structures—principles, universalisability tests, value aggregation procedures, and reason-giving relations \cite{Korsgaard1996,Scanlon1998}. Moral psychology, by contrast, works at a lower, cognitive LoA: it investigates the mechanisms that 
\emph{generate} moral judgment, including perceptual salience, affective appraisal, attentional dynamics, and social interpretation \cite{Haidt2001,Greene2001,Greene2004,Cushman2013,Decety2004}.

\noindent
Difficulties arise when content belonging to one LoA is treated as if it were the mechanism operating at another. If reflective theories are misread as cognitive architectures, the distinction blurs and with it the capacity to explain the processes that underlie behaviour. Much of the classical literature in Machine 
Ethics illustrates this tension. By taking the principles of Kantian, utilitarian, or virtue-theoretic ethics and treating them as if they described the internal processes that produce moral behaviour, these approaches implicitly assumed that moral agents—human or artificial—act by applying principles 
\cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Yet such principles occupy the reflective LoA: they explain \emph{why} an action may be defensible, not \emph{how} a moral judgment is generated.

\noindent
When reflective principles are used as behavioural generators—as algorithms intended to produce moral action—the resulting models may be internally elegant but remain out of step with what empirical research suggests about human moral cognition. Moral behaviour appears to arise not from propositional logic or rule 
execution, but from patterns of perceptual salience, affective appraisal, attentional orientation, and social interpretation \cite{Haidt2001,Greene2014,Decety2004,Vinciarelli2009}. These cognitive--affective processes form the conditions under which high-level principles acquire practical significance.

\noindent
This distinction is useful for interpreting the experimental findings presented later in the thesis (in Chapter~\ref{chap:exp_methods}). In the Watching--Eye paradigm, minimal cues of observation typically increase prosocial behaviour \cite{Haley2005,Bateson2006,Dear2019}. Yet when a silent,non-agentic humanoid robot is introduced into the same environment, this pattern appears attenuated. No reasoning, communication, or intentional signalling is involved. The modulation is more plausibly associated 
with shifts in salience, affective alignment, and perceived social presence~\cite{Kuchenbrandt2011,Malle2016,Zlotowski2015}. The accountability cue therefore loses some of its usual influence not because a principle is misapplied, but because the cognitive substrate on which such cues depend has been altered.

\noindent
Taken together, these considerations suggest that moral action is shaped less by the execution of explicit principles than by the dynamic interaction of perceptual, affective, and social processes. Classical, principle-first models operate at a reflective LoA and therefore illuminate a different aspect of moral 
life than the one examined in this thesis. What contemporary sociotechnical contexts bring into view—and what the empirical findings are consistent with—is that artificial agents can influence human behaviour through their impact on the evaluative conditions under which moral decisions are formed.

\noindent
The thesis proceeds from a methodological commitment that follows from this analysis: \emph{before we can design artificial systems with moral capacities or constraints, we must understand how such systems reshape the conditions under which human moral experience unfolds}. This requires reversing the typical order of explanation. Rather than beginning with ethical theory and working downward, 
the inquiry must begin with the empirical architecture of moral cognition, determine how artificial agents interact with it, and only then ask which forms of ethical oversight or design constraint are appropriate.

\noindent
Seen through the lens of Levels of Abstraction, the wider landscape of Machine Ethics becomes more intelligible. Distinct questions can be situated at the levels where they can be meaningfully addressed, and debates that once appeared unresolved take on a clearer structure. For the purposes of this thesis, the restoration of LoA discipline provides the conceptual foundation needed for the 
next stage of the argument: developing an account of moral cognition and synthetic presence that aligns the empirical findings with the explanatory level at which moral behaviour is actually generated.

\section{Evaluative Topology, Affective Architecture, and Synthetic Moral Perturbation}

\noindent
The previous sections showed that classical Machine Ethics, by operating at a reflective Level of Abstraction (LoA), overlooks the mechanisms that generate moral behaviour. Restoring LoA discipline reveals that the explanatory work must begin not with principles but with the cognitive--affective substrate from which moral appraisal and action arise. The task of this section is to make that substrate explicit and to articulate the positive framework that replaces 
the principle-first paradigm.

\noindent
The central claim is that moral behaviour emerges from the organisation of an \emph{evaluative field}: a structured, dynamic configuration shaped by gradients of salience, affective resonance, attentional orientation, contextual norms, and implicit social meaning. Ethical theories operate within this field as 
high-level structural constraints rather than algorithmic generators 
\cite{Korsgaard1996,Scanlon1998}. Their behavioural force depends on how they are realised within the cognitive--affective dynamics through which moral perception becomes moral action.

\subsection{The Evaluative Field}

\noindent
The idea of an evaluative field synthesises three well-established strands of empirical research and places them in the conceptual architecture needed for the remainder of the thesis.

\noindent
\textbf{(1) Moral psychology: affect, intuition, appraisal.}  
Dual-process theory \cite{Greene2001,Greene2004,Greene2014} and the Social Intuitionist Model \cite{Haidt2001} converge on the view that moral evaluation begins with rapid, affectively charged appraisals. Affective tagging, empathic resonance, and motivational relevance are recruited early in the process~\cite{Decety2004,Crockett2016,Moll2002}. Perceptual salience, attentional capture, and intuitive heuristics structure the evaluative landscape before reflective reasoning comes into play.

\noindent
\textbf{(2) Social Signal Processing and affective computing: cue modulation.}  
Work in Social Signal Processing shows that gaze direction, morphological cues, co-presence, and implicit monitoring reshape attentional and affective weighting prior to explicit cognition \cite{Pentland2007,Vinciarelli2009,Conty2016}. 
Human--Robot Interaction research extends this finding to artificial systems: humanoid robots and other embodied agents modulate perceived social presence, agency, and interpersonal expectations through mere presence~\cite{Kuchenbrandt2011,Malle2016,Zlotowski2015,Bremner2022}. These modulations operate at precisely the pre-reflective level identified by moral psychology.

\textbf{(3) Normative theory: structural constraints.}  
Philosophical ethics contributes the insight that moral theories specify \emph{structural constraints} rather than behavioural generators. Deontological principles articulate exclusionary limits \cite{Korsgaard1996}; consequentialism defines gradients of evaluative weight; virtue-theoretic accounts identify attractors within patterns of action and perception~\cite{Foot2001,Hursthouse1999}; 
sentimentalist and contractualist theories specify affective vectors and justificatory relations \cite{Prinz2007,Slote2010,Scanlon1998}. These structures shape the evaluative field but do not, on their own, produce moral behaviour.

When we consider these findings together, a more complete picture emerges. Moral evaluation begins with rapid, affectively charged appraisals. As work in moral psychology shows, what agents notice, how their attention is guided, and the intuitive heuristics they employ shape the evaluative field before any reflective reasoning occurs. These early processes are highly sensitive to social cues. Research in Social Signal Processing, and later in Human–Robot Interaction, shows that gaze, posture, co-presence, and other behavioural signals can alter patterns of attention and affective weighting in ways that precede and influence explicit judgement. Artificial agents participate in this structure. By their mere presence, they can modify perceived social salience, agency, and interpersonal expectations, thereby reshaping the evaluative landscape in which people decide how to act. Normative theories play a different role. They do not describe the causal mechanisms that generate behaviour. They specify structural constraints—principles that limit certain actions, gradients of evaluative weight, or patterns of perception and response that agents have reason to cultivate. These constraints guide moral assessment, but they are realised only within the cognitive and social processes that shape an agent’s immediate evaluative stance. When viewed through Floridi’s Levels of Abstraction, these strands form a coherent model. High-level normative theories define the standards by which actions can be assessed; low-level cognitive and affective mechanisms determine the generative processes through which agents arrive at their decisions; and social cues, including those introduced by artificial systems, modulate the field within which both operate. This is the evaluative architecture from which moral action emerges.


\subsection{Moral Behaviour as Movement Within an Evaluative Field}

\noindent
Within this framework, moral behaviour can be understood as emerging from how an agent moves through the evaluative field formed by perceptual, affective, and social cues. Attention amplifies or suppresses features of the environment, altering which cues become behaviourally salient \cite{Pfattheicher2015}. Affective processes saturate parts of the field with motivational relevance 
\cite{Crockett2016,Decety2004}. Contextual cues shift the weight of norms, expectations, and interpersonal meaning \cite{Haidt2001,Nettle2013}. Social signals further modulate perceived accountability and relational orientation 
\cite{Haley2005,Bateson2006,Dear2019}. 

\noindent
Several kinds of processes shape the evaluative field in which moral decisions are made. What agents attend to can amplify or suppress particular cues, locally altering the salience landscape \cite{Pfattheicher2015}. Their affective responses saturate parts of this field with motivational force, influencing how certain possibilities are experienced \cite{Crockett2016,Decety2004}. Contextual cues may deform the gradients that represent competing obligations, norms, or expectations, shifting how these considerations are weighted \cite{Haidt2001,Nettle2013}. Social signals—including cues of monitoring, accountability, and interpersonal meaning—can further modulate an agent’s interpretation of the situation \cite{Haley2005,Bateson2006,Dear2019}. These influences do not compete with the structural elements described by normative theory. They belong to different explanatory levels. When seen through Floridi’s framework of Levels of Abstraction, intuitive and affective mechanisms determine how agents move through an evaluative space whose higher-level structure is provided by reflective principles \cite{Floridi2008,Floridi2011}. This alignment dissolves the familiar rationalist–intuitionist divide. Rationalist accounts specify structural constraints—limits, weights, and relations that determine which considerations count as reasons—while the cognitive–affective domain explains how agents, in practice, navigate within those constraints \cite{Greene2014,Scanlon1998}. The two domains are therefore not rivals but parts of a single architecture, in which justificatory structures and generative mechanisms operate at different Levels of Abstraction.


\subsection{Synthetic Presence as Evaluative Modulator}

\noindent
The experiment presented in Chapter~\ref{chap:exp_methods} offers an empirical probe into this evaluative architecture. In the Watching--Eye paradigm, minimal cues of observation typically enhance prosocial tendencies through implicit monitoring \cite{Haley2005,Bateson2006}. When a silent, non-agentic humanoid 
robot is introduced into the same environment, this pattern appears attenuated. The shift does not originate in reasoning or principle-application. It is more plausibly understood as a modulation of the evaluative conditions through which the cue acquires behavioural significance \cite{Kuchenbrandt2011,Malle2016,
Zlotowski2015}. The robot’s ambiguous social ontology—perceptually agentic but not fully classifiable—alters the affective and attentional weighting that normally supports the Watching--Eye effect.

\noindent
Importantly, this modulation is \emph{disposition-sensitive}. The 
Prosocial--Empathic ecology shows the strongest attenuation, consistent with its reliance on empathic resonance and interpersonal salience. The Analytical--Structured ecology shows moderate attenuation, reflecting its dependence on interpretive coherence more than affective pull. The Emotionally--Reactive ecology exhibits minimal change, as its evaluative patterns are less stable and provide fewer structured gradients for perturbation to act upon.

\noindent
These differential patterns are consistent with the broader claim developed in this section: synthetic presence exerts its influence \emph{upstream} of deliberation and principle. It modifies the evaluative field within which behaviour is formed rather than overriding or contradicting reflective judgment. Viewed in this light, the humanoid robot functions analogously to a 
\emph{field operator}: its co-presence perturbs the local configuration of salience, affective alignment, and perceived social relevance, thereby altering the trajectory through which moral perception is carried into action. This operator-like role is not meant to imply a fixed mathematical mechanism, but it captures the structural insight suggested by the data—that synthetic presence 
acts on the generative conditions of moral cognition rather than on its reflective outputs.

\section{A Topological Lens on Moral Modulation}

\noindent
Within the framework developed in the preceding sections, the attenuation observed in the experiment can be interpreted through the geometry of the evaluative field. Moral behaviour does not shift because a principle is misapplied or because reflective deliberation has failed. It shifts because the \emph{field} in which such principles acquire behavioural force has been 
locally deformed. The perturbation occurs upstream of reasoning: at the level of salience gradients, affective vectors, attentional curvature, and perceived social ontology.

\begin{itemize}
	\item Deontological constraints lose local traction when accountability 
	salience collapses \cite{Bateson2006,Pfattheicher2015}.  
	\item Consequentialist value-gradients flatten when contextual meaning 
	becomes ambiguous \cite{Nettle2013,Ekstrom2012}.  
	\item Virtue-theoretic dispositions fail to express themselves when 
	affective attractors weaken \cite{Hursthouse1999,Foot2001}.  
	\item Sentimentalist vectors diminish when empathic resonance is displaced 
	\cite{Prinz2007,Slote2010}.  
	\item Contractualist justificatory relations loosen when the perceived 
	social field becomes indeterminate \cite{Scanlon1998}.  
\end{itemize}

\noindent
These patterns are not failures of normativity but signatures of a deeper structure: moral behaviour is \emph{field-sensitive}. Norms constrain the space of admissible trajectories, but the trajectories themselves are determined by the low-level dynamics of appraisal, orientation, and situational meaning. When synthetic presence enters the environment, it acts as a perturbation operator that alters the geometry of this field.

\noindent
The experiment therefore provides a concrete instance of a more general topological insight. The humanoid robot---minimally expressive, non-agentic, and silent---serves as a parametric deformation of the evaluative manifold. Its ambiguous social ontology induces a measurable change in the local structure of the salience landscape, redirecting the flow from moral 
perception to action. In this sense, NAO functions as a 
\emph{topological operator}: not a source of moral content, but a factor that modifies the shape of the field within which moral content is realised.

\medskip
\noindent
Seen through this lens, the attenuation effect is the behavioural trace of a geometric transformation. What changes is not the agent's principles, nor the agent’s reasoning, but the \emph{configuration of the evaluative space} in which those principles and reasons can do work. Synthetic presence thus reveals a property that classical Machine Ethics could not register: moral action emerges from the interplay of structural constraints and the 
topological dynamics of appraisal. The robot’s role is not to replace these dynamics, but to reshape them.


\subsection{Toward a Unified Framework}

\noindent
The notion of an evaluative topology provides the integrative architecture that the field has long lacked. It offers the structural bridge connecting normative theory, empirical psychology, and computational modelling. By distinguishing the Levels of Abstraction at which these domains operate, the 
framework clarifies how high-LoA normative structures shape the space of justification, while low-LoA cognitive--affective mechanisms determine the trajectories through which moral perception is transformed into action. Within this architecture, the influence of artificial agents becomes intelligible: they do not contribute beliefs, reasons, or principles, but modulate the salience, affective gradients, and social cues that configure the evaluative field itself.

\noindent
This synthesis completes the conceptual turn initiated by the literature review. It repositions the problem of ``moral AI'' from the design of principle-following agents to the analysis of how artificial systems participate in---and occasionally perturb---the generative processes that give moral judgments their behavioural force. The chapters that follow build on this topological foundation to develop a formal account of machine-mediated moral cognition. In that account, artificial systems are not ethical reasoners but \emph{operators on the evaluative field}: elements whose presence reshapes the conditions under which moral meaning is registered, weighted, and ultimately expressed in action.


\section{Integrative Synthesis: Toward a Cognitive–Affective Framework for Machine-Mediated Morality}

\noindent
The analyses in this chapter converge on a coherent picture of moral behaviour under artificial co-presence. Classical Machine Ethics treats reflective normative theories as behavioural generators~\cite{Bringsjord2006,Anderson2011,Arkin2009}, 
yet moral psychology shows that action emerges from a cognitive--affective architecture structured by salience, attention, empathy, and contextual cues~\cite{Haidt2001,Greene2001,Cushman2013}. Work in HRI and Social Signal Processing demonstrates that artificial agents modulate these mechanisms through minimal social cues~\cite{Vinciarelli2009,Malle2016,Kuchenbrandt2011,Zlotowski2015}. Evaluative topology integrates these strands by modelling behaviour as movement within a salience-weighted, affectively structured field. The experiment in Chapter~\ref{chap:exp_methods} is consistent with this view: synthetic presence perturbs the evaluative field upstream of deliberation and attenuates prosocial action.

Three conclusions follow from the literature:

\begin{enumerate}
	\item \textbf{Moral behaviour is generated at the cognitive LoA.}  Reflective theories provide justificatory standards 
	\cite{Korsgaard1996,Scanlon1998}, but behaviour is shaped by low-LoA affective and social mechanisms \cite{Greene2001,Decety2004,Conty2016}.
	
	\item \textbf{Artificial agents modulate the evaluative field.}  
	Presence alone can shift attention, empathy, and perceived social meaning ~\cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,Bremner2022}. The attenuation effect is consistent with this.
	
	\item \textbf{A viable programme for moral AI must begin with evaluative topology.}  Principle-executing architectures do not account for the mechanisms that generate behaviour \cite{Wallach2008,Arkin2009}.
\end{enumerate}

\noindent
Taken together, these insights reframe moral AI: artificial systems must be understood as \emph{operators on the evaluative field} rather than executors of moral rules. Synthetic presence reshapes salience, affective resonance, and accountability cues—perturbations that arise prior to explicit reasoning.

% ---------------------------------------------------------------------

\section{Global Synthesis: From Inferential Displacement to Synthetic Moral Topology}

\noindent
Across the reviewed literature, a consistent structure emerges: moral judgment and action arise from a cognitively embedded, affectively mediated evaluative field~\cite{Haidt2001,Greene2001,Decety2004,Vinciarelli2009}. Ethical theories operate reflectively, but behaviour is generated by the cognitive architecture through which salience and affect are organised. Artificial agents participate in this architecture by modulating these processes~\cite{Malle2016,Bremner2022}.

\subsection{From Question to Framework}

\noindent
The guiding question of this thesis—whether synthetic presence can perturb the inferential transformation from moral salience to action—emerges naturally from 
existing tensions. Classical Machine Ethics presumes principle-execution~\cite{Bringsjord2006,Anderson2011}, whereas Moral Psychology emphasises affectively charged appraisal \cite{Haidt2001,Cushman2013}. SSP and HRI show that social cues modulate these appraisals \cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,Malle2016}. Yet these strands have seldom been integrated.

\subsection{Rationale for a Multi-Hypothesis Approach}

\noindent
The literature identifies three plausible loci of modulation:
\begin{enumerate}
	\item \textbf{Evaluative deformation}: altered salience, monitoring, or affective weighting~\cite{Haley2005,Bateson2006,Pfattheicher2015}.  
	\item \textbf{Synthetic normativity}: effects arising from perceived social ontology~\cite{Zlotowski2015,Kuchenbrandt2011,Malle2016}.  
	\item \textbf{Perturbation of inferential pathways}: displaced empathy, attention, or contextual interpretation~\cite{Decety2004,Greene2014}.  
\end{enumerate}

\noindent
No single mechanism captures the full range of effects; a multi-hypothesis framework is therefore required.

\subsection{What the Literature Establishes}

\noindent
Three findings are robust across disciplines:

\begin{enumerate}
	\item \textbf{Moral behaviour is field-sensitive}, shaped by salience, affect, 
	and context \cite{Greene2001,Haidt2001,Decety2004}.  
	\item \textbf{Artificial agents modulate the field}, altering social meaning, 
	vigilance, and empathic stance \cite{Malle2016,Bremner2022}.  
	\item \textbf{Classical Machine Ethics cannot model this modulation} because 
	it ignores the cognitive LoA where behaviour is generated 
	\cite{Wallach2008,Arkin2009}.  
\end{enumerate}

\noindent
These findings motivate a shift: moral AI must be grounded in the structural dynamics of the evaluative field rather than in normative content alone.

\noindent
The literature review reveals a domain-level misalignment: reflective ethical principles have been treated as if they described the psychological machinery of moral behaviour \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Yet 
reflective norms articulate \emph{conditions of justification} 
\cite{Korsgaard1996,Scanlon1998}, whereas behaviour arises from rapid appraisal and attentional dynamics~\cite{Haidt2001,Greene2001,Cushman2013}. Collapsing these 
levels obscures the mechanisms through which behaviour is modulated.

\noindent
When empirical findings across Psychology, Affective Neuroscience, SSP, and HRI are placed together, a convergent evaluative architecture becomes visible. Moral judgment begins with rapid, affect-laden appraisal; social and contextual cues modulate evaluative weighting; and explicit reasoning intervenes downstream 
\cite{Decety2004,Crockett2016}. Classical Machine Ethics never incorporated this architecture and could therefore not accommodate modulation phenomena.

\noindent
Finally, the review isolates the theoretical gap motivating the experiment: 

\emph{can synthetic presence perturb the evaluative field upstream of explicit moral reasoning?} 

No classical framework poses this question because none operate 
at the LoA where such perturbations occur. The study is designed to probe this precise gap. In sum, the literature review identifies the cognitive layer at which moral behaviour is generated, clarifies why prior models were misaligned with this layer, and isolates the phenomenon requiring empirical investigation. Within this 
thesis, the literature review is not merely preparatory; it constitutes the first scientific result.


If there is a single thread running through this chapter, it is that moral life isfar more delicate---and far more interesting—than the theories we usually buildto explain it. We began with frameworks, distinctions, and formal models, but what the literature ultimately reveals is something more human: the way attention pulls
us, the way small cues colour our sense of what matters, the way meaning settlesinto a situation before we ever articulate a reason.

Artificial systems step into this landscape not as moral agents, nor as puzzles tobe solved, but as new participants in the background against which moralexperience unfolds. They shift the texture of a moment, sometimes imperceptibly,sometimes decisively, and in doing so they help us see what Moral Psychology, Information Ethics, and HRI each illuminate from different angles.

This chapter has tried to gather those strands into a single frame. The rest of the thesis asks what happens when we move from theory to experiment—when these ideas meet the subtle trajectories of real human behaviour. But the core insight remains the same: to understand how machines and humans share moral space, we must begin
with the architecture of the human mind, and with the quiet ways in which meaning is shaped before any reasoning begins.