\chapter{Literature Review}
\label{chap:lit_rev}

% Section 1
\section{Introduction: Scope, Objectives, and Theoretical Commitments}

\noindent
This chapter establishes the conceptual and methodological terrain on which the remainder of the thesis proceeds. This review isn’t just a background filler; but ratgehjr a the first test looking for the the assumptions which the experimental results depend on, the levels of abstraction they operate at, the mechanisms they take for granted, and the gaps they leave unexplained. It lets us ask: 

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Wheather the synthetic presence really does modulate the path from perception to action, which existing frameworks can even see that phenomenon? And which ones are blind to it by design?}
	\end{leftbar}
\end{center}

\bigskip
\noindent
By examining the published work through that lens, we start to see an emerging pattern: almost all of classical Machine Ethics operates at the reflective level—principles, rules, deliberation—while the phenomenon we are studying unfolds at the cognitive level, upstream of reasoning. That mismatch isn’t an opinion; it’s a structural finding that the literature itself reveals.

\bigskip
\noindent
The aim here is therefore to reposition the study of moral behaviour under artificial co-presence---and the design of artificial moral systems more broadly---within a theoretically unified space at the intersection of \emph{Machine Ethics}, \emph{Computational Morality}, and \emph{Social Signal Processing} (SSP). Although these fields emerged from distinct disciplinary lineages, the experimental results presented in Chapter~\ref{chap:exp_methods} show that they now converge around a single problem: artificial agents, even when silent, passive, and non-interactive, \emph{modulate the evaluative conditions under which moral judgment and action unfold}. Understanding this phenomenon requires an integration of normative philosophy, moral psychology, computational modelling, and HRI.

Hence, the project takes root here. The literature review is the first piece of evidence. It shows that if we stay at the reflective level, we can’t even formulate the right kind of question, let alone explain the modulation we later observe experimentally (Chapter~\ref{chap:exp_methods}). That’s why the review matters so much—it’s the tool that tells us where the explanation has to live before you collect single data points.

\noindent
One of the core findings of the literature is that classical Machine Ethics starts from the wrong end of the problem. The whole tradition begins by taking high-level ethical theories—Kantian tests, utilitarian calculations, virtue templates, deontic logics—and trying to encode them as if they were models of moral agency \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008,Guarini2006,Allen2005}. 

\medskip
\noindent
But if we look closely at what those theories actually do, they are not descriptions of how humans produce moral behaviour. They are descriptions of how humans \emph{justify} moral behaviour after the fact. This distinction is explicit in modern moral philosophy: Kantian universalisability, utilitarian aggregation, and contractualist justification articulate reflective standards for assessing reasons, not cognitive processes for generating action \cite{SidgwickMethods,Scanlon1998,Korsgaard1996}. They operate at a very high Level of Abstraction: they tell you what counts as a good reason, \textit{not how a person comes to act in the first place} \cite{Floridi2008,Floridi2011}.

It should be noted that while most of what traditionally falls under Machine Ethics—Computational Morality, formal deontic systems, encoded utility functions—belongs to the “\textit{pre-LLM}’’ era, the limitation identified here does not evaporate with the advent of large language models. If anything, the arrival of LLMs makes the limitation more sharply visible.

\medskip
\noindent
Recent work demonstrates that LLMs can perform exceptionally well on reflective moral tasks: they generate sophisticated reasoning, balance competing principles, and provide normatively articulate justifications that map cleanly onto established ethical frameworks \cite{Jin2022Moral,Scherrer2023,Nguyen2023,Aher2023,Charlton2023}. They also exhibit high performance on benchmarked moral analogy tasks and moral classification challenges \cite{Hendrycks2021,Emelin2023}. But all of this ability is situated at the reflective Level of Abstraction: the linguistic, justificatory, post-hoc LoA.

And humans do not act morally at that level. On every empirically supported account of moral cognition—from social intuitionism \cite{Haidt2001,Haidt2007}, to dual-process theory \cite{Greene2001,Greene2014,Cushman2013}, to affective neuroscience \cite{Phelps2006,Decety2008,Zaki2012}, to embodied and socially embedded models \cite{Buon2016,Malle2016,Bremner2022}—moral behaviour is driven by salience, affect, perceptual appraisal, social cues, and attentional orientation, not by the explicit application of normative principles. These processes sit one LoA below the linguistic–justificatory space in which LLMs operate.

Thus, although we now live in a “post-LLM’’ era, the fundamental issue is not that pre-LLM Machine Ethics was technically limited or symbolically brittle. The deeper problem is that both pre-LLM Machine Ethics and modern LLMs operate at the wrong Level of Abstraction if the goal is to model, predict, or understand human moral behaviour. This is precisely the mismatch Floridi’s LoA discipline is designed to diagnose \cite{Floridi2008,Floridi2011}: moral justification and moral production belong to different descriptive orders. LLMs amplify the upper order; they leave the generative order untouched.

Chronologically, the pattern is straightforward:

\begin{itemize}
	\item \textbf{Pre-LLM Machine Ethics} attempted to encode normative principles directly—deontic rules, utility functions, virtue schemas—and encountered the reflective/cognitive mismatch documented extensively in the literature \cite{Moor2006,Wallach2008,Cervantes2020,Coeckelbergh2023}.
	\item \textbf{Post-LLM models} generate better principles, better explanations, and more articulate moral rhetoric, but they encounter the same mismatch, now at a higher level of linguistic sophistication \cite{Bender2021,Mittelstadt2019,Whittlestone2019,Andrus2023,Kasirzadeh2023}.
\end{itemize}

The chronology therefore does not mark a methodological revolution; it exposes the persistence of a category error. The assumption that moral behaviour is fundamentally a matter of reasoning or principle-application has survived unchallenged into the LLM era. But contemporary empirical evidence shows that humans rarely deploy such reasoning in the production of moral action \cite{Cushman2013,Haidt2001,Young2012}.

As several recent critical analyses emphasise, LLMs produce moral reasoning without moral cognition \cite{Kasirzadeh2023,Andrus2023,Gardner2024}. They resolve dilemmas fluently~\footnote{The distinction between reflective and generative Levels of Abstraction (LoAs) is crucial here. Moral justification, principle-balancing, and linguistic explanation occur at a reflective LoA \cite{Floridi2008,Floridi2011}. Human moral behaviour, by contrast, arises from perceptual, affective, and socially embedded processes documented across moral psychology and social neuroscience \cite{Haidt2001,Greene2001,Zaki2012,Decety2008}. Recent analyses of LLM-based moral reasoning confirm that these models excel at reflective justification but do not reproduce the generative cognitive–affective mechanisms that produce moral action \cite{Kasirzadeh2023,Andrus2023,Gardner2024}. The arrival of LLMs therefore intensifies—rather than resolves—the LoA mismatch at the core of Machine Ethics.}; they do not reproduce the cognitive–affective processes by which humans come to feel that something is a dilemma in the first place. Moral language is not moral experience. Reflective justification is not perceptual-affective appraisal.

That is the chronological insight, if one seeks it: the technologies have evolved dramatically, but the underlying LoA mismatch remains unchanged. The surface has shifted; the category error has not budged. And that’s really the hinge of this work:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Synthetic systems can now talk morality far better than they can participate in the conditions that shape moral action. The two are not the same.}
	\end{leftbar}
\end{center}


\bigskip
\noindent

What becomes interesting, especially now, is that artificial systems are not just reasoning in the abstract; they’re entering our environments. They’re in phones, homes, classrooms, offices. Their presence affects how we behave, how we interpret situations, how we allocate attention.

So the shift isn’t from ‘pre-LLM Machine Ethics’ to ‘post-LLM Machine Ethics.’ The shift is from seeing AI as an agent that reasons to seeing AI as an element in the cognitive ecology—something that reshapes the conditions in which human moral behaviour unfolds. Whether the system speaks like Kant or Shakespeare or your best friend is irrelevant if its presence still modulates the way people notice, feel, and act. That’s the axis that matters. That is the core of this work. And this is where the category error comes in. Machine Ethics assumes that the principles of an ethical theory can be treated as the cognitive machinery of a moral agent-- \textit{as if humans behave by running Kantian tests or utilitarian calculations in their heads}. 

But we know that isn’t how moral action is produced. Human behaviour comes from a much lower level: from what captures our attention, what feels salient, how we read a face or a tone, how empathy gets triggered, how the context shifts our sense of what matters. These processes are fast, intuitive, emotional, and deeply social~\cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}.

Decades of work in moral psychology and neuroscience demonstrate that intuitive, affectively laden processes precede and shape explicit moral judgement \cite{Greene2004,Haidt2001,Moll2002}. The intuitive, affectively charged processes come first \cite{Haidt2001,Greene2001,Decety2004,Cushman2013}. They shape the space in which explicit reasoning even becomes possible: before reflection begins, appraisal mechanisms, empathic resonance, salience attribution, and motivational tagging have already constrained the field of viable responses \cite{Moll2002,Crockett2016,Prinz2007}. The reflective story we tell afterwards might be coherent, but it is downstream of the machinery that actually drives behaviour \cite{Greene2004,Haidt2001}.

So when Machine Ethics takes ethical principles and treats them as if they were the generator of moral action, it is working at the wrong level entirely. It is replacing the justification of moral behaviour with the mechanism of moral behaviour, and those are not the same thing \cite{Korsgaard1996,Scanlon1998,Foot2001}. High-level principles articulate normative standards, but the processes that produce moral action operate at a far more fundamental cognitive--affective level \cite{Floridi2008,Floridi2011}.

\medskip
\noindent
So when one tries to design a “moral machine” by encoding Kant or utilitarianism, one collapses these two levels of abstraction. One is treating reflective principles as if they were psychological mechanisms. And the literature shows very clearly that they are not. Ethical theories explain why an action can be defended; they do not explain how moral behaviour is formed \cite{Prinz2007,Foot2001}. 

\medskip

\noindent
That is the central limitation the literature review exposes. It shows that classical Machine Ethics is methodologically elegant but cognitively misaligned. It is operating at the wrong level to even see the phenomenon we are investigating. As empirical work in moral psychology, affective neuroscience, SSP, and HRI repeatedly shows, the relevant causal structure lies in the evaluative substrate of salience, affect, and social interpretation—not in the reflective principles invoked after the fact \cite{Vinciarelli2009,Kuchenbrandt2011,Malle2016,Zlotowski2015}.


\medskip
\noindent
\noindent
Classical Machine Ethics is beautifully constructed—methodologically elegant, logically clean—but it’s operating at a level that’s cognitively out of sync with where moral behaviour actually happens. It starts from principles, from rules, from reflective argumentation \cite{Bringsjord2006,Anderson2011,Wallach2008,Arkin2009}. But the causal work—\textit{the thing} that actually drives behaviour—lives one layer down, in salience, emotion, attention, and social interpretation \cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}.

\medskip
\noindent
If we look at the wrong layer, we simply don’t see the phenomenon we’re investigating. And this problem carries over into what’s usually called Computational Morality, just in a different form. Whether it’s logic engines, preference aggregators, or the newer wave of LLM-based moral modelling, the assumption is the same: moral behaviour can be approximated by symbolic inference—by treating moral judgment as a reasoning problem \cite{Guarini2006,Allen2005,Arkoudas2005,Mittelstadt2019,Bender2021}.

\medskip
\noindent
But the last twenty years of empirical work tell a very different story. Most moral judgments don’t start with slow deliberation; they start with fast, intuitive, emotionally charged appraisal~\cite{Haidt2001,Greene2004,Cushman2013}. They’re shaped by who’s present, how someone looks at you, the tone in the room, what feels at stake, and the affective and social cues embedded in the environment \cite{Decety2004,Conty2016,Vinciarelli2009,Zlotowski2015}. It’s a messy, context-sensitive process \cite{Moll2002,Prinz2007}. When we try to model morality as if it were a chain of propositions—if A then B, if C then D— we are abstracting away the very machinery that actually produces behaviour in humans. And that’s the machinery our experiment shows can be shifted by the simple presence of a humanoid robot \cite{Kuchenbrandt2011,Malle2016,Bremner2022}.

\medskip
\noindent
In other words: the classical computational models are not wrong because the logic is bad. They’re wrong because they are modelling the wrong thing. They are trying to capture moral reasoning, when the real action is happening in the evaluative landscape that sits underneath moral reasoning. That’s the level where synthetic presence does its work.


In Chapter~\ref{chap:moral_primer} we make very explicit that: \textit{any model of moral behaviour that leaves out the cognitive–affective machinery and the social-signalling dynamics behind moral judgment is simply not describing human beings.} It becomes unstable both scientifically and philosophically. This is where the Level-of-Abstraction issue gets predominant. If we would take high-level moral theories—the reflective content, the principles, the rules—and treat them as if they were the psychological mechanism that produces moral behaviour, we would end up with theories that look elegant but don’t actually predict what people do. They explain justification, not behaviour.We would develop artefacts; models that fail not because the logic is wrong, but because they’re modelling the wrong layer of the system. This becomes plainly clear in the experiment in Chaper~\ref{chap:exp_methods}.

\medskip
\noindent
The robot we use has no beliefs, no goals, no intentions, and no communicative acts; it is not reasoning or attempting to influence participants. Yet research consistently shows that even minimally expressive or non-agentic robots modulate human social behaviour \cite{Kuchenbrandt2011,Bremner2022,Bainbridge2011,Malle2016}. These effects operate through changes in salience, attention, and perceived social presence rather than explicit reasoning \cite{HaleyFessler2005,Bateson2006,Phelps2006,Zaki2012}. Such upstream perturbations cannot be captured by rule-based, utility-theoretic, or propositional models of morality, which mislocate moral action in reflective reasoning rather than in the intuitive, affective systems documented across moral psychology \cite{Haidt2001,Greene2014,Cushman2013,Dancy2004,Cervantes2020}.


\noindent
At this stage, the literature reveals a point that no strand of classical Machine Ethics has convincingly addressed. If the aim is to understand how humans behave morally in the presence of artificial agents—and to model that behaviour in a form that artificial systems can meaningfully operationalise—then the foundational assumptions of the field must be re-examined. Principle-first approaches, whether deontic, utilitarian, or virtue-theoretic, presuppose that moral norms can be implemented as explicit rules or evaluative operators \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Yet empirical research in moral psychology and affective neuroscience shows that moral behaviour does not arise from rule application but from cognitively embedded processes of appraisal, salience detection, affective resonance, and social interpretation \cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}.

\medskip

\noindent
Thus, moral norms cannot be treated merely as rules to be encoded. They must be understood in terms of their \emph{topological function}: the way they structure constraints, gradients, and permissible trajectories within the evaluative field through which moral perception is transformed into action \cite{Foot2001,Aristotle_nicomachean,Prinz2007}. Norms operate at a reflective Level of Abstraction, specifying justificatory structure rather than cognitive mechanism \cite{Korsgaard1996,Scanlon1998,Floridi2008,Floridi2011}. Their behavioural influence depends on how they interact with, and are realised by, low-level cognitive–affective processes.

\medskip

\noindent
For the same reason, moral judgment cannot be modelled as pure reasoning or symbolic inference. Dual-process and intuitionist models demonstrate that intuitive, affectively charged appraisals precede reflective judgment and constrain the space of subsequent deliberation \cite{Greene2004,Haidt2001,Cushman2013}. Attention, empathic resonance, perceptual salience, and social–contextual modulation shape the evaluative landscape long before propositional reasoning becomes active \cite{Moll2002,Decety2004,Vinciarelli2009}.

\medskip

\noindent
Nor can artificial agents be treated as carriers or executors of moral values. Research in HRI and Social Signal Processing shows that artificial systems act primarily as \emph{modulators}—as elements within the environment that reshape salience, perceived social presence, accountability cues, and evaluative expectations \cite{Kuchenbrandt2011,Malle2016,Zlotowski2015,Bremner2022}. Their influence operates upstream of explicit judgment, altering the evaluative field within which moral decisions are formed.

\medskip

\noindent
Once the problem is reframed in this way, the broader picture becomes clear. The limitations of classical Machine Ethics are not failures of logic but failures of explanatory level. Its models operate at a reflective LoA and therefore cannot detect, let alone predict, the cognitive–affective perturbations that empirical research has consistently shown to drive moral behaviour. When the evaluative landscape is foregrounded, the phenomena that appeared mysterious or anomalous under classical formulations become theoretically tractable: synthetic presence exerts moral influence not by embodying values or executing principles, but by reshaping the generative conditions under which moral action emerges.


\medskip
\noindent
What follows is therefore more than a literature review. It is a reconstruction of the conceptual lineage necessary to render the experimental findings intelligible. The chapter develops a unified framework grounded in four pillars:

\begin{enumerate}
	\item \textbf{Normative--Philosophical Foundations}: clarifying the role of deontological invariants, consequentialist gradients, virtue-theoretic dispositions, sentimentalist affective vectors, and contractualist justificatory structures—each reconceived through evaluative topology~\cite{Pentland2007,Vinciarelli2009,Picard1997,Decety2004}.
	\item \textbf{Empirical Moral Psychology}: synthesising dual-process theory, the Social Intuitionist Model, affective tagging, attentional capture, and social modulation as the core mechanisms by which moral salience becomes behaviour.
	\item \textbf{Social Signal Processing and Affective Computing}: grounding the argument in the extensive evidence that social cues—including gaze, co-presence, morphology, and synthetic embodiment—systematically perturb evaluation, expectation, and action.
	\item \textbf{Critical Reassessment of Machine Ethics and Computational Morality}: demonstrating why their monolithic, principle-first architectures are methodologically unsound and why any future model of moral behaviour must begin with the structural, field-level properties revealed in this thesis.
\end{enumerate}

\noindent
Through this triangulated lens—normative, psychological, and computational—the moral displacement effect demonstrated in the experiment becomes theoretically transparent. Artificial agents do not “make moral decisions” in any traditional sense; instead, they \emph{reshape the evaluative conditions under which humans do}. This chapter situates that insight within the broader intellectual landscape and establishes the theoretical commitments that guide the remainder of the thesis.


\noindent
Social Signal Processing provides the empirical and theoretical bridge that reveals \emph{why} the attenuation observed in the experiment cannot be understood through abstract ethical theory alone. Work on nonverbal regulation of social meaning \cite{Pentland2007,Vinciarelli2012}, anthropomorphisation and norm-perception in HRI \cite{Kuchenbrandt2011,Malle2016,Bremner2022}, and affective appraisal dynamics \cite{Picard1997} shows that human moral behaviour is continuously shaped by \emph{attentional capture, empathic resonance, and perceived social evaluation}. These mechanisms operate at the \emph{cognitive Level of Abstraction} \cite{Floridi2008}---the LoA at which perceptual, affective, and inferential processes modulate the evaluative topology that determines behavioural output. Importantly, this LoA sits \emph{below} the reflective LoA at which ethical theories articulate reasons, duties, or values. The result is a structurally asymmetric architecture: normative principles do not directly cause behaviour; rather, behaviour emerges from the field-like interaction of salience, affect, and social interpretation.

\noindent
Seen through this lens, the experimental attenuation revealed in Chapter~\ref{chap:experimental_methods} becomes a diagnostic probe into the moral field itself. The humanoid robot does not “apply a principle’’ nor transmit a norm; instead, it subtly transforms the perceptual--social environment in which moral appraisal unfolds. It reshapes the evaluative gradients through which the Watching-Eye cue gains its normative traction. A scientifically credible framework for moral AI must therefore begin from this empirical architecture: the dynamics of salience, affect, embodiment, and co-present social meaning—not from the reflective content of ethical theory. This is the central commitment of the present chapter.

\medskip

\noindent
The chapter proceeds by articulating six interlocking arguments that establish the methodological and conceptual ground for a new approach to moral AI:

\begin{enumerate}
	\item \textbf{Machine Ethics consists of two structurally distinct research programmes}, whose conflation has hindered both scientific progress and philosophical clarity.
	\item \textbf{Moral psychology reveals a dual-process, affectively embedded cognitive architecture} that is incompatible with treating moral judgement as purely rule-based or propositional.
	\item \textbf{Classical Machine Ethics fails because it collapses Levels of Abstraction}: it assumes that normative structures at a reflective LoA can serve as computational operators at a cognitive LoA.
	\item \textbf{Moral behaviour arises from an evaluative topology}, not from internal execution of ethical principles: perception, affect, and social meaning generate the gradients that guide action.
	\item \textbf{Synthetic presence perturbs this topology in measurable ways}, attenuating prosocial output by deforming the salience field rather than altering explicit reasoning.
	\item \textbf{A viable programme for moral AI must therefore model how machines reshape human moral experience}, before attempting any normative codification or ethical implementation.
\end{enumerate}

\medskip

\section{The Two Research Projects in Machine Ethics}

\noindent
Machine Ethics, as originally formulated by Moor \cite{Moor2006} and developed by Wallach and Allen \cite{Wallach2008}, is not a single unified research field. It is a composite of two methodologically divergent projects whose aims, assumptions, and Levels of Abstraction are frequently confounded. The first, \emph{Human--Machine Ethics}, concerns the normative governance of human behaviour \emph{around} and \emph{in the presence of} artificial systems: agency displacement, accountability, social influence, and moral risk. The second, \emph{Computational Machine Ethics}, attempts to design machines that themselves make morally adequate decisions by embedding ethical theories into computational architectures.

\noindent
The experimental results of this thesis show that conflating these projects is not merely a conceptual error—it produces empirical blindness. Human--Machine Ethics speaks directly to the phenomenon the experiment uncovers: the modulation of human moral behaviour by artificial co-presence. Computational Machine Ethics largely ignores this modulation, because it begins from the false premise that moral behaviour can be generated by encoding reflective ethical theories.

\medskip

\noindent
Human--Machine Ethics aligns naturally with research in HRI, media psychology, and SSP. Studies on robotic social facilitation \cite{Bremner2022}, anthropomorphic cueing \cite{Kuchenbrandt2011}, norm-perception under artificial observation \cite{Malle2016}, and social attention modulation \cite{Vinciarelli2012} demonstrate that artificial agents influence human moral behaviour even in the absence of explicit interaction. These findings are central to the present thesis: moral environments are \emph{ethically load-bearing} by virtue of their perceptual and affective structure alone. Classical Machine Ethics has never incorporated this fact.

\medskip

\noindent
Computational Machine Ethics, by contrast, emerged from attempts to implement ethical theories directly in artificial systems. Deontic logics \cite{Bringsjord2006,Anderson2011}, utilitarian optimisers \cite{Arkin2009}, and virtue-based architectures \cite{Wallach2008} all assume that moral behaviour can be generated by applying normative principles at runtime. Yet this assumption conflicts with decades of empirical evidence showing that human moral cognition is primarily intuitive, affective, and context-sensitive \cite{Haidt2001,Greene2001,Cushman2013}. Attempts to encode ethical theories as computational operators therefore violate LoA discipline: they treat reflective normative content as if it were a psychological mechanism.

\medskip

\noindent
The experiment demonstrates that this assumption is untenable. Synthetic presence~$\mathscr{R}$ systematically perturbs the evaluative topology that governs intuitive appraisal, attentional salience, empathic resonance, and contextual interpretation. Moral behaviour changes because the \emph{field} changes—not because an abstract principle is executed differently~\cite{Nettle2013,Eckstrom2012}. This insight directly implicates Human--Machine Ethics in the study of moral behaviour under artificial co-presence and reveals the inadequacy of Computational Machine Ethics as a behavioural model.

\noindent
A scientifically coherent future for moral AI therefore requires recognising that Machine Ethics contains two distinct explanatory projects, operating at different Levels of Abstraction and constrained by different empirical realities. Only by respecting this structural division can normative theory, cognitive science, and computational modelling be brought into principled alignment.

% Interim Conclusion
\noindent
Taken together, Sections~\ref{chap:lit_rev}--1 and~2 establish the methodological reorientation that grounds the remainder of this chapter. The analysis of Machine Ethics as two structurally distinct research programmes reveals a foundational misalignment: Computational Machine Ethics operates at an inappropriate Level of Abstraction (LoA), treating reflective normative structures as if they were generative psychological mechanisms, while Human--Machine Ethics has been conceptually underutilised despite its direct relevance to the empirical modulation of moral behaviour. Crucially, the experimental findings in Chapter~\ref{chap:experimental_methods} reinforce this structural diagnosis. A silent, non-agentic robot altered participants’ donation behaviour \emph{even under a strong moral prime} (the Watching-Eye cue), consistent with the robust social-monitoring effects documented across field and laboratory studies \cite{Bateson2006,Haley2005,Dear2019}. These findings demonstrate that synthetic presence reshapes the perceptual--affective conditions under which moral salience becomes action. No framework that models moral behaviour as propositional rule retrieval or principle execution can account for such a field-level deformation.

\medskip

\noindent
Having clarified the conceptual landscape and identified the methodological failures that motivate a systematic reframing of Machine Ethics, the next step is to specify the theoretical resources required to reconstruct a scientifically grounded model of moral cognition. Sections~3 and~4 therefore articulate the psychological and philosophical commitments that classical Machine Ethics overlooked: the dual-process structure of moral cognition; the cognitive--affective substrate of evaluation; and the LoA discipline that distinguishes reflective justification from cognitive mechanism. These sections also provide the conceptual infrastructure needed to understand \emph{why} the experiment produced a uniform attenuation effect across participants and \emph{why} that attenuation is best interpreted as a deformation of the evaluative topology rather than as a deviation from principle, preference, or conscious moral reasoning.

% Section 3
\section{Moral Psychology and Moral Philosophy: Cognitive--Affective vs.\ Rationalist--Intuitionist Models}

\noindent
Any attempt to model moral behaviour---whether human or artificial---must begin from a clear understanding of the psychological architecture through which moral judgment is produced. Decades of empirical work demonstrate that moral cognition is not primarily a matter of propositional reasoning or deliberative inference, but a \emph{dual-process system} in which intuitive, affectively inflected pathways interact with slower, reflective reasoning. The Social Intuitionist Model \cite{Haidt2001}, Greene’s neurocognitive dual-process account \cite{Greene2001,Greene2004,Greene2014}, and Cushman’s action-based inference framework \cite{Cushman2013} converge on a common insight: moral evaluation originates in rapid, affectively loaded appraisals shaped by perceptual salience, empathy, and attentional dynamics. Affective tagging and empathy architectures \cite{Decety2004,Crockett2016} show that moral judgments emerge from mechanisms integrating motivational relevance, value-laden salience, and social meaning long before explicit reasoning becomes operative.

\medskip

\noindent
This cognitive--affective model stands in sharp contrast with the rationalist traditions that have historically informed Machine Ethics. Kantian, utilitarian, and contractualist theories articulate \emph{justificatory structure}: universalisability conditions, value aggregation, or reason-giving relations. These theories operate at a reflective LoA concerned with \emph{why} actions are justifiable, but they do not---and were never intended to---describe \emph{how} moral judgments are generated within actual cognitive systems. As Sidgwick \cite{SidgwickMethods}, Scanlon \cite{Scanlon1998}, and Korsgaard \cite{Korsgaard1996} emphasise, justification belongs to the domain of reflective endorsement, not causal explanation.

\medskip

\noindent
Machine Ethics inherited only one side of this philosophical division. By adopting ethical theories as computational templates, it implicitly treated rationalist structures as if they were generative psychological mechanisms. Yet the empirical architecture of moral cognition does not support this view. Moral behaviour is governed by attentional capture, affective resonance, and socially modulated salience \cite{Conty2016,Dear2019,Pfattheicher2015}---precisely the mechanisms perturbed in the experiment. The Watching-Eye effect \cite{Haley2005,Bateson2006,Nettle2013,Eckstrom2012} operates not through rule endorsement but through implicit monitoring, affective vigilance, and social-evaluative sensitivity. This is the cognitive LoA at which empathy, salience, and contextual modulation operate \cite{Decety2004,Greene2014,Moll2002}. The attenuation effect observed in Chapter~\ref{chap:experimental_methods} thus belongs squarely within the cognitive--affective architecture of moral psychology, not within any reflective principle or moral theory.

\medskip

\noindent
This insight has direct implications for Computational Machine Ethics. If moral cognition fundamentally arises from evaluative salience, attentional modulation, and affective integration, then any computational model that treats moral behaviour as propositional inference or rule execution is descriptively incomplete. The next section articulates the methodological consequences of this fact through the concept of Level-of-Abstraction discipline.

% Section 4
\section{Levels of Abstraction and the Failure of Machine Ethics}

\noindent
Floridi’s notion of a \emph{Level of Abstraction} (LoA) provides the conceptual apparatus required to diagnose the foundational error in classical Machine Ethics. An LoA specifies the \emph{observables}, the \emph{granularity}, and the \emph{explanatory constraints} appropriate to a given description of a system \cite{Floridi2008,Floridi2011}. Normative theories occupy a high LoA: they articulate justificatory relations, principles of right action, and standards of rational agency. Moral psychology, by contrast, operates at a lower LoA, modelling the mechanisms through which perception, affect, salience, attention, and social meaning generate evaluative trajectories. Treating the former as if it directly governed the latter collapses the boundary between reflective and cognitive explanation.

\medskip

\noindent
Machine Ethics systematically commits this collapse. Deontic architectures \cite{Bringsjord2006,Anderson2011}, utilitarian optimisation systems \cite{Arkin2009}, and virtue-based computational agents \cite{Wallach2008} assume that principles defined at a reflective LoA can serve as \emph{behavioural generators}. Logic-based systems treat moral judgment as derivation; rule-governed architectures treat it as constraint propagation; utility-based systems treat it as maximisation. All presume that moral behaviour is produced by algorithms executing normative content. Empirical cognitive science directly contradicts this assumption: moral behaviour is generated by affective appraisal \cite{Crockett2016}, empathic resonance \cite{Decety2004}, attentional selection \cite{Pfattheicher2015}, and social-contextual modulation \cite{Vinciarelli2012,Haley2005}. These mechanisms constitute the \emph{evaluative topology} within which reasons, norms, and values acquire behavioural force.

\medskip

\noindent
The experiment demonstrates the consequences of violating LoA discipline. The Watching-Eye cue introduces a deontic expectation of accountability, yet the behavioural manifestation of this expectation is attenuated by the presence of a silent, non-agentic robot. This perturbation does not occur at the level of explicit reasoning or principle application; it occurs at the level of \emph{salience fields}, \emph{affective gradients}, and \emph{implicit social ontology}. Studies in HRI show that robots alter human social evaluation even in minimal-interaction contexts \cite{Bremner2022,Kuchenbrandt2011,Malle2016,Zlotowski2015,Banks2020}. Studies in SSP show that social cues reshape evaluation and behaviour via perceptual and affective channels \cite{Pentland2007,Vinciarelli2009,Picard1997}. The attenuation observed in the experiment is therefore a paradigmatic case of cross-LoA interference: a reflective norm (accountability) fails to exert its expected behavioural influence because the cognitive LoA at which it is realised has been deformed.

\medskip

\medskip
\noindent
\textbf{Interim Synthesis.} The analyses developed in Sections~3 and~4 converge on a single structural insight: classical Machine Ethics begins at the wrong place in the explanatory hierarchy. It treats reflective normative theories—Kantian maxims, utilitarian utilities, virtue-trait encodings, and rule-based constraint systems—as if they were \emph{mechanisms} of moral cognition, thereby collapsing Levels of Abstraction and obscuring the architectures that actually generate moral behaviour. Moral psychology and SSP reveal a different picture: moral action emerges not from the execution of principles but from the dynamic interaction of perceptual salience, affective resonance, attentional capture, embodiment, and social modulation. These low-LoA mechanisms constitute the \emph{evaluative topology} within which reasons, values, and obligations acquire behavioural force.

\noindent
Seen through this lens, the experiment in Chapter~\ref{chap:experimental_methods} functions as a decisive test case. A silent, non-agentic robot attenuates prosocial donation even in the presence of a strong accountability cue. This attenuation cannot be explained by any model that treats moral behaviour as propositional inference or rule retrieval. Instead, it discloses a deformation of the evaluative field itself: a disruption of empathic salience, a weakening of the Watching-Eye gradient, and a uniform directional shift across dispositional ecologies. The phenomenon is therefore diagnostic not of failed principle-application but of a perturbed cognitive--affective substrate.

\medskip

\noindent
The thesis therefore adopts a clear and principled stance: \emph{before we can design moral machines, we must understand how machines reshape human moral experience}. Classical Machine Ethics begins at the \emph{wrong end} of the explanatory hierarchy: it starts with reflective normative theory and assumes that these principles can be used as behavioural generators. A scientifically credible programme for moral AI must invert this order. It must first identify the empirical architecture that makes moral behaviour possible; second, model how artificial systems modulate the evaluative field in which that behaviour unfolds; and only then, on this empirically grounded basis, determine what forms of normative oversight or ethical design are justified.


\noindent
This reframing provides the conceptual orientation for the remainder of the chapter. The next sections examine the psychological, philosophical, and computational structures that classical Machine Ethics could not accommodate: the cognitive--affective machinery of moral judgment, the topological configuration of evaluative forces, and the cross-LoA perturbations induced by artificial co-presence. These resources form the foundation for a new, empirically grounded and topologically disciplined account of moral behaviour under synthetic presence.

% Section 5
\section{Evaluative Topology, Affective Architecture, and Synthetic Moral Perturbation}

\noindent
If Sections~3 and~4 establish that classical Machine Ethics fails by mislocating moral cognition at the wrong Level of Abstraction, the present section articulates the positive theoretical alternative: a \emph{topological} account of moral behaviour grounded in the cognitive--affective mechanisms identified by empirical psychology and Social Signal Processing. The central claim is that moral behaviour is governed not by the internal execution of principles, but by the dynamic configuration of an \emph{evaluative field}: a structured, multi-dimensional manifold shaped by gradients of salience, affective resonance, attentional pathways, contextual norms, and implicit social meaning. Ethical theories operate within this field as \emph{structural constraints}, not as generative mechanisms.

\medskip

\noindent
The notion of an evaluative topology synthesises several strands of empirical and philosophical insight. From moral psychology, we inherit the idea that affective and intuitive systems provide the primary route through which motivational salience attaches to moral cues \cite{Haidt2001,Greene2001,Crockett2016}. From SSP and affective computing, we gain the recognition that social cues—including gaze, morphological salience, co-presence, and implicit monitoring—modulate the perceptual filters and affective weights through which situations are evaluated \cite{Pentland2007,Vinciarelli2009,Picard1997}. From philosophy, we draw on the structural character of normative theories: deontological invariants, consequentialist gradients, virtue-based dispositional attractors, sentimentalist affective vectors, and contractualist justificatory equilibria. Reinterpreted through an LoA-sensitive framework, these are not rules to be executed but \emph{roles} played within a larger evaluative topology.

\medskip

\noindent
Within this topological framework, moral behaviour emerges from trajectories through the evaluative manifold. Attention generates local curvature; affect saturates regions of the field with motivational orientation; contextual cues deform or amplify gradients; and implicit social signals modulate the salience of norms, duties, and empathic responses. This framework dissolves the traditional philosophical opposition between rationalist and intuitionist models: rationalist structures become constraints on admissible trajectories, while intuitionist mechanisms determine the field through which those trajectories unfold. It also resolves the computational impasse: behaviour is not produced by any principle-encoding module, but by the system’s real-time navigation through a salience-weighted topology.

\medskip

\noindent
The experiment presented in Chapter~\ref{chap:experimental_methods} provides a decisive empirical probe into this architecture. The Watching-Eye cue creates a prosocial salience gradient by activating implicit accountability mechanisms \cite{Haley2005,Bateson2006,Pfattheicher2015}. Yet the introduction of a silent, non-agentic robot attenuates this gradient despite the absence of communicative intent or normative instruction. The perturbation arises because the robot’s ambiguous social ontology—perceptually agentic, ontologically indeterminate—reshapes the affective–attentional conditions through which the Watching-Eye cue exerts its effect. In this sense, synthetic presence functions as a \emph{field operator}: it modifies the salience landscape and thus alters the pathways through which moral perception becomes moral action.

\medskip

\noindent
Crucially, this perturbation is \emph{layer-sensitive}. The Prosocial--Empathic ecology showed the strongest attenuation because its evaluative topology depends most heavily on empathic resonance and interpersonal salience—the very mechanisms displaced by the robot’s presence. The Analytical--Structured ecology showed moderate attenuation because its field relies more on interpretive stability than affective reactivity; the robot destabilised its sense-making substrate without significantly disrupting affective curvature. The Emotionally Reactive ecology showed minimal change because its evaluative topology lacks stable gradients; perturbation at this LoA therefore produces negligible displacement. These differential effects underscore the central insight: \textit{synthetic presence perturbs the evaluative topology upstream of both principle and trait}.

\medskip

\noindent
This topological reading clarifies why classical Machine Ethics could not predict the observed phenomenon. Moral behaviour changes not because an encoded principle is misapplied, but because the evaluative field in which principles would acquire force has been deformed. Deontological norms cannot motivate action when the salience of accountability is suppressed; consequentialist gradients flatten when contextual meaning becomes ambiguous; virtue-based dispositions fail to express themselves when affective attractors lose curvature; sentimentalist mechanisms weaken when empathic resonance is displaced; and contractualist justificatory relations dissolve when the perceived social field becomes indeterminate. The experiment thus confirms the central thesis of this section: moral behaviour is \emph{field-sensitive}, and synthetic agents act as \emph{perturbation operators} on the evaluative topology.

\medskip

\noindent
Evaluative topology therefore provides the structural bridge needed to integrate normative theory, empirical psychology, and computational modelling. It offers a principled account of how high-LoA normative structures interface with low-LoA cognitive--affective machinery and explains why synthetic presence can reshape moral behaviour without expressing beliefs, intentions, or normative content. This sets the stage for the final section, which synthesises the chapter’s arguments into a unified model of machine-mediated moral cognition.

% Section 6
\section{Integrative Synthesis: Toward a Cognitive--Affective Model of Machine-Mediated Morality}

\noindent
The analyses developed across Sections~1--5 point toward a unified account of moral behaviour under synthetic presence. Classical Machine Ethics fails because it begins with reflective normative theories and assumes that these can serve as behavioural generators. Moral psychology shows that moral behaviour emerges from a cognitive--affective architecture defined by salience, attention, empathy, and contextual modulation. SSP and HRI demonstrate that artificial agents modulate these mechanisms even in the absence of interaction. Evaluative topology integrates these insights by representing moral behaviour as the product of trajectories through a salience-weighted, affectively structured evaluative field. The experiment reveals that synthetic presence perturbs this field, deforming the conditions under which moral salience becomes action.

\medskip

\noindent
This integrated framework yields three core claims that define the thesis’s contribution to the philosophy of moral AI:

\begin{enumerate}
	\item \textbf{Moral behaviour is generatively rooted in the cognitive LoA.}  
	Reflective ethical theories articulate justificatory structure, but moral action emerges from cognitive--affective dynamics. Ethical norms acquire behavioural force only when the evaluative field enables them to do so.
	
	\item \textbf{Artificial agents reshape the evaluative field before they act within it.}  
	Studies in HRI and SSP \cite{Malle2016,Kuchenbrandt2011,Bremner2022,Zlotowski2015,Pentland2007,Vinciarelli2009} show that synthetic presence modulates attention, empathy, and social meaning. The experiment confirms that these perturbations are sufficient to attenuate moral behaviour even when normative content remains unchanged.
	
	\item \textbf{A scientifically credible programme for moral AI must begin with evaluative topology, not normative codification.}  
	The field must reverse the methodological order: start with the empirical structure of moral cognition; analyse how artificial systems modulate this structure; and only then determine how normative oversight or ethical design may be justified. Computational systems cannot generate moral behaviour by executing principles unless the evaluative field has the curvature required to make such principles operative.
\end{enumerate}

\medskip

\noindent
This synthesis reframes the foundational commitments of moral AI research. Artificial systems should not be conceptualised as bearers of ethical rules or evaluative principles; they should be modelled as \emph{operators on the evaluative field} in which human moral cognition unfolds. The experiment demonstrates that even minimal robotic presence deforms salience gradients, attenuates empathic resonance, and weakens interpersonal accountability cues. These perturbations occur at the cognitive LoA, far upstream of explicit reasoning or reflective endorsement.

\medskip

\noindent
The result is a conceptual and methodological paradigm shift: \emph{moral AI must be grounded in the science of moral cognition, not in the logic of ethical theory}. Synthetic moral perturbation is not an anomaly; it is the natural consequence of a multi-layered architecture in which perceptual salience, affective openness, and contextual interpretation shape the pathways through which moral meanings become moral actions. Evaluative topology provides the theoretical vocabulary needed to describe this architecture, diagnose its vulnerabilities, and design artificial systems that stabilise rather than distort the moral field.

\medskip

\noindent
With this framework in place, the final part of the thesis articulates a global synthesis: a unified model of moral perturbation and ethical interpretation that consolidates the normative, psychological, and topological analyses developed across the project.

% Section 7
\section{Global Synthesis: From Inferential Displacement to Synthetic Moral Topology}

\noindent
The literature reviewed across this chapter establishes a series of convergent 
insights that collectively redefine the conceptual landscape for thinking about 
moral behaviour under artificial co-presence. Rather than offering isolated conclusions from discrete subfields, the evidence forms a coherent picture: 

"\textit{moral judgement and moral action emerge from a cognitively embedded, affectively structured, socially modulated evaluative field.}" 

Ethical theories supply 
reflective standards, but they do not generate behaviour; cognitive architecture 
does. Artificial agents, even when minimally interactive, participate in this 
architecture by modulating salience, affect, attention, and perceived social 
meaning. These insights, taken together, provide the intellectual foundation on 
which the remainder of the thesis builds.

\subsection*{From Question to Framework: A Literature-Grounded Progression}

\noindent
The guiding research question that motivates the empirical work of the thesis---%
whether synthetic presence can perturb the inferential transformation through 
which moral salience becomes action---arises directly from unresolved tensions 
in the literature. Classical Machine Ethics assumes that moral behaviour follows 
from the execution of encoded principles \cite{Bringsjord2006,Anderson2011}. 
Moral psychology shows that it does not \cite{Haidt2001,Greene2001,Cushman2013}. 
Social cognition and SSP demonstrate that social cues modulate moral behaviour 
via attentional capture and affective appraisal 
\cite{Pentland2007,Vinciarelli2009,Conty2016}. HRI studies reveal that artificial 
agents trigger many of these same modulations 
\cite{Kuchenbrandt2011,Malle2016,Bremner2022}. Yet these strands have not been 
integrated into a unified account capable of explaining \emph{how} artificial 
co-presence reshapes moral behaviour.

\noindent
The inferential displacement question is therefore not merely a research 
convenience; it emerges as the \emph{necessary organising problem} at the nexus 
of these literatures, where normative theory, moral psychology, computational 
modelling, and social robotics intersect.

\subsection*{Why a Multi-Hypothesis Framework Was Needed}

\noindent
The transition from a single research question to a structured hypothesis set is 
mandated by the heterogeneity of mechanisms identified in the literature. 
Existing research suggests at least three theoretically distinct pathways through 
which artificial agents may modulate moral behaviour:

\begin{enumerate}
	\item \textbf{Evaluative deformation:}  
	Studies on gaze cues, reputation systems, and social monitoring show that 
	attentional and affective gradients can be reshaped by subtle cues 
	\cite{Haley2005,Bateson2006,Pfattheicher2015}. This literature suggests that 
	synthetic presence may alter the curvature of the evaluative field.
	
	\item \textbf{Synthetic normativity:}  
	Research in HRI demonstrates that artificial agents can acquire quasi-normative 
	status by virtue of their perceived social ontology 
	\cite{Zlotowski2015,Kuchenbrandt2011,Malle2016}. This literature suggests that 
	robots may introduce new justificatory expectations without explicit agency.
	
	\item \textbf{Perturbation of inferential pathways:}  
	Neurocognitive and affective models show that moral judgements rely on the 
	integration of empathic resonance, attentional selection, and contextual 
	interpretation \cite{Decety2004,Greene2014}. Artificial presence may displace 
	these pathways by introducing competing social signals.
\end{enumerate}

\noindent
The literature therefore requires a multi-hypothesis framework: different domains 
predict different modes of perturbation, and no single mechanism is sufficient to 
explain the range of modulations documented in HRI, SSP, and moral psychology.

\subsection*{Theoretical Implications from the Literature Alone}

\noindent
When synthesised, the reviewed evidence supports three high-level claims about 
moral behaviour under artificial co-presence—claims that follow from \emph{the 
	literature itself}, independent of any empirical findings of the present thesis:

\begin{enumerate}
	\item \textbf{Moral behaviour is field-sensitive.}  
	Decades of work in moral psychology, affective neuroscience, and social 
	cognition show that moral action emerges from a structured evaluative field 
	defined by salience, attention, affect, and context 
	\cite{Greene2001,Haidt2001,Decety2004}.
	
	\item \textbf{Artificial agents modulate this field.}  
	HRI and SSP research demonstrate that artificial co-presence reshapes social 
	meaning, perceived accountability, empathic stance, and attentional allocation 
	\cite{Malle2016,Bremner2022,Vinciarelli2012,Zlotowski2015}. Artificial systems 
	therefore participate in the cognitive LoA rather than the normative one.
	
	\item \textbf{Machine Ethics, as traditionally formulated, cannot model this modulation.}  
	Encoding ethical principles at the reflective LoA cannot account for 
	cognitive--affective modulation at the perceptual LoA 
	\cite{Wallach2008,Arkin2009}. The literature makes clear that moral behaviour 
	cannot be generated, predicted, or regulated solely by propositional structures.
\end{enumerate}

\noindent
These points converge on a pivotal conclusion: a viable framework for moral AI 
must be grounded not in the normative \emph{content} of ethical theories but in 
the \emph{structural dynamics} of the evaluative field.

\subsection*{Conceptual Synthesis Box (Reframed as Literature-Driven)}

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Conceptual Synthesis: Why Prosocial Behaviour Is the Right Lens for Moral Modulation Studies]
		Across moral psychology, social cognition, HRI, and SSP, converging evidence 
		shows that prosocial behaviour is the behavioural endpoint of a cognitive--affective 
		architecture that transforms moral salience into action. Social cues, attentional 
		signals, empathic triggers, and embodied co-presence modulate this architecture 
		continuously. Because moral cognition is inherently practical, perturbations to its 
		perceptual, affective, or interpretive components manifest most reliably in 
		behavioural output. This literature therefore establishes prosocial action as a 
		methodologically robust proxy for detecting how artificial co-presence reshapes 
		the evaluative field in which moral meaning becomes behaviourally operative.
	\end{tcolorbox}
\end{center}

\subsection*{Final Literature-Grounded Transition}

\noindent
Through this synthesis, the chapter completes the theoretical task appropriate to 
a literature review: it maps the conceptual space, identifies structural tensions, 
and isolates the explanatory gaps that motivate empirical investigation. The 
review establishes that understanding moral behaviour under artificial co-presence 
requires an integrated framework grounded in cognitive architecture, affective 
dynamics, social signalling, and Level-of-Abstraction discipline. This framework 
sets the stage for the empirical analysis that follows, providing both the 
theoretical motivation and the interpretative resources required to examine how 
synthetic presence modulates the evaluative conditions under which moral 
perception becomes moral action.


\bigskip

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Literature-Driven Conclusion: Why Moral Behaviour Must Be Studied as a Field-Sensitive Phenomenon]
		The interdisciplinary literature reviewed in this chapter converges on a 
		single structural insight: \emph{moral behaviour cannot be explained by 
			principle-based models alone}. Across moral psychology, social cognition, 
		developmental studies, HRI, and Social Signal Processing, converging 
		evidence shows that moral action emerges from the interaction of 
		perceptual salience, affective appraisal, attentional modulation, 
		embodied cues, and socially mediated meaning. Ethical theories provide 
		reflective standards for justification, but the \emph{production} of moral 
		behaviour arises at a lower Level of Abstraction—within a cognitive–
		affective evaluative field shaped by context, embodiment, and social 
		signals. 
		
		This literature reveals three core commitments that guide the remainder 
		of the thesis: (i) moral behaviour is generated by field-like structures 
		of evaluation rather than by the execution of encoded principles; 
		(ii) artificial agents, even when passive or minimal in appearance, can 
		reshape these fields by altering attentional, affective, and social 
		conditions; and (iii) a scientifically credible framework for moral AI 
		must therefore begin from an empirically grounded model of moral 
		cognition, not from the direct implementation of normative theory.
		
		These commitments reposition Machine Ethics within a broader 
		scientific landscape: moral behaviour must be analysed as 
		\emph{field-sensitive}, \emph{contextually modulated}, and 
		\emph{cognitively grounded}. Any artificial system capable of influencing 
		human decision-making thereby participates—intentionally or not—in the 
		structural conditions under which moral salience becomes action. 
		Understanding this participation is the central task that motivates the 
		chapters that follow.
	\end{tcolorbox}
\end{center}


\noindent
Through this synthesis, the chapter completes the theoretical arc that began with the research question of inferential displacement. It demonstrates that synthetic agents reshuffle the very conditions under which moral salience becomes action, and it establishes the empirical and philosophical foundation on which the remainder of the thesis builds its general theory of moral perturbation.
