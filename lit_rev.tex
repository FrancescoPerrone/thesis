\chapter{Literature Review}
\label{chap:lit_rev}

% Section 1
\section{Introduction: Scope, Objectives, and Theoretical Commitments}

\noindent
This chapter establishes the conceptual and methodological terrain on which the remainder of the thesis proceeds. This review isn’t just a background filler; but ratgehjr a the first test looking for the the assumptions which the experimental results depend on, the levels of abstraction they operate at, the mechanisms they take for granted, and the gaps they leave unexplained. It lets us ask: 

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Wheather the synthetic presence really does modulate the path from perception to action, which existing frameworks can even see that phenomenon? And which ones are blind to it by design?}
	\end{leftbar}
\end{center}

\bigskip
\noindent
By examining the published work through that lens, we start to see an emerging pattern: almost all of classical Machine Ethics operates at the reflective level—principles, rules, deliberation—while the phenomenon we are studying unfolds at the cognitive level, upstream of reasoning. That mismatch isn’t an opinion; it’s a structural finding that the literature itself reveals.

\bigskip
\noindent
The aim here is therefore to reposition the study of moral behaviour under artificial co-presence---and the design of artificial moral systems more broadly---within a theoretically unified space at the intersection of \emph{Machine Ethics}, \emph{Computational Morality}, and \emph{Social Signal Processing} (SSP). Although these fields emerged from distinct disciplinary lineages, the experimental results presented in Chapter~\ref{chap:exp_methods} show that they now converge around a single problem: artificial agents, even when silent, passive, and non-interactive, \emph{modulate the evaluative conditions under which moral judgment and action unfold}. Understanding this phenomenon requires an integration of normative philosophy, moral psychology, computational modelling, and HRI.

Hence, the project takes root here. The literature review is the first piece of evidence. It shows that if we stay at the reflective level, we can’t even formulate the right kind of question, let alone explain the modulation we later observe experimentally (Chapter~\ref{chap:exp_methods}). That’s why the review matters so much—it’s the tool that tells us where the explanation has to live before you collect single data points.

\noindent
One of the core findings of the literature is that classical Machine Ethics starts from the wrong end of the problem. The whole tradition begins by taking high-level ethical theories—Kantian tests, utilitarian calculations, virtue templates, deontic logics—and trying to encode them as if they were models of moral agency \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008,Guarini2006,Allen2005}. 

\medskip
\noindent
But if we look closely at what those theories actually do, they are not descriptions of how humans produce moral behaviour. They are descriptions of how humans \emph{justify} moral behaviour after the fact. This distinction is explicit in modern moral philosophy: Kantian universalisability, utilitarian aggregation, and contractualist justification articulate reflective standards for assessing reasons, not cognitive processes for generating action \cite{SidgwickMethods,Scanlon1998,Korsgaard1996}. They operate at a very high Level of Abstraction: they tell you what counts as a good reason, \textit{not how a person comes to act in the first place} \cite{Floridi2008,Floridi2011}.

It should be noted that while most of what traditionally falls under Machine Ethics—Computational Morality, formal deontic systems, encoded utility functions—belongs to the “\textit{pre-LLM}’’ era, the limitation identified here does not evaporate with the advent of large language models. If anything, the arrival of LLMs makes the limitation more sharply visible.

\medskip
\noindent
Recent work demonstrates that LLMs can perform exceptionally well on reflective moral tasks: they generate sophisticated reasoning, balance competing principles, and provide normatively articulate justifications that map cleanly onto established ethical frameworks \cite{Jin2022Moral,Scherrer2023,Nguyen2023,Aher2023,Charlton2023}. They also exhibit high performance on benchmarked moral analogy tasks and moral classification challenges \cite{Hendrycks2021,Emelin2023}. But all of this ability is situated at the reflective Level of Abstraction: the linguistic, justificatory, post-hoc LoA.

And humans do not act morally at that level. On every empirically supported account of moral cognition—from social intuitionism \cite{Haidt2001,Haidt2007}, to dual-process theory \cite{Greene2001,Greene2014,Cushman2013}, to affective neuroscience \cite{Phelps2006,Decety2008,Zaki2012}, to embodied and socially embedded models \cite{Buon2016,Malle2016,Bremner2022}—moral behaviour is driven by salience, affect, perceptual appraisal, social cues, and attentional orientation, not by the explicit application of normative principles. These processes sit one LoA below the linguistic–justificatory space in which LLMs operate.

Thus, although we now live in a “post-LLM’’ era, the fundamental issue is not that pre-LLM Machine Ethics was technically limited or symbolically brittle. The deeper problem is that both pre-LLM Machine Ethics and modern LLMs operate at the wrong Level of Abstraction if the goal is to model, predict, or understand human moral behaviour. This is precisely the mismatch Floridi’s LoA discipline is designed to diagnose \cite{Floridi2008,Floridi2011}: moral justification and moral production belong to different descriptive orders. LLMs amplify the upper order; they leave the generative order untouched.

Chronologically, the pattern is straightforward:

\begin{itemize}
	\item \textbf{Pre-LLM Machine Ethics} attempted to encode normative principles directly—deontic rules, utility functions, virtue schemas—and encountered the reflective/cognitive mismatch documented extensively in the literature \cite{Moor2006,Wallach2008,Cervantes2020,Coeckelbergh2023}.
	\item \textbf{Post-LLM models} generate better principles, better explanations, and more articulate moral rhetoric, but they encounter the same mismatch, now at a higher level of linguistic sophistication \cite{Bender2021,Mittelstadt2019,Whittlestone2019,Andrus2023,Kasirzadeh2023}.
\end{itemize}

The chronology therefore does not mark a methodological revolution; it exposes the persistence of a category error. The assumption that moral behaviour is fundamentally a matter of reasoning or principle-application has survived unchallenged into the LLM era. But contemporary empirical evidence shows that humans rarely deploy such reasoning in the production of moral action \cite{Cushman2013,Haidt2001,Young2012}.

As several recent critical analyses emphasise, LLMs produce moral reasoning without moral cognition \cite{Kasirzadeh2023,Andrus2023,Gardner2024}. They resolve dilemmas fluently~\footnote{The distinction between reflective and generative Levels of Abstraction (LoAs) is crucial here. Moral justification, principle-balancing, and linguistic explanation occur at a reflective LoA \cite{Floridi2008,Floridi2011}. Human moral behaviour, by contrast, arises from perceptual, affective, and socially embedded processes documented across moral psychology and social neuroscience \cite{Haidt2001,Greene2001,Zaki2012,Decety2008}. Recent analyses of LLM-based moral reasoning confirm that these models excel at reflective justification but do not reproduce the generative cognitive–affective mechanisms that produce moral action \cite{Kasirzadeh2023,Andrus2023,Gardner2024}. The arrival of LLMs therefore intensifies—rather than resolves—the LoA mismatch at the core of Machine Ethics.}; they do not reproduce the cognitive–affective processes by which humans come to feel that something is a dilemma in the first place. Moral language is not moral experience. Reflective justification is not perceptual-affective appraisal.

That is the chronological insight, if one seeks it: the technologies have evolved dramatically, but the underlying LoA mismatch remains unchanged. The surface has shifted; the category error has not budged. And that’s really the hinge of this work:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Synthetic systems can now talk morality far better than they can participate in the conditions that shape moral action. The two are not the same.}
	\end{leftbar}
\end{center}


\bigskip
\noindent

What becomes interesting, especially now, is that artificial systems are not just reasoning in the abstract; they’re entering our environments. They’re in phones, homes, classrooms, offices. Their presence affects how we behave, how we interpret situations, how we allocate attention.

So the shift isn’t from ‘pre-LLM Machine Ethics’ to ‘post-LLM Machine Ethics.’ The shift is from seeing AI as an agent that reasons to seeing AI as an element in the cognitive ecology—something that reshapes the conditions in which human moral behaviour unfolds. Whether the system speaks like Kant or Shakespeare or your best friend is irrelevant if its presence still modulates the way people notice, feel, and act. That’s the axis that matters. That is the core of this work. And this is where the category error comes in. Machine Ethics assumes that the principles of an ethical theory can be treated as the cognitive machinery of a moral agent-- \textit{as if humans behave by running Kantian tests or utilitarian calculations in their heads}. 

But we know that isn’t how moral action is produced. Human behaviour comes from a much lower level: from what captures our attention, what feels salient, how we read a face or a tone, how empathy gets triggered, how the context shifts our sense of what matters. These processes are fast, intuitive, emotional, and deeply social~\cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}.

Decades of work in moral psychology and neuroscience demonstrate that intuitive, affectively laden processes precede and shape explicit moral judgement \cite{Greene2004,Haidt2001,Moll2002}. The intuitive, affectively charged processes come first \cite{Haidt2001,Greene2001,Decety2004,Cushman2013}. They shape the space in which explicit reasoning even becomes possible: before reflection begins, appraisal mechanisms, empathic resonance, salience attribution, and motivational tagging have already constrained the field of viable responses \cite{Moll2002,Crockett2016,Prinz2007}. The reflective story we tell afterwards might be coherent, but it is downstream of the machinery that actually drives behaviour \cite{Greene2004,Haidt2001}.

So when Machine Ethics takes ethical principles and treats them as if they were the generator of moral action, it is working at the wrong level entirely. It is replacing the justification of moral behaviour with the mechanism of moral behaviour, and those are not the same thing \cite{Korsgaard1996,Scanlon1998,Foot2001}. High-level principles articulate normative standards, but the processes that produce moral action operate at a far more fundamental cognitive--affective level \cite{Floridi2008,Floridi2011}.

\medskip
\noindent
So when one tries to design a “moral machine” by encoding Kant or utilitarianism, one collapses these two levels of abstraction. One is treating reflective principles as if they were psychological mechanisms. And the literature shows very clearly that they are not. Ethical theories explain why an action can be defended; they do not explain how moral behaviour is formed \cite{Prinz2007,Foot2001}. 

\medskip

\noindent
That is the central limitation the literature review exposes. It shows that classical Machine Ethics is methodologically elegant but cognitively misaligned. It is operating at the wrong level to even see the phenomenon we are investigating. As empirical work in moral psychology, affective neuroscience, SSP, and HRI repeatedly shows, the relevant causal structure lies in the evaluative substrate of salience, affect, and social interpretation—not in the reflective principles invoked after the fact \cite{Vinciarelli2009,Kuchenbrandt2011,Malle2016,Zlotowski2015}.


\medskip
\noindent
\noindent
Classical Machine Ethics is beautifully constructed—methodologically elegant, logically clean—but it’s operating at a level that’s cognitively out of sync with where moral behaviour actually happens. It starts from principles, from rules, from reflective argumentation \cite{Bringsjord2006,Anderson2011,Wallach2008,Arkin2009}. But the causal work—\textit{the thing} that actually drives behaviour—lives one layer down, in salience, emotion, attention, and social interpretation \cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}.

\medskip
\noindent
If we look at the wrong layer, we simply don’t see the phenomenon we’re investigating. And this problem carries over into what’s usually called Computational Morality, just in a different form. Whether it’s logic engines, preference aggregators, or the newer wave of LLM-based moral modelling, the assumption is the same: moral behaviour can be approximated by symbolic inference—by treating moral judgment as a reasoning problem \cite{Guarini2006,Allen2005,Arkoudas2005,Mittelstadt2019,Bender2021}.

\medskip
\noindent
But the last twenty years of empirical work tell a very different story. Most moral judgments don’t start with slow deliberation; they start with fast, intuitive, emotionally charged appraisal~\cite{Haidt2001,Greene2004,Cushman2013}. They’re shaped by who’s present, how someone looks at you, the tone in the room, what feels at stake, and the affective and social cues embedded in the environment \cite{Decety2004,Conty2016,Vinciarelli2009,Zlotowski2015}. It’s a messy, context-sensitive process \cite{Moll2002,Prinz2007}. When we try to model morality as if it were a chain of propositions—if A then B, if C then D— we are abstracting away the very machinery that actually produces behaviour in humans. And that’s the machinery our experiment shows can be shifted by the simple presence of a humanoid robot \cite{Kuchenbrandt2011,Malle2016,Bremner2022}.

\medskip
\noindent
In other words: the classical computational models are not wrong because the logic is bad. They’re wrong because they are modelling the wrong thing. They are trying to capture moral reasoning, when the real action is happening in the evaluative landscape that sits underneath moral reasoning. That’s the level where synthetic presence does its work.


In Chapter~\ref{chap:moral_primer} we make very explicit that: \textit{any model of moral behaviour that leaves out the cognitive–affective machinery and the social-signalling dynamics behind moral judgment is simply not describing human beings.} It becomes unstable both scientifically and philosophically. This is where the Level-of-Abstraction issue gets predominant. If we would take high-level moral theories—the reflective content, the principles, the rules—and treat them as if they were the psychological mechanism that produces moral behaviour, we would end up with theories that look elegant but don’t actually predict what people do. They explain justification, not behaviour.We would develop artefacts; models that fail not because the logic is wrong, but because they’re modelling the wrong layer of the system. This becomes plainly clear in the experiment in Chaper~\ref{chap:exp_methods}.

\medskip
\noindent
The robot we use has no beliefs, no goals, no intentions, and no communicative acts; it is not reasoning or attempting to influence participants. Yet research consistently shows that even minimally expressive or non-agentic robots modulate human social behaviour \cite{Kuchenbrandt2011,Bremner2022,Bainbridge2011,Malle2016}. These effects operate through changes in salience, attention, and perceived social presence rather than explicit reasoning \cite{HaleyFessler2005,Bateson2006,Phelps2006,Zaki2012}. Such upstream perturbations cannot be captured by rule-based, utility-theoretic, or propositional models of morality, which mislocate moral action in reflective reasoning rather than in the intuitive, affective systems documented across moral psychology \cite{Haidt2001,Greene2014,Cushman2013,Dancy2004,Cervantes2020}.


\noindent
At this stage, the literature reveals a point that no strand of classical Machine Ethics has convincingly addressed. If the aim is to understand how humans behave morally in the presence of artificial agents—and to model that behaviour in a form that artificial systems can meaningfully operationalise—then the foundational assumptions of the field must be re-examined. Principle-first approaches, whether deontic, utilitarian, or virtue-theoretic, presuppose that moral norms can be implemented as explicit rules or evaluative operators \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Yet empirical research in moral psychology and affective neuroscience shows that moral behaviour does not arise from rule application but from cognitively embedded processes of appraisal, salience detection, affective resonance, and social interpretation \cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}.

\medskip

\noindent
Thus, moral norms cannot be treated merely as rules to be encoded. They must be understood in terms of their \emph{topological function}: the way they structure constraints, gradients, and permissible trajectories within the evaluative field through which moral perception is transformed into action \cite{Foot2001,Aristotle_nicomachean,Prinz2007}. Norms operate at a reflective Level of Abstraction, specifying justificatory structure rather than cognitive mechanism \cite{Korsgaard1996,Scanlon1998,Floridi2008,Floridi2011}. Their behavioural influence depends on how they interact with, and are realised by, low-level cognitive–affective processes.

\medskip

\noindent
For the same reason, moral judgment cannot be modelled as pure reasoning or symbolic inference. Dual-process and intuitionist models demonstrate that intuitive, affectively charged appraisals precede reflective judgment and constrain the space of subsequent deliberation \cite{Greene2004,Haidt2001,Cushman2013}. Attention, empathic resonance, perceptual salience, and social–contextual modulation shape the evaluative landscape long before propositional reasoning becomes active \cite{Moll2002,Decety2004,Vinciarelli2009}.

\medskip

\noindent
Nor can artificial agents be treated as carriers or executors of moral values. Research in HRI and Social Signal Processing shows that artificial systems act primarily as \emph{modulators}—as elements within the environment that reshape salience, perceived social presence, accountability cues, and evaluative expectations \cite{Kuchenbrandt2011,Malle2016,Zlotowski2015,Bremner2022}. Their influence operates upstream of explicit judgment, altering the evaluative field within which moral decisions are formed.

\medskip

\noindent
Once the problem is reframed in this way, the broader picture becomes clear. The limitations of classical Machine Ethics are not failures of logic but failures of explanatory level. Its models operate at a reflective LoA and therefore cannot detect, let alone predict, the cognitive–affective perturbations that empirical research has consistently shown to drive moral behaviour. When the evaluative landscape is foregrounded, the phenomena that appeared mysterious or anomalous under classical formulations become theoretically tractable: synthetic presence exerts moral influence not by embodying values or executing principles, but by reshaping the generative conditions under which moral action emerges.

\noindent
What follows, then, is not merely a synthesis of existing work but a structural reorganisation of the field. By applying Floridi’s notion of a \emph{Level of Abstraction} \cite{Floridi2008,Floridi2011} to the foundations of Machine Ethics for the first time, the literature review demonstrates that the field has been operating at an explanatory level incapable of capturing the mechanisms that actually generate moral behaviour. Classical approaches begin with reflective ethical theories—deontic logics, utilitarian calculi, virtue templates—and treat these as if they were computational models of moral agency \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Yet moral psychology and affective neuroscience have shown consistently that moral action arises from perceptual salience, affective appraisal, attentional capture, and social meaning \cite{Haidt2001,Greene2001,Decety2004,Crockett2016,Conty2016}. Social Signal Processing and HRI research further reveal that artificial agents perturb precisely these low-level evaluative dynamics \cite{Vinciarelli2009,Kuchenbrandt2011,Malle2016,Zlotowski2015}. 

\medskip

\noindent
Through this reframing, the literature review achieves a clear result: it exposes a fundamental LoA mismatch at the heart of Machine Ethics and shows that no principle-first, rule-codification framework can access the phenomena under investigation. Moral norms operate at a reflective LoA, specifying justificatory relations \cite{Korsgaard1996,Scanlon1998}, whereas moral behaviour is produced at the cognitive LoA through the dynamic interplay of affect, salience, and social interpretation. By bringing these strands together, the review establishes an integrated conceptual framework in which \emph{synthetic presence} becomes intelligible as a perturbation of the evaluative field itself—a theoretical insight that classical Machine Ethics could not formulate, and a necessary foundation for interpreting the empirical results of this thesis.


\section{The Two Research Projects in Machine Ethics}

\noindent
Machine Ethics does not constitute a unified field in the way that English literature or molecular biology do. It lacks a single community, a shared methodology, and a cohesive disciplinary core. What the literature refers to as “Machine Ethics” is in fact an umbrella designation for two fundamentally different research programmes that ended up sharing a name. Their conflation is widespread in the literature, yet they operate at distinct Levels of Abstraction \cite{Floridi2008,Floridi2011} and aim to explain different phenomena.

\medskip

\noindent
The first programme is what I call \emph{Human–Machine Ethics}. This strand examines how humans think, feel, and behave in the presence of artificial agents. It encompasses questions of accountability, agency displacement, social influence, norm perception, and moral risk. Its empirical backbone comes from Human–Robot Interaction, media psychology, and Social Signal Processing. Evidence from these domains shows that artificial systems—whether humanoid robots, embodied agents, or even minimally interactive media—systematically modulate attention, empathy, prosociality, and interpersonal expectations merely through their presence \cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,Malle2016,Bremner2022,Zlotowski2015}. This research programme aligns directly with the phenomenon investigated in this thesis: the modulation of human moral behaviour by a robot’s silent co-presence.

\medskip

\noindent
The second programme is \emph{Computational Machine Ethics}. This project attempts to design machines that make ethically adequate decisions by embedding moral theories into computational architectures. Deontic logics, utilitarian optimisation engines, rule-based ethical governors, and virtue-inspired templates all fall under this category \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008,Guarini2006,Allen2005}. The central assumption is that moral behaviour can be generated by applying ethical principles at runtime, often via symbolic inference, constraint satisfaction, or rule execution. In this sense, Computational Machine Ethics treats moral judgement as a reasoning problem rather than as a perceptual–affective process.

\medskip

\noindent
The literature routinely conflates these two programmes, as if progress in one automatically informs the other. But they sit at different Levels of Abstraction and answer different explanatory questions: Human–Machine Ethics investigates how artificial systems modulate human evaluative processes, whereas Computational Machine Ethics attempts to construct artificial evaluative systems by formalising normative content.

\medskip

\noindent
The empirical results of this thesis underscore why this distinction is indispensable. Human–Machine Ethics predicts precisely the kind of modulation observed experimentally: even a non-interactive robot can reshape attentional and affective salience, thereby altering the evaluative conditions under which prosocial behaviour is generated \cite{Conty2016,Decety2004,Haidt2001}. Computational Machine Ethics, by contrast, is structurally incapable of recognising such modulation because it presupposes that moral behaviour is produced by reflective, principle-driven reasoning—an assumption contradicted by decades of work in moral psychology and affective neuroscience \cite{Greene2001,Greene2004,Crockett2016,Prinz2007}.

\medskip

\noindent
Thus, the apparent lack of unity in “Machine Ethics” is not an artefact of interpretation but an accurate reflection of the field’s conceptual structure. The label obscures two independent activities: one empirically grounded, concerned with how humans behave in sociotechnical environments; the other formally oriented, concerned with encoding ethical principles into artificial agents. Without maintaining this distinction, research risks becoming blind to the very phenomenon contemporary AI and robotics force us to confront: that artificial agents, even when passive, \emph{modulate the evaluative field} through which human moral decisions take shape.

\section{A Clarifying Perspective on Where This Work Belongs—and Where the Field Must Go}
\noindent
It is tempting to ask where this research “belongs.” Does it fall under Affective Computing, with its emphasis on computational models of emotion \cite{Picard1997}? Does it align with Human–Robot Interaction, where the behavioural consequences of artificial social agents are examined \cite{Kuchenbrandt2011,Malle2016,Bremner2022}? Or does it sit within moral psychology, which has spent decades analysing the cognitive and affective substrates of moral behaviour \cite{Haidt2001,Greene2001,Decety2004,Crockett2016}? Each discipline contributes an essential piece, but none, on its own, provides the conceptual framework needed to understand the phenomenon at stake. For the purposes of this thesis, the disciplinary label is secondary; the primary task is the conceptual clarification that makes the inquiry possible.

\medskip

\noindent
The central confusion this thesis confronts is not empirical but conceptual. For nearly two decades, work collected under the name “Machine Ethics” has blurred two fundamentally distinct enterprises: understanding how humans behave morally in sociotechnical settings, and designing machines that behave according to encoded ethical theories. These projects occupy different Levels of Abstraction \cite{Floridi2008,Floridi2011}, draw on different forms of evidence, and target different explanatory aims. Treating them as a single field has produced a methodological entanglement in which elegant theories obscure the very phenomena they are meant to illuminate.

\medskip

\noindent
The distinction becomes clear once the discipline of Levels of Abstraction is applied. Human moral behaviour emerges at the cognitive LoA: it is shaped by perceptual salience, affective resonance, attentional dynamics, and social-cue interpretation \cite{Haidt2001,Greene2004,Decety2004,Conty2016}. Ethical theories—Kantian, utilitarian, contractualist—operate at a reflective LoA concerned with justification rather than generation \cite{Korsgaard1996,Scanlon1998}. When researchers treat high-LoA normative principles as if they were low-LoA psychological mechanisms, the result is not an incomplete theory but an artefact: a framework unable to predict behaviour, accommodate perturbations, or explain modulation phenomena.

\medskip

\noindent
The experimental findings of this thesis make this point explicit. A humanoid robot with no beliefs, goals, or communicative acts nevertheless alters the evaluative conditions under which humans convert moral perception into prosocial action. Such modulation does not arise from reflective reasoning; it arises from shifts in salience, affective alignment, and attentional orientation \cite{Greene2001,Decety2004,Conty2016}. Any framework that models moral action as rule retrieval, utility computation, or principle execution remains blind to these dynamics because it operates at the wrong LoA.

\medskip

\noindent
This is why the disciplinary categorisation of the work is not the central issue. The point is not where the research should be filed but what becomes visible once conceptual discipline is restored. Through this lens, the field of Machine Ethics reorganises itself. \emph{Human–Machine Ethics} emerges as an empirically grounded inquiry into how artificial agents modulate human evaluative processes \cite{Vinciarelli2009,Bremner2022,Zlotowski2015}. \emph{Computational Machine Ethics} reveals itself as a reflective programme concerned with principled design, centred on formalisms such as deontic logic \cite{Bringsjord2006,Anderson2011}, utility maximisation \cite{Arkin2009}, and virtue-engineering \cite{Wallach2008}. Both are legitimate, but conflating them obscures the cognitive phenomena that modern AI and robotics bring to the foreground.

\medskip

\noindent
Clarification, however, is only the first step. Once the LoA distinction is restored, one must ask what research agenda follows. The answer is both more modest and more ambitious than any principle-encoding programme. Moral behaviour is not computed; it is formed. It emerges from a dynamic evaluative field structured by affective gradients, perceptual cues, attentional flows, and socially mediated expectations \cite{Haidt2001,Greene2004,Decety2004,Vinciarelli2009}. Artificial agents—robots, avatars, conversational AIs—modulate this field simply by entering it. A scientifically credible programme for moral AI must therefore begin not with ethics as a set of principles but with the architecture of moral cognition.

\medskip

\noindent
Three consequences follow immediately. \textbf{First}, empirical grounding becomes non-negotiable. Any model of moral behaviour must integrate findings from moral psychology, affective neuroscience, developmental research, HRI, and Social Signal Processing. A theory that cannot accommodate the influence of gaze, posture, co-presence, or anthropomorphic cues cannot accommodate human moral behaviour \cite{Haley2005,Bateson2006,Conty2016}. \textbf{Second}, artificial agents must be modelled as operators, not reasoners: their role is not to apply rules but to modulate the evaluative conditions under which humans act \cite{Kuchenbrandt2011,Zlotowski2015,Malle2016}. \textbf{Third}, normative theory must be interpreted topologically rather than procedurally: norms specify constraints, gradients, and attractors in the evaluative space through which behaviour flows \cite{Foot2001,Aristotle_nicomachean,Prinz2007}.

\medskip

\noindent
This reframing also answers the practical question often posed by engineers: what is the actionable takeaway? The takeaway is not a new ethical theory, nor a list of rules to embed in code. It is the recognition that artificial agents shape human moral behaviour not by argument but by presence, not by reasoning but by salience, not by principles but by perceptual modulation. Designing systems without understanding the evaluative field they inhabit is a form of conceptual blindness.

\medskip
\noindent
The future of moral AI does not lie in machines that reason like philosophers, but in machines that coexist with humans in ways that can be predicted, understood, and—when necessary—constrained. Any credible programme must therefore begin where moral behaviour itself begins: within the evaluative machinery that transforms perception and affect into action.


% Section 3
\section{Moral Psychology and Moral Philosophy: Cognitive--Affective vs.\ Rationalist--Intuitionist Models}

\noindent
Once the conceptual confusion is removed, the next step is to examine the machinery that actually produces moral behaviour. Here the empirical story is remarkably consistent. For nearly two decades, work in moral psychology, affective neuroscience, and behavioural science has converged on a single conclusion: moral judgment is not primarily a reasoning task but a \emph{dual-process system}. Fast, intuitive, emotionally charged processes perform the bulk of the causal work. They respond to perceptual salience, attentional capture, empathic resonance, and situational demands \cite{Haidt2001,Greene2001,Greene2004}. Slower, reflective processes intervene later—often to justify, refine, or override the initial intuitive appraisal—but the initial appraisal performs the primary generative role \cite{Cushman2013,Decety2004,Crockett2016}.

\medskip

\noindent
This picture is reinforced by the major theoretical models in the field. Haidt’s Social Intuitionist Model \cite{Haidt2001}, Greene’s neurocognitive dual-process framework \cite{Greene2001,Greene2004,Greene2014}, and Cushman’s action-based inference models \cite{Cushman2013} all converge on the claim that moral evaluation begins with rapid, affectively valenced appraisals long before explicit reasoning is engaged. Neuroscientific findings corroborate this: affective tagging, motivational relevance, empathy circuitry, and social-interpretive processes are recruited early, often prior to conscious deliberation \cite{Moll2002,Decety2004,Conty2016}.

\medskip

\noindent
This stands in sharp contrast with the philosophical traditions on which classical Machine Ethics has historically relied. Kantian ethics, utilitarian frameworks, and contractualism articulate \emph{justificatory} structures: universalisability conditions, value aggregation procedures, or principles governing the exchange of reasons \cite{Korsgaard1996,Scanlon1998}. They are not intended as accounts of the psychological mechanisms that \emph{produce} moral judgments. As the philosophers themselves emphasise, these theories operate at a reflective Level of Abstraction; they describe the standards by which actions can be defended, not the cognitive architecture through which actions arise.

\medskip

\noindent
Machine Ethics, however, adopted only this reflective dimension and treated it as though it described the entire system. It assumed that humans behave morally by applying principles, and that artificial agents could do likewise by encoding those principles directly into computational structures \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. But the empirical literature shows decisively that moral behaviour is not generated by rule application. It emerges from a cognitive--affective substrate shaped by salience, emotion, attention, embodiment, and social interpretation.

\medskip

\noindent
This empirical fact explains why studies of human moral behaviour in context—across HRI, media psychology, and Social Signal Processing—identify recurrent patterns governed by attentional capture, affective resonance, perceived monitoring, and contextual meaning \cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,Malle2016}. Consider the Watching-Eye effect: people alter their behaviour when exposed to minimal cues of observation, even a pair of stylised eyes \cite{Haley2005,Bateson2006,Dear2019}. The shift is not the result of endorsed rules but of subtle environmental modulation of evaluative posture.

\medskip

\noindent
This cognitive level—the level of salience, empathy, vigilance, and contextual modulation—is precisely where moral behaviour is shaped. It is also where the attenuation effect in our experiment resides. The humanoid robot does not reason, speak, or request anything; nonetheless, its silent co-presence perturbs the evaluative field sufficiently to alter prosocial action. This is the cognitive--affective layer in operation, the layer classical Machine Ethics never modelled.

\medskip

\noindent
What follows from this is analytically unavoidable. If moral behaviour emerges from perceptual salience, affective pull, attentional alignment, and social interpretation, then computational models that treat morality as rule-following or propositional inference are modelling the wrong phenomenon. They are elegant but descriptively incomplete: they capture the reflective Level of Abstraction while missing the cognitive Level of Abstraction entirely.

\medskip

\noindent
This is why the discussion moves next to Levels of Abstraction. Once the mismatch is recognised, it becomes clear that many of the philosophical debates and engineering efforts in Machine Ethics were conducted at an inappropriate explanatory level from the outset. The remainder of the thesis unpacks the consequences of this realisation and reconstructs a framework in which moral cognition, evaluative topology, and synthetic presence can be understood in principled alignment.

\noindent
With this distinction in place, the argument can now shift from diagnosing the structural error in classical Machine Ethics to examining the positive framework required to replace it.

% Section 4
\section{Levels of Abstraction and the Failure of Machine Ethics}

\noindent
The conceptual tool that dissolves much of the confusion in Machine Ethics is Floridi’s notion of a \emph{Level of Abstraction} (LoA) \cite{Floridi2008,Floridi2011}. The idea is structurally simple but analytically powerful: any explanation requires specifying the level at which a system is being described. The LoA determines which variables are observable, the appropriate grain of detail, and the kinds of explanations that can legitimately be offered. Ethical theories operate at a high, reflective LoA: they articulate justificatory structures—principles, universalisation tests, value aggregation procedures, and reason-giving relations \cite{Korsgaard1996,Scanlon1998}. Moral psychology, by contrast, operates at a lower, cognitive LoA: it investigates the mechanisms that \emph{generate} moral judgment, including perceptual salience, affective appraisal, attentional dynamics, and social meaning \cite{Haidt2001,Greene2001,Greene2004,Cushman2013,Decety2004}.

\medskip

\noindent
Confusion arises when content belonging to one LoA is treated as if it were the mechanism operating at another. If reflective theories are misread as cognitive architectures, the distinction collapses, and with it the capacity to explain behavioural phenomena. Classical Machine Ethics has repeatedly committed this collapse for nearly two decades. By taking the principles of Kantian, utilitarian, or virtue-theoretic ethics and treating them as if they described the internal processes that produce moral behaviour, the field implicitly assumed that moral agents—human or artificial—act by applying principles \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. But these principles occupy the reflective LoA: they explain \emph{why} an action might be defensible, not \emph{how} a moral judgment is generated.

\medskip

\noindent
When these reflective principles are used as behavioural generators—as algorithms meant to produce moral action—the resulting models are elegant but fundamentally misaligned with human moral cognition. Real moral behaviour does not follow from propositional logic or rule execution. It emerges from what may be described as the \emph{evaluative topology}: the structured field of salience gradients, affective forces, attentional pathways, and social interpretations that determine what appears morally significant in the moment \cite{Haidt2001,Greene2014,Decety2004,Vinciarelli2009}. These low-level mechanisms—affective appraisal, empathic resonance, vigilance, contextual modulation—form the terrain within which high-level principles even acquire meaning.

\medskip

\noindent
The experimental findings of this thesis show precisely what happens when LoA discipline is violated. In the Watching-Eye paradigm, the accountability cue ordinarily increases prosocial behaviour \cite{Haley2005,Bateson2006,Dear2019}. Yet when a silent, non-agentic humanoid robot is introduced into the environment, this effect is attenuated. No reasoning, communication, belief, or intention is involved. The modulation arises from presence alone: the robot perturbs the evaluative field by shifting salience, affective alignment, and perceived social ontology \cite{Kuchenbrandt2011,Malle2016,Zlotowski2015}. The accountability cue loses traction not because a principle is misapplied, but because the cognitive substrate on which it depends has been displaced.

\medskip

\noindent
This synthesis yields a clear conclusion: moral action does not originate in the execution of principles but emerges from the dynamic interaction of perceptual, affective, and social processes. Classical Machine Ethics begins at the wrong point in the explanatory hierarchy. It treats high-level normative theories as if they were low-level cognitive mechanisms and thereby becomes blind to the central phenomenon that contemporary sociotechnical environments introduce: artificial agents modulating human evaluative fields through their mere presence.

\medskip

\noindent
The thesis therefore advances a strong and methodologically grounded claim: \emph{before we can design moral machines, we must understand how machines reshape human moral experience}. This requires inverting the traditional order of explanation. The task is not to begin with ethical theory and push downward, but to begin with the empirical architecture of moral cognition, determine how artificial agents perturb it, and only then ask what forms of ethical oversight or design constraint are justified.

\medskip

\noindent
Once Levels of Abstraction are applied, the path forward becomes clear. We can distinguish coherent questions from incoherent ones, identify which debates were aimed at the wrong level of the system, and recover the conceptual clarity necessary for progress. More than any single empirical result, this restoration of LoA discipline is the tool that allows the broader project of moral AI to proceed in the right direction.


\section{Evaluative Topology, Affective Architecture, and Synthetic Moral Perturbation}

\noindent
If the preceding sections establish that classical Machine Ethics operates at the wrong Level of Abstraction (LoA), the task now is to articulate the positive alternative: a topological account of moral behaviour grounded in the cognitive--affective mechanisms documented in empirical psychology \cite{Haidt2001,Greene2001,Greene2004,Cushman2013,Decety2004,Crockett2016} and in the social-modulatory processes identified by Social Signal Processing and HRI \cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,Malle2016,Bremner2022}.  

\medskip

\noindent
The central thesis of this section is that moral behaviour does not arise from the execution of encoded principles. Instead, it emerges from the dynamic configuration of an \emph{evaluative field}: a structured, multidimensional landscape shaped by gradients of salience, affective resonance, attentional pathways, contextual norms, and implicit social meaning. Ethical theories operate within this field not as algorithmic generators but as high-LoA structural constraints \cite{Korsgaard1996,Scanlon1998}. Their force depends on how they are realised within the cognitive--affective dynamics through which moral perception becomes moral action.

\subsection{The Evaluative Field}

\noindent
The notion of an evaluative topology synthesises three major strands of established research.

\medskip
\noindent
\textbf{(1) Moral psychology: affect, intuition, appraisal.}  
Dual-process theory \cite{Greene2001,Greene2004,Greene2014} and the Social Intuitionist Model \cite{Haidt2001} show that moral evaluation begins with rapid, affectively valenced appraisals. Affective tagging, empathic resonance, and motivational relevance are recruited early \cite{Decety2004,Crockett2016,Moll2002}. Attentional capture, perceptual salience, and intuitive heuristics structure the evaluative space long before reflective reasoning is engaged.

\medskip
\noindent
\textbf{(2) Social Signal Processing and affective computing: cue modulation.}  
Work in SSP demonstrates that gaze direction, morphological cues, co-presence, and implicit monitoring reshape attentional and affective weighting long before explicit cognition intervenes \cite{Pentland2007,Vinciarelli2009,Conty2016}. HRI studies confirm that humanoid robots and artificial agents modulate social meaning and perceived agency through mere presence \cite{Kuchenbrandt2011,Malle2016,Zlotowski2015,Bremner2022}.

\medskip
\noindent
\textbf{(3) Normative theory: structural constraints.}  
Philosophical ethics contributes the insight that moral theories provide structural invariants—deontological constraints \cite{Korsgaard1996}, consequentialist gradients, virtue-theoretic attractors \cite{Foot2001,Hursthouse1999}, sentimentalist affective vectors \cite{Prinz2007,Slote2010}, and contractualist justificatory relations \cite{Scanlon1998}. These normative forms define the shape of the evaluative field but do not generate behaviour directly.

\medskip

\noindent
Reinterpreted through LoA-sensitive analysis \cite{Floridi2008,Floridi2011}, these strands form a coherent architecture: high-LoA normative structures supply the constraints; low-LoA cognitive--affective mechanisms determine the trajectories; and social signals reshape the field within which both operate.

\subsection{Moral Behaviour as Trajectory}

\noindent
Within this topological framework, moral behaviour is best understood as movement through an evaluative manifold.  

\begin{itemize}
	\item \textbf{Attention} introduces local curvature by amplifying or suppressing cues \cite{Pfattheicher2015}.  
	\item \textbf{Affect} saturates regions of the field with motivational energy \cite{Crockett2016,Decety2004}.  
	\item \textbf{Contextual cues} deform gradients, shifting the relative weight of obligations, norms, and expectations \cite{Haidt2001,Nettle2013}.  
	\item \textbf{Social signals} modulate perceived accountability and interpersonal meaning \cite{Haley2005,Bateson2006,Dear2019}.  
\end{itemize}

\noindent
This model dissolves the rationalist–intuitionist divide. Rationalist structures do not compete with intuitive mechanisms; they operate at different LoAs. The reflective domain imposes structural constraints, while the cognitive--affective domain determines how the system actually moves within those constraints \cite{Greene2014,Scanlon1998}.

\subsection{Synthetic Presence as Field Operator}

\noindent
The experiment presented in Chapter~\ref{chap:exp_methods} provides an empirical probe into this architecture. The Watching-Eye cue ordinarily induces a prosocial salience gradient via implicit social monitoring \cite{Haley2005,Bateson2006}.  

\noindent
Yet the introduction of a silent, non-agentic robot attenuates this gradient. The effect does not originate in reasoning or principle-application. It arises from a deformation of the evaluative field itself. The robot’s ambiguous social ontology—perceptually agentic but ontologically indeterminate—reshapes the affective and attentional conditions through which the Watching-Eye cue acquires behaviour-guiding force \cite{Kuchenbrandt2011,Malle2016,Zlotowski2015}. In this sense, synthetic presence acts as a \emph{field operator}: its mere co-presence modifies the salience landscape and alters the trajectory from moral perception to moral action.

\medskip
\noindent
Crucially, the perturbation is \emph{disposition-sensitive}.  
\begin{itemize}
	\item The \textbf{Prosocial--Empathic ecology} exhibits the strongest attenuation, reflecting its dependence on empathic resonance and interpersonal salience—the very mechanisms displaced by synthetic presence.  
	\item The \textbf{Analytical--Structured ecology} shows moderate attenuation, consistent with reliance on interpretive coherence rather than affective pull.  
	\item The \textbf{Emotionally Reactive ecology} shows minimal change, as its evaluative landscape lacks stable gradients onto which perturbation could anchor.  
\end{itemize}

\noindent
These differential effects underscore the core insight: synthetic presence perturbs moral behaviour \emph{upstream} of principle, trait, and deliberation.

\subsection{Topology and the Limits of Machine Ethics}

\noindent
This topological analysis explains why classical Machine Ethics could not predict the observed phenomenon. Moral behaviour under synthetic presence does not change because a rule is misapplied or because deliberation fails. It changes because the evaluative field in which principles acquire force has shifted.

\begin{itemize}
	\item Deontological norms lose traction when accountability salience collapses \cite{Bateson2006,Pfattheicher2015}.  
	\item Consequentialist gradients flatten when contextual meaning becomes ambiguous \cite{Nettle2013,Ekstrom2012}.  
	\item Virtue-theoretic dispositions cannot express themselves when affective attractors weaken \cite{Hursthouse1999,Foot2001}.  
	\item Sentimentalist mechanisms fade when empathic resonance is displaced \cite{Prinz2007,Slote2010}.  
	\item Contractualist justificatory relations dissolve when the perceived social field becomes indeterminate \cite{Scanlon1998}.  
\end{itemize}

\noindent
The experiment therefore confirms the structural thesis: moral behaviour is field-sensitive, and synthetic agents act as perturbation operators on the evaluative topology.

\subsection{Toward a Unified Framework}

\noindent
The concept of evaluative topology provides precisely the integrative framework that Machine Ethics has lacked. It offers the structural bridge linking normative theory, empirical psychology, and computational modelling. It clarifies how high-LoA normative structures interface with low-LoA cognitive--affective mechanisms, and why artificial agents can reshape moral action without expressing beliefs, intentions, or normative content.

\noindent
This framework completes the foundational turn of the thesis. The subsequent chapters build on this topological architecture to formalise a general model of machine-mediated moral cognition—one in which artificial systems are not ethical reasoners but modulators of the evaluative conditions through which moral meaning gains behavioural expression.


% Section 6
\section{Integrative Synthesis: Toward a Cognitive--Affective Model of Machine-Mediated Morality}

\noindent
The analyses developed across this chapter converge on a unified account of moral behaviour under artificial co-presence. Classical Machine Ethics begins with reflective normative theories and treats them as behavioural generators \cite{Bringsjord2006,Anderson2011,Arkin2009}. Moral psychology shows that moral action instead emerges from a cognitive--affective architecture grounded in salience, attention, empathy, and contextual modulation \cite{Haidt2001,Greene2001,Cushman2013}. Work in HRI and SSP demonstrates that artificial agents modulate these mechanisms through minimal social cues \cite{Vinciarelli2009,Malle2016,Kuchenbrandt2011,Zlotowski2015}. Evaluative topology integrates these insights by modelling moral behaviour as trajectories through a salience-weighted, affectively structured field. The experiment confirms this: synthetic presence perturbs the evaluative field upstream of deliberation, thereby attenuating prosocial action.

\medskip

\noindent
Three core conclusions follow from the literature:

\begin{enumerate}
	\item \textbf{Moral behaviour is generated at the cognitive LoA.}  
	Reflective ethical theories articulate standards of justification \cite{Korsgaard1996,Scanlon1998}, but empirical work shows that behaviour is produced by low-LoA affective and social mechanisms \cite{Greene2001,Decety2004,Conty2016}. Norms gain behavioural force only when the evaluative field affords it.
	
	\item \textbf{Artificial agents reshape the evaluative field before they act within it.}  
	SSP and HRI research indicates that presence alone modulates attention, empathy, vigilance, and perceived social meaning \cite{Pentland2007,Vinciarelli2009,Kuchenbrandt2011,Bremner2022}. The experimental attenuation effect confirms this literature-driven prediction.
	
	\item \textbf{A viable programme for moral AI must begin with evaluative topology.}  
	The literature shows that computational systems cannot generate moral behaviour through principle execution alone \cite{Wallach2008,Arkin2009}. Normative codification must be constrained by a model of the cognitive--affective architecture through which moral behaviour is actually formed.
\end{enumerate}

\noindent
These claims collectively reframe the foundational commitments of moral AI. Artificial systems cannot be conceptualised merely as executors of moral rules; they must be understood as \emph{operators on the evaluative field} within which human moral cognition unfolds. Synthetic presence deforms salience gradients, attenuates empathic resonance, and weakens accountability cues—perturbations that occur far upstream of explicit reasoning.

\medskip

% Section 7
\section{Global Synthesis: From Inferential Displacement to Synthetic Moral Topology}

\noindent
The literature reviewed in this chapter reveals a coherent picture: moral judgment and action arise from a cognitively embedded, affectively structured, socially modulated evaluative field \cite{Haidt2001,Greene2001,Decety2004,Vinciarelli2009}. Ethical theories supply reflective standards, but they do not generate behaviour; cognitive architecture does. Artificial agents participate in this architecture by shaping salience, affect, and perceived social meaning \cite{Malle2016,Bremner2022}.

\subsection{From Question to Framework}

\noindent
The guiding research question—whether synthetic presence can perturb the inferential transformation through which moral salience becomes action—emerges naturally from unresolved tensions in the literature. Machine Ethics assumes that behaviour follows from principle execution \cite{Bringsjord2006,Anderson2011}; moral psychology shows it does not \cite{Haidt2001,Cushman2013}. SSP reveals that social cues modulate evaluative processes \cite{Pentland2007,Vinciarelli2009}. HRI shows that artificial agents evoke these cues through minimal presence \cite{Kuchenbrandt2011,Malle2016}. Yet these strands have rarely been synthesised.

\subsection{Why a Multi-Hypothesis Framework Was Needed}

\noindent
The literature identifies three distinct mechanisms through which artificial agents may modulate moral behaviour:
\begin{enumerate}
	\item \textbf{Evaluative deformation} via shifts in salience, monitoring, and affective weighting \cite{Haley2005,Bateson2006,Pfattheicher2015}.
	\item \textbf{Synthetic normativity} arising from the perceived social ontology of robots \cite{Zlotowski2015,Kuchenbrandt2011,Malle2016}.
	\item \textbf{Perturbation of inferential pathways} through displacement of empathy, attention, or contextual interpretation \cite{Decety2004,Greene2014}.
\end{enumerate}

\noindent
No single mechanism captures the phenomenon; a multi-hypothesis framework is required to align the interdisciplinary evidence.

\subsection{What the Literature Alone Establishes}

\noindent
Across the reviewed domains, three findings are robust:

\begin{enumerate}
	\item \textbf{Moral behaviour is field-sensitive}, emerging from salience, affect, attention, and contextual cues \cite{Greene2001,Haidt2001,Decety2004}.
	\item \textbf{Artificial agents modulate this field} by altering social meaning, vigilance, and empathic stance \cite{Malle2016,Bremner2022,Vinciarelli2012}.
	\item \textbf{Classical Machine Ethics cannot model this modulation}, because principle-based formalisms ignore the cognitive LoA where behaviour is actually generated \cite{Wallach2008,Arkin2009}.
\end{enumerate}

\noindent
From this, a literature-driven conclusion follows: a viable framework for moral AI must be grounded not in normative content but in the structural dynamics of the evaluative field.

\noindent
The literature review exposes a categorical error at the foundation of classical Machine Ethics. Across two decades of work, the same misalignment recurs: principles drawn from ethical theory—Kantian universalisability tests, utilitarian utilities, virtue-theoretic templates—are treated as if they were the psychological mechanisms that generate moral behaviour \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. Yet the literature makes clear that these operate at fundamentally different Levels of Abstraction. Reflective norms articulate \emph{conditions of justification} \cite{Korsgaard1996,Scanlon1998}; cognitive--affective systems explain \emph{behavioural production} \cite{Haidt2001,Greene2001,Cushman2013}. Frameworks that collapse these levels cannot predict or explain human moral behaviour, particularly under synthetic presence. The review makes this structural failure explicit.

\medskip

\noindent
The review also reveals a neglected architecture: moral cognition emerges from an evaluative field shaped by affect, salience, and social signalling. When empirical findings are placed side by side—across moral psychology \cite{Haidt2001,Greene2004}, affective neuroscience \cite{Decety2004,Crockett2016}, Social Signal Processing \cite{Pentland2007,Vinciarelli2009}, and Human--Robot Interaction \cite{Kuchenbrandt2011,Malle2016,Bremner2022}—a convergent picture becomes visible. Moral judgments originate in rapid, affect-laden appraisal; attentional dynamics determine which cues become morally salient; social signals such as gaze, posture, and co-presence modulate evaluative weighting; and explicit reasoning intervenes only downstream. This interdisciplinary convergence exposes a unified evaluative architecture that classical Machine Ethics never incorporated and could not accommodate.

\medskip

\noindent
Finally, the review identifies the theoretical gap that motivates the experiment. Once the evaluative architecture is made explicit, a precise, previously unformulated question emerges: \emph{can synthetic presence perturb the evaluative field upstream of explicit moral reasoning?} No existing Machine Ethics framework even poses this question, because none operate at the LoA where such perturbations occur. The literature review therefore performs an essential scientific function: it isolates the causal layer in which moral behaviour is generated and shows that current models fail to explain modulation at this level. The empirical study is designed explicitly to probe this gap.

\medskip

\noindent
In short, the literature review demonstrates that the field has been asking the wrong questions at the wrong level of abstraction; it identifies the level at which the genuine causal machinery of moral behaviour operates; and it isolates the precise phenomenon requiring empirical investigation. It clears the conceptual ground on which the remainder of the thesis rests and provides the foundation for a new account of moral behaviour under synthetic presence. In this project, the literature review is not merely preparatory; it constitutes the first scientific result.

