\chapter{Conclusion}
\label{chap:conclusion}

Think back to the question that opened this thesis. Not the formal version with symbols and indices, but the simpler one that sits beneath it:

\begin{center}
	\begin{leftbar}
		\textit{Can the mere presence of a synthetic, non-agentic entity perturb the 
			pathway through which morally salient cues become action?}
	\end{leftbar}
\end{center}

Everything that followed—the topology, the dispositions, the perturbation 
operator—was built to make that question answerable without handwaving.  
But the question itself remains disarmingly small. It asks whether a body that 
does not think, does not intend, and does not judge can nonetheless shift the 
trajectory from perception to (\textit{moral}) action.  

It is the kind of question that appears almost trivial until one tries to answer 
it.  To ask whether \emph{presence alone} matters is to ask 
what moral action depends on in the first place—how attention settles, how 
salience is allocated, how accountability is felt before reasons are ever 
articulated.  It requires stepping beneath the level at which people speak of 
moral principles, and into the evaluative machinery that makes principles usable 
at all.

The study carried out here does not, and could not, tell us whether a robot 
elicits “genuine’’ social cognition or whether humans respond to a sophisticated 
simulation.  That deeper metaphysical question remains open.  That question exceeds the scope of the experiment.  But the work does suggest that something prior to it: that the evaluative field through which moral perception becomes behaviour is sensitive to objects that do not think, do not intend, and do not judge.

A small presence can shift a trajectory. That was the animating intuition of the
opening parable: a traveller’s path bending under forces too subtle to name,
long before he could speak of reasons. The experiment provides a cautious
empirical analogue. The deformation is modest, graded, and shaped by
dispositional topology—but it is detectable. And detectability is the
philosophical point. It suggests that moral action is not insulated from the
ambient ecology of perception; it is responsive to it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Across the chapters that followed, this question—posed in its simplest form at 
the outset—unfolded into a structured inquiry into how humans register moral 
salience in the presence of non-human bodies. The theoretical work traced the 
architecture that precedes explicit reasoning: the layers of attention, affective 
orientation, and situational meaning through which a cue becomes morally charged.  
The experimental work then placed participants within a deliberately minimal 
moral environment, introducing $\gamma_R$ as a silent, embodied presence whose 
influence, if it existed, could arise only through these upstream evaluative 
processes.

In this closing chapter, we return to the commitments articulated at the start,
now informed by empirical evidence rather than anticipation. The results permit
a measured claim: the evaluative process appears sensitive to synthetic
presence.
 

\medskip
\noindent
\textbf{(H1) Evaluative Deformation} is cautiously supported.  
Behavioural output under $\Sigma \cup \mathscr{R}$ diverges in a directionally 
consistent manner from that under $\Sigma$ alone, aligning with the Bayesian 
posterior and the aggregate attenuation observed in the data.

\noindent
\textbf{(H2) Synthetic Normativity} is conceptually supported, refined.  
NAO does not introduce new normative content, but modulates the salience of 
existing cues through its perceptual and embodied profile. Its normative 
relevance arises from its ontological status at the operative Level of 
Abstraction, not from agency or intention.

\noindent
\textbf{(H3) Synthetic Perturbation of Moral Inference} is supported in structure.  
The influence of $\gamma_R$ manifests in the perceptual–affective transition 
linking moral salience to action, with the clearest refractive modulation 
emerging in the Prosocial–Empathic cluster. The perturbation concerns the 
\emph{pathway} from cue to behaviour, not deliberation itself.
 

\medskip

The evaluative mapping 
\[
f(\alpha_E, \beta_C, \gamma_R)
\]
suggested that any perturbation should express itself heterogeneously across the
dispositional manifold. This expectation is reflected in the latent ecologies
recovered through clustering: the Prosocial–Empathic group shows the clearest
descriptive attenuation, the Analytical–Structured group shows little
displacement, and the Emotionally Reactive group displays variable
susceptibility. Synthetic presence does not act on a single trait; it interacts
with a configuration.

Epistemically, the thesis treats these findings with the caution appropriate to a
graded effect in a modest dataset. Bayesian estimation retains the uncertainty
inherent in zero-inflated donation data and uneven cluster sizes. The posterior
for the donation difference tilts toward attenuation, but does so in a way that
mirrors the heterogeneity of the cognitive–affective landscape rather than
reducing it to a binary contrast. In a domain where moral cognition is itself
sensitive to ambiguity, it is fitting that the data preserve that structure.

This yields the broader claim toward which the thesis has been moving:

\begin{center}
	\begin{leftbar}
		\textit{Human moral appraisal is structurally sensitive to the perceptual ecology in 
			which it unfolds.}
	\end{leftbar}
\end{center}

Moral behaviour emerges from a cascade of evaluative transitions—what is noticed,
what is foregrounded, what is rendered uncertain. Synthetic presence helps make
this cascade visible. A body that does not judge can nonetheless redirect the
flow of evaluative attention, altering the probability that moral salience
becomes action.

In gathering these strands, the Conclusion returns to where the Introduction 
first gestured: artificial systems participate in human moral life not by 
reasoning or speaking but by altering the background against which moral meaning 
crystallises. The experiment suggests that their influence might be subtle, structured, and dispositional. Following draw out what this might implies for Social Robotics, Affective Computing, and the Philosophy of Moral Cognition, preparing the ground for the final synthesis.


\section{Contributions to Human--Robot Interaction, Affective Computing, and Moral Cognition}

The results developed across the thesis now take on a clearer shape.  
Once the evaluative deformation was shown to be detectable—and shown to depend on 
the dispositional topology of the human observer—the phenomenon revealed itself 
as one that cannot be contained within any single discipline.  
HRI, affective computing, and moral cognition each illuminate one facet, but the 
perturbation induced by $\mathscr{R}$ is a boundary phenomenon: it arises 
precisely where perceptual ecology, affective appraisal, and moral action meet.  
What follows is therefore not a catalogue of separate contributions, but a 
synthesis that reflects the structural unity of the work.

\subsection{Contribution to Human--Robot Interaction}

Research in HRI has largely centred on robots as \emph{interactive partners}:
agents that communicate, collaborate, persuade, align, negotiate, or assist%
~\cite{GoodrichSchultz2007,Breazeal2003,Lee2010,Hancock2011,Fischer2011}.
The present study offers a different vantage point.

\begin{center}
	\begin{leftbar}
		\textit{A robot need not act, speak, or engage to influence human behaviour.
			Co-presence alone may reshape the evaluative field in which action is formed.}
	\end{leftbar}
\end{center}

NAO’s silent, minimally animated presence appears to alter the conditions under
which participants interpreted and responded to a morally salient cue.
This positions the robot not as a participant in a dyadic exchange, but as part
of the \emph{environmental scaffolding} that regulates attention, salience, and
accountability.
Its influence arises from its perceptual affordances and ontological ambiguity—
features that operate upstream of social interaction, at the level where the
evaluative topology is configured.

In this light, robotic presence can be understood as a form of \emph{semiotic
	pressure}: a contextual operator that shapes how meaning settles before any
behaviour is produced.
The contribution to HRI is therefore conceptual and methodological:
it extends the field’s explanatory vocabulary beyond interactional mechanics
toward field-level effects—subtle modulations of the perceptual–affective
conditions under which humans determine what a situation calls for.

This reframing suggests that future HRI research should treat robots not only as
agents with whom we interact, but also as elements of the local moral ecology
whose mere presence may shift the dynamics of human judgement and action.

\subsection{Synthetic Presence and Floridi’s Account of Moral Agency}

The empirical finding that a silent, non-deliberative robot may bend the
trajectory from moral perception to action invites a deeper philosophical
question—one that lingers beneath the statistical surface:

\begin{center}
	\begin{leftbar}
		\textit{If NAO appears to reshape the evaluative conditions under which moral cues acquire behavioural force, what does this imply for its status within Floridi’s informational taxonomy of agency, patiency, and moral relevance?}
	\end{leftbar}
\end{center}
\bigskip
Floridi’s framework distinguishes three categories:  
(i) \emph{moral agents}, capable of producing morally qualifiable actions;  
(ii) \emph{moral patients}, capable of being morally harmed or benefited; and  
(iii) a broader class of \emph{morally relevant informational objects}, whose 
properties can modulate the normative landscape without possessing agency or 
interests of their own~\cite{FloridiSanders2004,Floridi2013}.  

Nothing in the present experiment promotes NAO into the first two categories.  
It neither acts, decides, nor suffers. Yet the data might suggest that it is not 
normatively inert. What NAO alters is not the moral rule, nor the participant’s 
belief about the scenario, but the \emph{evaluative field} in which moral 
appraisal takes shape. In the formal notation developed earlier,
\[
f(\alpha_E, \beta_C, \gamma_R),
\]
its contribution is captured by the perturbation operator $\gamma_R$:  
a modification of the perceptual–affective conditions under which moral 
salience becomes operative.

In Floridi’s terms, NAO is best understood as a \emph{morally relevant artefact}.  
Its morally consequential role does not arise from agency but from 
\emph{informational presence}: the way its embodied form, subtle motion, and 
spatial occupation reconfigure what is noticed, how accountability is felt, and 
which cues enter the evaluative pathway. At the operative Level of Abstraction, 
participants do not meet NAO as a computational system; they meet it as a 
\emph{semiotic body}~\cite{Coeckelbergh2010,Coeckelbergh2020}—a perceptually social 
object that invites, but never completes, the attribution of mindedness.  

This ontological ambiguity is not noise around the effect; it is the mechanism 
of the effect. Because NAO does not judge, command, or reason, its influence 
cannot be located in agency.  
Instead, it resides in the way its presence subtly bends the salience gradients 
through which the Watching–Eye cue acquires behavioural weight.  
The experiment therefore refines Hypothesis~H2: synthetic systems need not 
introduce new norms; they can reshape the uptake of existing ones by altering 
the evaluative topology in which those norms are interpreted.

Seen through Floridi’s lens, the central contribution of the experiment becomes 
clearer. It demonstrates that:

\medskip
\begin{center}
	\begin{leftbar}
		\textit{Artificial systems may exert morally significant influence prior to—and entirely independent of—any capacity for ethical reasoning.}
	\end{leftbar}
\end{center}
\medskip

Their normative import appears to arise from their role as 
\emph{environmental operators} within human cognitive ecologies~\cite{Floridi2013}. 
Rather than challenging Floridi’s taxonomy, the findings bring its diagnostic 
power into sharper focus. They show why a non-agentic artefact can participate in 
moral life without ever becoming a moral subject or object.

Seen from this perspective, the result is not unsettling but clarifying.  
Nothing mysterious has entered the moral domain. What has changed is the 
background against which moral meaning takes shape. The robot does not add a new 
voice to the moral conversation; it alters the acoustics of the room in which the 
conversation is already taking place.  

Once this is recognised, the phenomenon becomes intelligible in a deeper sense. 
Moral cognition does not unfold in a vacuum, insulated from the material and 
informational textures that surround it. It unfolds in fields—shaped by bodies, 
presences, absences, and ambiguities. Synthetic artefacts matter, not because 
they reason, but because they quietly reweight the forces that carry perception 
toward action. And it is precisely here, in these subtle modulations of the 
evaluative field, that their moral significance resides.


\subsection{Contribution to Affective Computing and Social Signal Processing}

From the perspective of Affective Computing and Social Signal Processing, the
results of this thesis may speak to a long-standing ambition: to make latent
cognitive--affective processes empirically tractable without reducing them to
surface behaviour alone. What the experiment suggests is not merely that moral
behaviour can be measured, but that it can serve as a \emph{diagnostic trace} of
deeper evaluative dynamics when the surrounding field is carefully perturbed.

The attenuation observed under synthetic co-presence suggests that moral
evaluation, although never directly observable, leaves patterned signatures in
behaviour when the evaluative topology is displaced. This aligns with a
foundational premise of affective computing: that latent states become legible
through systematic relations between context and response, rather than through
introspection or explicit report alone~\cite{Picard1997,CalvoD2010,Vinciarelli2009,Vinciarelli2014}.
Here, moral action itself serves as the signal.

A methodological insight follows naturally:

\begin{center}
	\begin{leftbar}
		\textit{Latent evaluative structure may be inferred not by isolating behaviour
			from context, but by observing how behaviour changes when context is subtly
			altered.}
	\end{leftbar}
\end{center}

The dispositional ecologies recovered through clustering—Prosocial--Empathic,
Analytical--Structured, and Emotionally Reactive—illustrate this principle in
concrete form. They suggest that susceptibility to perturbation is not uniformly
distributed, but organised around relatively stable regions of evaluative space.
In this sense, affective modulation by artificial systems appears to be a
\emph{structure-sensitive} phenomenon: the same environmental cue does not carry
the same weight across cognitive--affective configurations. This connects
Affective Computing’s concern with latent state inference to Social Signal
Processing’s emphasis on how stable individual differences shape responsiveness
to social cues~\cite{Pentland2007,Habashi2016,Banks2020}.

Finally, the Bayesian modelling framework contributes an epistemic stance as much
as a statistical one. By representing uncertainty as a graded posterior rather
than collapsing it into binary outcomes, the analysis treats ambiguity as an
informative feature of moral cognition rather than as noise to be eliminated.
This perspective aligns naturally with hierarchical Bayesian approaches in
computational cognitive science~\cite{Tenenbaum2011,Kruschke2014}, and with the
broader aim of affective computing: to model evaluative processes as
probabilistic, context dependent, and dynamically constrained.


\subsection{Contribution to Moral Cognition Research}

Seen through the lens of Moral Psychology, the findings converge on a view of
moral judgement that is intuitive, situated, and structurally sensitive.
The attenuation observed under synthetic co-presence does not appear at the level
of articulated belief or explicit principle application. It appears to emerge
earlier, within the cognitive--affective space where salience is first registered
and evaluative weight is assigned. What shifts is not what participants think is
right, but how strongly a morally relevant cue presses toward action.

This places the results squarely within, and in dialogue with, intuitionist and
dual-process accounts of moral cognition~\cite{Haidt2001,Greene2001,Greene2002,
	Greene2014,Cushman2013}. Moral behaviour here is not treated as the endpoint of
reflective calculation, but as the outcome of a trajectory that begins with
perception, attention, and affective appraisal. The present study extends
existing evidence that moral judgement is sensitive to subtle environmental
modulation—implicit monitoring, gaze cues, ambient social presence~\cite{Haley2005,
	Bateson2006,Conty2016,Dear2019}—by suggesting that \textbf{this sensitivity is not limited
to human observers, but may generalise to \emph{synthetic} embodied presences}
\cite{Zlotowski2015,Groom2010,Malle2016}.

A further contribution lies in indicating that this sensitivity is not uniformly
distributed. The clearest descriptive attenuation emerges within the
Prosocial--Empathic ecology, characterised by heightened interpersonal
attunement and affective resonance, while other dispositional regimes show more
muted or absent effects. This pattern is consistent with a prediction implicit in
the formal mapping $f(\alpha_E,\beta_C,\gamma_R)$: that the transformation from
moral salience to action is shaped by the internal organisation of the
dispositional manifold. Empathy-related traits appear to amplify responsiveness
to social context, and this responsiveness may extend beyond human sociality to
synthetic bodies occupying the same perceptual field~\cite{Decety2004,
	Habashi2016,Hilbig2013}.

Taken together, these findings portray moral cognition as both intuitive and
ecologically embedded. Moral action arises from the interaction between latent
evaluative structure and the perceptual environment in which judgement unfolds.
Synthetic presence matters not because it introduces new moral content, but
because—by perturbing the field—it helps render visible the contours along which
moral meaning becomes behaviour.

\subsection{Integrative Contribution: A Unified Field-Theoretic Perspective}

What emerges, when these strands are brought together, is not a collection of
parallel contributions but a single explanatory stance. Across Psychology,
Human--Robot Interaction, Affective Computing, and perhaps Information Ethics, a common structural pattern becomes visible: moral behaviour is best understood not as
the execution of rules or the expression of stable preferences, but as movement
within an evaluative field shaped by context, disposition, and presence.

The unifying move of this thesis is to make that structure explicit. By treating
moral action as the output of an evaluative mapping
$f(\alpha_E,\beta_C,\gamma_R)$, the work offers a vocabulary capable of describing
phenomena that might otherwise remain fragmented across disciplines.
Environmental cues ($\alpha_E$), dispositional architecture ($\beta_C$), and
synthetic presence ($\gamma_R$) are not competing explanations but jointly
\textbf{constitutive dimensions of a single landscape}. Moral appraisal, on this view, is
not a point event but a trajectory: a passage through a structured space of
salience, affect, and practical pull.

This perspective resonates with long-standing findings in Moral Psychology, where
affective appraisal and intuitive processing play a generative role
\cite{Haidt2001,Greene2001,Decety2004}; with social signal processing, which shows
that minimal cues can reorganise attentional and evaluative structure
\cite{Pentland2007,Vinciarelli2009,Conty2016}; and with HRI research suggesting
that robotic presence can modulate human behaviour even in the absence of
explicit interaction \cite{Kuchenbrandt2011,Malle2016,Bremner2022,Zlotowski2015}.
What binds these literatures is not a shared methodology, but a shared level of
explanation. Floridi’s Levels of Abstraction provide the hinge: the influence of
synthetic presence appears to operate at the perceptual and affective LoA,
independently of any internal capacity for reasoning or intention.

Seen from this angle, the central insight can be stated simply:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Artificial systems need not act, decide, or reason to shape moral
			behaviour; it may be sufficient that they reorganise the conditions under
			which moral salience is registered.}
	\end{leftbar}
\end{center}
\bigskip
\noindent

This reframing does more than connect empirical results to philosophical theory.
It might help explaining--or it might provide minimal tools for investigating--why moral influence can arise without agency, why ambiguity rather than intention can be causally relevant, and why small changes in perceptual ecology can produce measurable shifts in behaviour. Artificial agents, on this account, are not competitors to human moral reasoning. They function as operators on the topology through which moral meaning becomes action.

The field-theoretic perspective articulated here thus closes a conceptual loop.
It links experimental measurement, computational modelling, and philosophical
analysis within a single explanatory structure—one in which moral life is shaped
less by what entities are, than by how they quietly configure the spaces in which
human judgement unfolds.

\section{One Explanatory Object, Not Three Literatures}

At this point, it becomes possible to see the shape of the work as a whole.  
What may have appeared, at earlier stages, as a dialogue between distinct
traditions—Moral Psychology, Information Ethics, and Social Robotics—resolves
into something more unified. This thesis was never an exercise in juxtaposition. It was organised, from the outset, around a single explanatory object that none
of those literatures could fully grasp in isolation.

That object is the evaluative transformation
\[
f(\alpha_E, \beta_C, \gamma_R),
\]
the process through which moral salience is translated into action under
conditions of situated perception, dispositional structure, and synthetic
co-presence. Each body of work engaged in this thesis illuminates a different
dimension of that transformation, but all are oriented toward the same
phenomenon.

Moral Psychology provides the generative core: it explains how intuitive,
affective, and trait-sensitive processes give rise to evaluative trajectories,
and why such trajectories should be differentially susceptible to contextual
modulation. Floridi’s Information Ethics supplies the ontological and epistemic
frame: it clarifies how artificial entities can become morally relevant at a
given Level of Abstraction without possessing agency, intention, or moral
standing. Social Robotics and Social Signal Processing, finally, furnish the
empirical arena in which these abstract claims encounter the world—where
synthetic presence can be instantiated, perturbed, and measured with precision.

From this vantage point, the field-theoretic perspective is not a metaphor
imported for rhetorical effect. It is the natural generalisation of the mapping
itself. Moral cognition unfolds in a structured space of salience, attention, and
affective pull; synthetic presence operates by subtly reshaping the geometry of
that space. No single literature could have revealed this alone. Moral psychology
without Levels of Abstraction lacks an account of why non-agentic artefacts
matter. Information ethics without empirical grounding lacks a mechanism by which
moral relevance becomes behaviourally manifest. HRI without an evaluative model
lacks an explanation for why minimal, silent presence should exert any influence
at all.

What this thesis offers, then, is not a bridge hastily thrown between adjacent
fields, but a reorganisation of them around a common evaluative architecture.
The experimental results suggest that artificial systems participate in human
moral life neither by reasoning nor by instructing, but by modulating the
perceptual and ontological conditions under which moral meaning takes shape. That
insight—quiet, structural, and easily missed—is the conceptual kernel around
which the research programme coheres.

\section{Final Synthesis and Closing Reflections}
\label{sec:conclusion}
\noindent
Step back once more from the apparatus of the thesis—the models, the estimates,
the carefully delimited effects—and what remains is a single, quietly demanding
insight. Moral action might not arise in isolation. It might be shaped, bent, and
sometimes gently redirected by the perceptual and affective conditions in which
it unfolds. What this work suggests is that synthetic presence belongs among
those conditions. A non-agentic, silent artefact can enter the moral scene and
appear to alter the path from noticing to acting, without ever offering a
reason, a command, or a judgement.

This is not because the robot introduces new moral content. It does not.
Nor because it persuades, instructs, or interacts. Rather, its presence appears
to reshape the evaluative field itself—the background against which moral
salience is registered and allowed to matter. The empirical results trace this
effect with care. The deformation is modest, graded, and contingent on
dispositional topology. It is most visible where moral sensitivity is already
high, and muted where evaluative processing is more insulated. Yet across
analyses, methods, and inferential lenses, a consistent structure emerges:
synthetic presence might be leaving a detectable imprint on moral behaviour by altering the conditions under which moral meaning becomes action.

What gives this result its weight is not its size, but its location. The effect
emerges upstream of deliberation, prior to explicit reasoning, in the
perceptual–affective space where attention settles and salience acquires force.
Contemporary Moral Psychology has long argued that this is where moral cognition
does its primary work. Floridi’s Levels of Abstraction clarify why artefacts can
matter here without becoming agents. Human–Robot Interaction and Affective
Computing provide the means to observe and model such shifts. Taken together,
these perspectives might converge on a simple but consequential picture: moral life could be ecological. It unfolds within a field, and that field appears sensitive to bodies that do not think but are nonetheless perceived.

The formalism developed throughout the thesis,
\[
f(\alpha_E, \beta_C, \gamma_R),
\]
was never meant as ornament. It is a way of holding this picture steady. Moral
behaviour is treated not as the execution of principles, but as a trajectory
through a structured evaluative landscape—one shaped by environmental cues,
dispositional geometry, and perturbations introduced by artificial systems. The
robot functions here as an operator on that landscape. It leans on the field,
subtly altering its gradients, and in doing so shifts the likelihood that moral
salience will carry through to action.

Seen from this angle, the ethical implications sharpen. Classical Machine Ethics
has asked how to make machines reason morally. The findings here suggest that a
prior question deserves sustained attention: how machines may already shape
moral reasoning by their presence alone. NAO is not a moral agent, and nothing in
this work argues that it should be treated as one. Its relevance lies elsewhere.
It inhabits the moral environment as an informational object whose perceptual and
semiotic properties can reorganise human evaluative space. Ethical significance,
in this sense, precedes agency. It arises at the level of conditions, not
conclusions.

The thesis therefore closes where it began: with a small question that turns out
to reach deep. Can mere presence matter? The answer, cautiously but clearly, is
that it might. It matters not by overriding judgement, but by shaping the terrain in which judgement moves. It matters not by dictating action, but by altering the ease
with which action follows from what is noticed and felt. This insight does not
resolve the larger metaphysical questions about simulation, authenticity, or
social cognition. Those remain open. What it does offer is something more basic:
a way of seeing where moral influence begins.

If there is a future research programme suggested by this work, it is not one
that starts with artificial moral agents. It starts with \textbf{moral ecologies}. With the fields of salience, attention, and affect that humans and machines now share.
The task ahead is to learn how to map these fields, how to detect their deformations, and how to take responsibility for the environments we are already building together.

The thesis ends here. But the evaluative landscape it describes does not. It is
already around us—quiet, structured, and responsive to presences that do not
speak, yet still manage to matter.