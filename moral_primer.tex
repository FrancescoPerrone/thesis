\chapter{Cognitive–Affective Architecture of Moral Judgment}
\label{chap:moral_primer}
\thispagestyle{pprintTitle}


% Adjusting epigraph settings
\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushright}

The conceptual apparatus developed in this chapter is not an ornamental introduction to moral theory. It is the minimum set of distinctions required to make the research question itself intelligible. The project asks:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Can the mere presence of a synthetic, non-agentic entity perturb the inferential 
			transformation through which morally salient cues are converted into observable moral 
			behaviour?}
	\end{leftbar}
\end{center}

\bigskip
\noindent


that is to say whether the mere presence of a synthetic agent can alter the trajectory by which human beings transform a morally salient perception into a morally relevant action. Such a question does not belong to the domain of ethical theory; it belongs to the domain of moral cognition.

To address it, one must understand the cognitive–affective substrate in which moral judgments are formed, weighted, and enacted. The behaviour observed in the experiment does not arise at the level of explicit reasoning, rule application, or reflective justification. It arises upstream, within the processes that determine what becomes salient, how empathic resonance is allocated, which cues are attended to, and how the felt sense of accountability is modulated. These are the mechanisms through which moral evaluation becomes behaviourally operative; without a precise understanding of them, the central phenomenon of this thesis is not only unexplained but incorrectly described.

Reflective moral theories---Kantian maxims, utilitarian calculus, contractualist reasoning---do not operate at this level. They articulate justificatory relations, not generative mechanisms. They tell us why an action may be defensible, not how the human cognitive system produces the behaviour in the
first place \cite{Korsgaard1996,Scanlon1998,SidgwickMethods}. For this reason, any attempt to explain the experimental effect by appealing to ethical principles is methodologically misaligned. 

It begins at a Level of Abstraction that the phenomenon does not inhabit \cite{Floridi2008,Floridi2011}.

What is required instead is an account of moral cognition as an action-guiding evaluative process: a process in which affect, attention, salience, social interpretation, and contextual meaning jointly determine how moral cues acquire behavioural force. A large body of work in moral psychology and cognitive neuroscience demonstrates that these mechanisms---affective appraisal, empathic
resonance, intuitive evaluation, and attentional modulation---constitute the causal substrate of moral judgment \cite{Haidt2001,Greene2001,Greene2004,Cushman2013,Decety2004,Crockett2016}. Only within such a framework can the influence of synthetic presence be meaningfully specified. Without it, the
experimental result risks being mischaracterised as a change in moral belief or a failure of deliberation, when in fact it is a perturbation of the evaluative field that precedes both~\cite{Conty2016,Nettle2013,Pfattheicher2015}.

The purpose of this chapter is therefore clarificatory in the strictest sense. It isolates the cognitive--affective mechanisms that constitute the causal substrate of moral behaviour; it distinguishes them from the reflective structures of ethical theory; and it establishes the Level of Abstraction at
which the research question resides \cite{Floridi2008,Floridi2011}.

By doing so, it provides the conceptual conditions under which the empirical findings of the thesis can be correctly interpreted. The experiment does not test principles, preferences, or doctrines. It tests the stability of the evaluative machinery through which moral meaning becomes action. Understanding that machinery is the only way to understand the phenomenon under investigation.

This is why the chapter must take the form it does.
Not to broaden the discussion of morality, but to focus it precisely at the level where the phenomenon of synthetic moral perturbation arises.


A recurring theme across the reviewed literature is that failures in Machine Ethics stem from two related errors: \emph{category mistakes} and \emph{LoA conflation}. 

Category mistakes arise when reflective normative principles are
treated as if they described the psychological mechanisms that generate moral behaviour; LoA conflation occurs when descriptive cognitive regularities are mistaken for normative constraints or vice versa \cite{Floridi2008,Floridi2011}. Both errors follow from neglecting the fact that justificatory structures live at
a high Level of Abstraction, whereas moral behaviour is produced at a lower, cognitive--affective LoA documented in moral psychology and social cognition~\cite{Haidt2001,Greene2004,Decety2004}. 

Recognising these distinctions is methodologically essential: without LoA discipline, interpretive models of
moral perturbation become confused, and empirical findings—such as the attenuation effects examined in this thesis (Chapter~\ref{chap:exp_methods})—risk being mischaracterised as
failures of reasoning rather than as deformations of the evaluative field.

\section{Descriptive and Normative Domains}

The term ``morality'' spans at least two analytically distinct domains. The first is \emph{descriptive morality}: 


\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{the empirical study of how humans form moral judgments, experience moral emotions, and engage in normatively salient actions.}
	\end{leftbar}
\end{center}

\bigskip
\noindent

 This includes developmental psychology \cite{Kohlberg1981}, social–cognitive models \cite{Haidt2007, DorisSep2020}, affective neuroscience \cite{Moll2002, Decety2004}, and evolutionary accounts of cooperation and prosociality \cite{Joyce2006, Tomasello2016}. 
 
 The second is \emph{normative morality}: 
 
 \bigskip
 \noindent
 \begin{center}
 	\begin{leftbar}
 		\textit{the domain of ethical theorising concerned with how one ought to act.}
 	\end{leftbar}
 \end{center}
 
 \bigskip
 \noindent
 
This domain encompasses deontological, consequentialist, contractualist, and virtue-theoretic traditions \cite{Hursthouse1999, Hooker2000, Anscombe1957, Korsgaard2009}.

These domains are distinct but interdependent. Descriptive accounts illuminate how agents actually evaluate and respond to situations, while normative theories articulate standards for justified action. Empirical models of moral cognition acquire meaning partly through the normative vocabulary within which moral judgments are articulated, while normative theories must remain constrained by what agents are psychologically capable of performing or understanding.

The distinction between descriptive and normative morality is introduced at this point in the chapter because it provides the final conceptual boundary required before the empirical and theoretical analysis can proceed. Without it, two serious confusions would arise—each of which would undermine the scientific aims of the project.

First, moral terminology in technical disciplines is often used ambiguously. Words like obligation, responsibility, harm, or trust are employed as if their meaning were self-evident, yet researchers oscillate unconsciously between describing how agents in fact behave and prescribing how they ought to behave. This sliding between domains produces conceptual instability: experimental findings are mistaken for ethical insights, and normative claims are misinterpreted as empirical predictions.

Second, the research question of this thesis is strictly descriptive: \textit{Can synthetic presence alter the evaluative processes through which humans convert moral perception into moral action?}

To answer this question, the project must operate within the empirical domain of moral psychology. If this boundary is not explicitly marked, the analysis risks drifting into normative interpretation—treating behavioural attenuation as moral deficiency, or treating reflective theories as mechanistic explanations.

The descriptive–normative distinction therefore performs a crucial clarificatory function:

\noindent
The distinction identifies the \textbf{level at which the thesis operates}. The aim is not to determine what people \emph{should} do in the presence of robots, but to explain what \emph{does} happen within the cognitive--affective architecture when artificial agents enter the evaluative field. Such phenomena require descriptive tools: models of attention, salience, empathy, and social meaning—not principles or moral doctrines \cite{Haidt2001,Greene2004,Decety2004,Pfattheicher2015,Conty2016}. 

\noindent
Second, the distinction prevents the \textbf{misinterpretation of empirical findings} as moral judgments. If a robot’s presence reduces prosocial behaviour, this is a psychological effect, not a moral failure. It does not imply that agents have acted wrongly or that the robot has transgressed any ethical boundary. It reflects a perturbation in the evaluative machinery that gives moral cues their behavioural force \cite{Haley2005,Bateson2006,Dear2019,Kuchenbrandt2011,Malle2016}. 

\noindent
Third, the distinction isolates the \textbf{causally relevant components of morality} for the experiment. The mechanisms at stake—affective resonance, accountability salience, attentional modulation—belong entirely to descriptive cognition \cite{Decety2004,Crockett2016,Vinciarelli2009,Conty2016}. Normative theories are indispensable for understanding the structure of moral reasoning, but they do not generate behaviour \cite{Scanlon1998,Korsgaard1996,SidgwickMethods}. Keeping the domains separate ensures that the phenomenon is examined at the correct Level of Abstraction \cite{Floridi2008,Floridi2011}. 

\noindent
Finally, the distinction prepares the ground for \textbf{integrating normative theory later without conceptual confusion}. Normative materials will reappear, not as behavioural engines, but as structural constraints within the evaluative topology—deontic invariants, consequentialist gradients, virtue-theoretic attractors, sentimentalist vectors, and contractualist equilibria~\cite{Greene2014,Foot2001,Hursthouse1999,Annas2011}. This reinterpretation is only possible once descriptive and normative domains have been clearly disentangled.

\noindent
In sum, the distinction is introduced here because it secures the conceptual boundary conditions of the thesis. It establishes the domain in which the claims are made, prevents methodological conflation, and ensures that the phenomenon under investigation—moral perturbation under synthetic presence—is analysed at the level where it actually occurs. The orientation of the thesis is therefore precise: moral cognition is the object of study; normative theory provides the vocabulary of justification; and coherence requires that these domains remain distinct.


The project now turns to a minimal operational definition of morality. This may appear abrupt, but its placement at this point in the chapter is deliberate. The preceding sections established the conceptual boundaries required to analyse moral cognition without collapsing distinct Levels of Abstraction or importing normative assumptions into descriptive models. Having drawn these boundaries, the thesis now requires a definition precise enough to guide empirical and theoretical analysis, yet modest enough to avoid the philosophical commitments associated with substantive normative theories.

\subsection{Why Definitions Vary}

There is no single universally accepted definition of morality, and this plurality is neither accidental nor superficial. Different research programmes emphasise different elements of the moral domain. Cognitive approaches foreground the mechanisms by which agents form evaluative judgments \cite{Doris2010}; affective traditions emphasise the emotional systems that underpin moral concern \cite{Nussbaum2001}; rationalist accounts privilege normative reasoning \cite{Korsgaard2009}; social-scientific models attend to conventions and cultural norms \cite{Bicchieri2016}; evolutionary frameworks focus on the adaptive functions of cooperation and prosociality \cite{Joyce2006,Tomasello2016}. Philosophical traditions likewise diverge in grounding morality in rationality, sentiment, virtue, utility, social contracts, or evolutionary pressures.

Computational treatments often inherit only one strand of this diversity. They default to rule-based perspectives not because such models accurately describe human moral cognition, but because they are structurally convenient to implement \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. This convenience has encouraged the misleading interpretation of moral behaviour as rule following and has fostered oversimplified models of moral decision-making that obscure the cognitive–affective architecture through which real moral judgments are produced \cite{Haidt2001,Greene2001,Cushman2013,Decety2004}.

A primary aim of this chapter is therefore corrective: to replace these inherited simplifications with a framework grounded in contemporary moral psychology, cognitive science, and social-signal research \cite{Pentland2007,Vinciarelli2009,Conty2016}. Only such a framework can support the empirical and conceptual analysis required by the research question.


\subsection{Minimal Operational Definition for This Thesis}

For the purposes of this thesis, we adopt the following minimal, action-oriented definition:

\begin{quote}
	\emph{Moral cognition is the evaluative process through which agents detect normatively salient features of a situation, generate judgments regarding permissible or obligatory actions, and select behaviour accordingly.}
\end{quote}

This definition is intentionally modest. It avoids substantive normative commitments while capturing the components required for empirical investigation: \emph{evaluation}, \emph{judgment}, and \emph{action}. It aligns with contemporary accounts of moral psychology that treat morality as grounded in both affective and cognitive mechanisms \cite{Haidt2007, Decety2004, Moll2002}. It also coheres with the theoretical machinery of this thesis, including evaluative topology, levels of abstraction, and the notion of semiotic perturbation. \textit{Moral cognition thus functions as a mapping from situational cues to action policies, modulated by trait-level and affective structures} \cite{Doris2002, Doris2010}.

\section{Judgments: Factual and Normative}

A central distinction for understanding moral cognition is the difference between \emph{factual} and \emph{normative} judgments. Although both concern evaluations of situations, they operate at different logical and conceptual levels. Factual judgments describe states of affairs: they answer questions about what \emph{is} the case. Normative judgments concern what \emph{ought} to be done, what is \emph{permissible}, \emph{required}, or \emph{forbidden}. The distinction is classical in philosophy, yet it remains frequently blurred in computational and psychological treatments of morality \cite{Black1972, Deigh2010}.

Factual judgments derive their correctness conditions from empirical features of the world. Their truth depends on evidence, observation, or inference. Normative judgments, by contrast, embed claims about reasons for action and the standards that govern deliberation. They express commitments that are action-guiding and prescriptive in force, even when articulated implicitly \cite{Hare1981, Korsgaard2009}. What follows from this distinction is more than a semantic bifurcation: it marks a functional divide in the cognitive architecture that underwrites evaluative thought. A judgment about what \emph{is} the case engages classificatory and predictive mechanisms; a judgment about what \emph{ought} to be the case recruits additional systems responsible for assigning motivational weight, integrating affective cues, and generating the directional force that links evaluation to action.\marginpar{\footnotesize\emph{Key distinction:}\\Factual = descriptive;\\Normative = action-guiding.}

Moral cognition refers to the ensemble of perceptual, affective, and inferential processes through which agents register morally relevant features of a situation and transform them into evaluative representations \cite{Greene2001, Haidt2001, Mendez2009}. It encompasses both explicit moral judgment and upstream mechanisms that detect salience, encode social meaning, and initiate the transition from evaluative appraisal to behaviour \cite{Cushman2013, Young2012}. Introducing this construct at this stage is essential, because it clarifies that the descriptive–normative distinction is mirrored in the cognitive architecture that processes them: factual information is registered by systems specialised for prediction and classification, whereas normative evaluation recruits additional mechanisms that assign motivational force and action-guiding significance.\footnote{In moral psychology, this distinction is often operationalised by contrasting cognitive processes supporting representational accuracy with those supporting valuation and action selection. See \cite{Moll2002, Greene2004, Shenhav2017}.}\marginpar{\footnotesize\emph{Moral cognition:}\\Perception $\rightarrow$ appraisal $\rightarrow$ action-guidance.}

In moral cognition, the distinction is not merely verbal but functional. Psychological models indicate that factual information serves as input to evaluative appraisal~\cite{Cushman2013ActionOutcomeValue, CushmanYoung2009Beyond, CushmanGreene2012Common, Mikhail2007UMG}, but normative judgment involves the additional step of mapping descriptive cues onto action-guiding evaluations~\cite{Hindriks2015Normative, Greene2014Beyond, Railton2017MoralLearning, Cushman2013ActionOutcomeValue}. Treating normative judgments as a special case of factual ones therefore collapses essential differences in their psychological and functional architecture. For empirical research in moral psychology—and particularly for any paradigm seeking to measure moral behaviour—the distinction ensures that observable responses are not misinterpreted as direct indicators of moral endorsement or norm acceptance.\marginpar{\footnotesize\emph{Methodological note:}\\Behaviour $\neq$ endorsement unless interpretive architecture is specified.}

This distinction between descriptive input and normative evaluation sets the stage for a further refinement. Once we recognise that moral cognition incorporates specialised mechanisms for assigning salience, generating evaluative force, and transforming appraisal into behaviour, it becomes clear that moral judgments themselves cannot be exhaustively characterised as simple outputs of belief or emotion. They arise from the coordinated operation of multiple cognitive systems—perceptual, affective, inferential, and motivational—whose interaction determines not merely \emph{what} is judged, but \emph{how} and \emph{why} it guides action. In other words, the transition from factual uptake to normative appraisal presupposes an internal architecture of judgment: a structured evaluative act with identifiable components that jointly confer its distinctive normative authority. It is to this internal architecture that we now turn.


\section{The Structure of Moral Judgments}

Moral judgments are not mere expressions of preference or affective reaction. They exhibit a characteristic structure combining evaluative content, justificatory grounding, and action-guiding force~\cite{Smith1994MoralProblem, Railton1986MoralRealism, Blackburn1998RulingPassions, Gibbard1990WiseChoices, Korsgaard1996Sources}. A moral judgment typically involves at least three components:

\begin{enumerate}
	\item \textbf{Salience detection}: recognition that a situation involves normatively relevant features (harm, fairness, honesty, obligation, care). This process draws upon perceptual, affective, and social-cognitive systems \cite{Moll2002, Decety2004}.
	\item \textbf{Evaluative appraisal}: an assessment of those features in light of internalised norms, dispositions, or reasons. This appraisal may be intuitive or reflective, emotionally charged or deliberative, depending on the individual and context \cite{Nussbaum2001, Doris2010}.
	\item \textbf{Practical commitment}: a transition from evaluation to action guidance, where the judgment functions as a reason for or against performing a particular behaviour \cite{Anscombe1957, Korsgaard2009}.
\end{enumerate}

These components jointly distinguish moral judgments from other evaluative acts such as aesthetic preferences or strategic choices. They also underwrite the thesis’s operational understanding of moral cognition as an \emph{evaluative mapping} from cues to action. 

Importantly, this structure accommodates both intuitive and deliberative models. 
Intuitive processes may dominate in everyday moral encounters; nonetheless, these 
judgments retain justificatory structure, even when reasons are not explicitly articulated~
\cite{Haidt2001, Greene2014Beyond, Railton2017MoralLearning, Mikhail2007UMG}. 
Conversely, deliberative processes involve explicit reasoning, \textit{counterfactual 
	consideration}, and appeal to principles or character traits \cite{Hursthouse1999}. 
This duality will be further elaborated in the discussion of psychological and 
neuroscientific foundations that follows.

\medskip
\noindent
This distinction between intuitive and deliberative processes is not merely taxonomical; it marks the beginning of a deeper inquiry into the cognitive architecture that makes moral judgment possible. To understand why certain stimuli reliably elicit prosocial behaviour while others disrupt or attenuate it, we must examine the underlying mechanisms through which moral salience is perceived, represented, and acted upon. The transition from intuition to deliberation is mediated by identifiable affective, perceptual, and 
executive systems, each contributing distinct computational roles within the broader moral economy. As the following section illustrates, contemporary psychological and neuroscientific research converges on a model of moral cognition as a distributed and dynamically interactive network. This framework not only clarifies how humans ordinarily navigate morally charged environments, but also establishes the theoretical scaffolding required to interpret how such processes may be perturbed—subtly yet measurably—by the presence of agents whose social and ontological status is ambiguous, such as humanoid robots. 
In this sense, the empirical foundations surveyed below serve as the conceptual substrate upon which our experimental analysis later builds.


\subsection{Psychological and Neuroscientific Foundations of Moral Decision-Making}

A substantial body of work in cognitive neuroscience demonstrates that moral 
decision-making is not the product of a single ``moral centre'' but emerges from 
coordinated activity across distributed affective, social-cognitive, and executive networks. These systems jointly determine how agents detect morally salient cues, generate evaluative appraisals, and select action policies. The architecture is, in this sense, inherently \emph{practical}: the neural substrates implicated in moral judgment are deeply intertwined with those responsible for value computation, behavioural control, and action selection.\footnote{This stands in contrast to folk-psychological depictions of moral judgment as a purely contemplative process concerned with identifying moral facts. Neuroscientific evidence overwhelmingly supports action-guidance as the primary functional orientation of moral cognition.} 
Rather than isolating ``moral reasoning'' as a \textit{sui generis} faculty, contemporary research positions it within a larger computational system whose governing question is not ``What is right?'' but ``What should I do given this situation?''~\cite{Bechara2000, Moll2002, Greene2014Beyond}.

\paragraph{Affective and Value-Based Systems.}
Among the most extensively studied structures contributing to moral evaluation are the ventromedial prefrontal cortex (vmPFC) and orbitofrontal cortex (OFC). These regions compute affective and motivational value, integrating emotional information with anticipated outcomes. Lesion studies demonstrate that damage to the vmPFC disrupts the ability to factor emotional and social consequences into decision-making, often resulting in choices that appear normatively inappropriate or insensitive to harm~\cite{Bechara2000}. Functional imaging studies show robust vmPFC activation during tasks involving interpersonal harm, care, and empathic concern \cite{Moll2002}. These observations suggest that moral judgments rely on mechanisms that encode the valenced quality of behavioural options and link them to affectively grounded somatic markers.

The amygdala and anterior insula further contribute to the rapid detection of morally salient information~\cite{Garrigan2016, Eres2018, Fede2020}
. The amygdala is sensitive to threat, intentional aggression, and aversive outcomes, providing early affective tagging~\cite{LeDoux1998, Phelps2006}
 that biases attention and behavioural readiness. The anterior insula responds to disgust, norm violations, and aversive interoceptive states~\cite{Moll2005, Sanfey2003, Chang2013}. Together, these regions enable rapid, pre-reflective processing of emotionally charged cues, thereby initiating downstream evaluative computation. Electrophysiological evidence indicates that these affective signals can precede conscious deliberation~\cite{Sarlo2012, Luo2006}, suggesting that emotional valence functions as an early gatekeeper in moral cognition.

\paragraph{Social-Cognitive and Interpretive Systems.}
Moral judgments frequently hinge on the mental states of agents: their beliefs, 
intentions, and reasons for action~\cite{Mikhail2007, YoungSaxe2011, CushmanYoung2009}. The temporo-parietal junction (TPJ), medial 
prefrontal cortex (mPFC)~\cite{Saxe2003, SaxeKanwisher2003}, and posterior superior temporal sulcus (pSTS) constitute a network specialised for theory-of-mind and mental-state attribution~\cite{Pelphrey2004, VanOverwalle2009}. TPJ activation is reliably observed in tasks requiring participants to distinguish between intentional and accidental harms, to attribute blame or forgiveness, or to infer whether an agent acted under ignorance or malice. This sensitivity to mental-state information demonstrates that moral cognition tracks reasons and intentions, not merely outcomes~\cite{YoungSaxe2010, Cushman2013}.

The anterior cingulate cortex (ACC) plays an integrative role in moral cognition by monitoring conflict between competing evaluative signals \cite{Botvinick2004, Shackman2011}. In classic moral dilemmas—such as those involving trade-offs between harm minimisation and fairness constraints—the ACC shows increased activation during conflict detection and the recruitment of cognitive control \cite{Greene2004, Decety2012}. This suggests that the ACC contributes to arbitrating between intuitive emotional responses and more deliberative evaluations, particularly in situations where values compete or intentions are ambivalent \cite{Shenhav2013, Etkin2011}.

\paragraph{Executive and Action-Guidance Systems.}
The dorsolateral prefrontal cortex (dlPFC) supports controlled cognitive operations, including the inhibition of prepotent affective responses, the representation of rules, and the evaluation of abstract or long-term consequences \cite{Miller2001, Koechlin2003}. Disruption of dlPFC activity via transcranial magnetic stimulation has been shown to alter participants' willingness to endorse harmful actions in instrumental contexts, indicating that this region contributes to regulating intuitive aversions when normative or goal-directed reasoning requires overriding them \cite{Tassy2012, Greene2004TMS}. Rather than functioning as a classical “rational override,” the dlPFC appears to contribute to integrating affective, deontic, and goal-directed considerations into coherent action policies \cite{Shenhav2017, Cushman2013ValueIntegration}.

\noindent
Importantly, the dlPFC does not operate in isolation. Its interactions with vmPFC, ACC, and parietal regions indicate that executive control is embedded within a broader network that also encodes affective and interpretive information \cite{Hare2009, Mansouri2009, Bressler2010}. These distributed processes jointly shape the computation of moral decisions as behavioural commitments rather than as purely abstract evaluations \cite{Shenhav2013ControlValue, Gintis2014MoralComputation}.

\paragraph{Functional Integration and Practical Orientation.}
Across these subsystems, a coherent picture emerges: moral cognition is not a contest between “emotion” and “reason” but a dynamic interplay among affective valuation, social interpretation, and executive control \cite{Haidt2001, Greene2014Integration, Cushman2013DualSystems}. This architecture is fundamentally action-oriented. vmPFC and OFC compute the affective value of potential actions \cite{Ongur2000, Rangel2008}; TPJ and mPFC provide intention-sensitive interpretations of agents’ behaviours \cite{SaxeKanwisher2003, YoungSaxe2010}; the ACC detects conflicts between competing behavioural tendencies \cite{Botvinick2004, Shackman2011}; and the dlPFC regulates whether intuitive biases should be suppressed, enacted, or weighed against normative constraints \cite{Miller2001, Tassy2012}. Even primary affective structures such as the amygdala and insula contribute to shaping behavioural readiness by generating rapid somatic markers and prioritising morally relevant features of the environment \cite{Phelps2006, Chang2013}.

Lesion studies, electrophysiological findings, and neuroimaging results converge on the conclusion that moral judgment is primarily a mechanism for generating and constraining action under conditions of social meaning. From this perspective, moral cognition is best understood as a form of evaluative control: a mapping from cue detection to practical commitment~\cite{Greene2014Beyond, Cushman2013Action}. This view aligns with philosophical accounts emphasising the action-guiding nature of moral evaluation~\cite{Anscombe1957, Korsgaard2009}, while grounding such accounts in empirical evidence about the neural architecture of agency, valuation, and control.

\paragraph{From Moral Architecture to Perturbation by Synthetic Agents.}
This \textit{distributed, action-oriented architecture} provides the conceptual and empirical framework for understanding the experimental work developed later in this thesis. 

\noindent
If moral judgment emerges from systems designed to transform perceptual, affective, and interpretive cues into behavioural output, then \textit{alterations to the social or perceptual environment can shift the evaluative computations that guide action}. This point is not merely theoretical: later chapters develop its empirical instantiation by demonstrating how perturbations to the social field modulate the transition from moral salience to prosocial behaviour (see Hypothesis~\ref{hyp:synthetic_perturbation} in Chapter~\ref{chap:experimental_methods}). 

\noindent
A humanoid robot constitutes a particularly revealing form of perturbation: it is perceptually social (in virtue of its humanoid morphology) yet ontologically indeterminate (neither fully agentic nor behaviourally inert). Such indeterminacy can alter attentional allocation, dampen affective resonance, and introduce uncertainty into mental-state attribution. In doing so, it may shift the weighting, timing, or accessibility of evaluative signals, thereby modulating the likelihood that moral appraisal culminates in prosocial action.


Understanding this architecture is therefore essential for interpreting the empirical results that follow. Our experiment does not measure abstract judgments but the practical enactment of moral cognition within a context made ambiguous by the presence of a synthetic observer. The neuroscientific foundations surveyed here thus provide the theoretical scaffolding for explaining how robotic presence can attenuate prosocial action in subtle, yet systematically measurable ways.

\noindent
What follows, however, requires a final conceptual step. If moral cognition is an architecture for transforming evaluative information into action, then \textit{any alteration to the informational field is at least in principle a moral intervention}. The presence of a synthetic agent---especially one exhibiting humanlike form yet lacking a clear place within our evolved social ontology---constitutes precisely such an intervention. It does not supply new moral content; rather, it reconfigures the \emph{conditions under which content becomes behaviourally operative}. In this sense, the moral landscape is not only defined by principles or dispositions but by the topology of the environment in which they are enacted.

\noindent
This insight has two important implications that structure the remainder of the thesis. First, it shifts the explanatory burden from conscious deliberation to the \textit{situated dynamics of evaluative processing}. The experiment that follows examines not what participants claim to value, but how their moral cognition actually functions when confronted with an entity whose status is neither fully social nor fully inert. Second, it reframes the normative question: the significance of artificial agents lies not merely in what they do, but in how their mere presence \textit{reconfigures the normative affordances} of a shared environment. This reframing will prove central when, in later chapters, we consider the limitations of existing Machine Ethics frameworks and the conceptual tension between engineered normativity and human moral practice.

\noindent
In this way, the Moral Primer sets the stage for two convergent lines of inquiry. The empirical chapters will show how minimal synthetic presence can modulate the behavioural expression of moral cognition. The normative chapters will argue that such modulation exposes a broader oversight in contemporary ethical theory for artificial systems: namely, the assumption that moral agency can be understood independently of the environments that scaffold, shape, and sometimes distort human evaluative capacities.

\noindent
Taken together, these threads suggest a view of artificial agents not as moral subjects, nor merely as tools, but as \textit{operators on moral space}: entities capable of bending, refracting, or diluting the pathways through which moral meaning becomes action. The full implications of this claim will emerge only when the empirical and philosophical analyses are placed in dialogue. For now, it suffices to note that understanding how humans make moral decisions under conditions of social and ontological ambiguity is not merely preparatory background---it is the conceptual linchpin for everything that follows.


\noindent
These conceptual foundations also illuminate two methodological commitments that guide the remainder of the thesis: the \textit{Level of Abstraction} at which moral cognition is analysed, and the \textit{topological structure} of the evaluative processes under perturbation. In Floridi’s sense, an LoA fixes the informational parameters relevant to explanation; it determines which distinctions matter and which are bracketed for the sake of epistemic tractability. Here our chosen LoA does not concern the metaphysics of moral agency, nor the normative justification of principles, but the \emph{functional transformation} by which perceptual and affective cues become action-guiding evaluations. It is at this LoA that robotic presence can be treated not as a moral agent but as a \emph{modulator of the evaluative field}.%
\footnote{For discussion of the methodological role of Level of Abstraction in analysing informational systems, see Floridi (2010, 2011, 2013).}

\noindent
Once this LoA is fixed, moral cognition can be understood topologically: as a system that maps inputs to behavioural outputs through a structured configuration of salience, attention, affective resonance, and interpretive inference. Altering the structure of the environment—as occurs with the introduction of a synthetic observer—can therefore be modelled as a deformation of the evaluative landscape. The experiment developed later in this thesis investigates precisely such a deformation: not a change in moral principles, nor a shift in explicit reasoning, but a modification of the \emph{shape} of the cognitive–affective space through which moral meaning travels on its way to action.

\noindent
This topological perspective additionally clarifies why synthetic agents matter ethically even when they perform no overt behaviour \cite{Coeckelbergh2010, Alfano2013MoralEnvironment}. At our operative LoA, the morally relevant property of a robot is not its autonomy or its adherence to ethical rules, but its capacity to warp the attentional and affective gradients that structure human moral appraisal \cite{Zlotowski2015SocialCues, BigmanGray2018}. A robot may therefore function as a semantic attractor or normative deflector, subtly redistributing the vectors through which moral salience exerts its behavioural pull \cite{Alfano2013Trust, Railton2017Scaffolding}. Later empirical chapters provide evidence for such redistributions; later normative chapters examine how these redistributions challenge the assumptions of Machine Ethics, which typically locates moral significance in the agent rather than in the environmental perturbation it induces \cite{FloridiSanders2004, Coeckelbergh2020}.

\noindent
Seen through this joint lens of LoA and moral topology, the empirical question posed by the experiment acquires its full significance: not whether a robot is moral, nor whether it communicates norms, but whether its presence reshapes the evaluative field in which human agents convert moral perception into prosocial behaviour. The answer to that question, and its implications for both moral psychology and the ethics of artificial agents, unfolds in the chapters that follow.

\noindent
To make this intuition formally explicit—without committing the remainder of the thesis to a mathematical framework—we may describe the evaluative system as a mapping
\[
f : \Sigma \longrightarrow \Delta,
\]
where $\Sigma$ denotes the space of perceptual and affective cues, and $\Delta$ the space of action-guiding evaluative states. Within this framework, the presence of a synthetic agent $\mathscr{R}$ can be modelled as a perturbation operator
\[
\mathcal{P}_{\mathscr{R}} : \Sigma \to \Sigma',
\]
inducing a deformation of the salience landscape such that the composed transformation
\[
f_{\mathscr{R}} = f \circ \mathcal{P}_{\mathscr{R}}
\]
yields a different evaluative gradient. In topological terms, $\mathcal{P}_{\mathscr{R}}$ alters the curvature of the moral field, reshaping the trajectories along which attention, affect, and social meaning propagate toward behavioural output. This formalism is offered only as a conceptual anchor: the thesis does not employ topological machinery beyond this illustrative role, but the notion of perturbation provides a precise vocabulary for interpreting the empirical findings that follow.

\bigskip
\paragraph{Philosophical Synthesis: Rational, Virtuous, and Intuitive Moral Minds.}
This perspective also illuminates a deeper philosophical point. Across the history of moral thought, the relation between perception, evaluation, and action has been theorised along three dominant lines. A Kantian picture holds that moral judgment is grounded in rational principles and universalizable maxims; here, perturbation would matter only insofar as it obstructs rational access to duty. An Aristotelian picture instead treats moral action as the expression of a cultivated character, where perception and moral salience form a unified excellence of practical reasoning (\textit{phronesis}). A Humean picture goes further, locating the origin of moral judgment in sentiment, affect, and intuitive appraisal. The cognitive–affective architecture surveyed above aligns most closely with this latter tradition: it suggests that moral judgment is grounded not in detached reasoning but in an integrated evaluative sensitivity to the social world. Thus, when the structure of that world is altered—when its cues are reframed, occluded, or semantically displaced—the moral response shifts accordingly.

\bigskip
\paragraph{Concluding Perspective: Why This Matters for the Thesis.}
Placed within this triangulation of mathematics, psychology, and philosophy, the experimental work to come acquires a precise significance. The thesis does not ask whether robots are moral agents, nor whether they instantiate ethical principles. Rather, it investigates how synthetic presence restructures the evaluative environment in which \emph{human} moral cognition unfolds. By treating robotic co-presence as a perturbation of the moral field, the experiment operationalises a long-standing philosophical question in empirical form: \textit{to what extent is moral action sensitive to the topology of the situation rather than to the content of explicit reasoning?} The answer developed in later chapters shows that even minimal, silent, behaviourally neutral artificial agents can shift the gradients through which moral salience becomes prosocial behaviour. 

\noindent
This insight anchors both the methodological rationale of the experiment and the broader argumentation of the thesis. In the Introduction, it frames the motivation for studying synthetic social influence; in the General Conclusion, it becomes the conceptual pivot for evaluating the ethical implications of robotic presence in human environments. The moral mind, understood through the lenses of abstraction and topology, is not merely a processor of principles but a dynamic, situationally sensitive system—one whose pathways can be subtly bent by the artificial others increasingly woven into our social world.


%%%

\section{Dual-Process Architectures in Moral Cognition}
The distributed moral architecture described in the previous section naturally motivates a family of theories known as \emph{dual-process models}. 

These models posit that moral judgment arises from the interaction of rapid, affectively grounded appraisals with slower, more controlled processes of deliberation and cognitive regulation~\cite{Greene2007, Haidt2001, Cushman2013, Evans2008}. Importantly, dual-process models no longer depict these systems as antagonistic. Contemporary formulations emphasise their continual integration, consistent with the topological and action-oriented framework established earlier \cite{Greene2004, Greene2014Integration, Cushman2013DualSystems}.

This emphasis on dual-process architectures is not merely a matter of theoretical convenience; it reflects a deeper convergence across multiple research programmes concerned with the computational structure of social and moral behaviour \cite{Evans2008, Greene2007}. In Social Signal Processing, Vinciarelli and colleagues argue that human social interaction is supported by parallel channels of affective and inferential processing, where rapid, embodied cues—such as posture, gaze, and micro-expressions—operate alongside higher-level reasoning about intentions and norms \cite{Vinciarelli2012, Vinciarelli2009}. Affective Computing similarly treats emotion as a multi-layered control system in which fast appraisal mechanisms shape behavioural orientation long before explicit reasoning is engaged \cite{Picard1997, Scherer2009}.

By contrast, much of Machine Ethics has historically leaned toward monolithic, deliberative models that treat moral judgment as if it were exclusively rule-based, thereby neglecting the affective and perceptual grounding that empirical evidence shows to be indispensable for real human moral cognition \cite{WallachAllen2009, Moor2006}. The dual-process framework adopted here therefore marks a principled departure from those purely deliberative approaches: it aligns the theoretical lens of this thesis with the empirical reality that moral decision-making arises from the interaction of intuitive, affect-laden appraisals and slower, controlled processes of evaluation and regulation \cite{Haidt2001, Cushman2013}.

This perspective also anticipates the logic of the experimental design developed 
later in this thesis: if moral judgment arises from the continual integration of fast affective cues with controlled interpretive processes, then perturbing either stream—for example, by introducing a humanoid robot that is perceptually salient yet ontologically indeterminate—should measurably alter the resulting behavioural output \cite{BigmanGray2018, Zlotowski2015}. Crucially, such a perturbation is not merely of theoretical interest; it provides empirical traction on a broader question that has become increasingly central in Affective Computing and Social Signal Processing, namely: \emph{how should artificial systems register, represent, and respond to the moral significance of human behaviour in ways that reflect the dynamics of human moral cognition rather than static rule-based prescriptions?} Discoveries that reveal how synthetic agents modulate human moral action therefore speak directly to the design of 
computational architectures capable of participating in, rather than merely implementing, moral practices. A human-centric, discovery-oriented approach to moral AI must be guided by the empirical structure of moral cognition itself—its affective grounding, its interpretive flexibility, and its inherently practical orientation—rather than by fixed normative templates that fail to scale across social contexts. Dual-process theory thus provides both the conceptual and methodological scaffolding for understanding how robotic presence reshapes the balance between intuitive and deliberative pathways, and for developing affective-computational models that align with the lived moral ecology in which human agents actually operate \cite{Greene2014, Evans2008}.


\paragraph{Intuitive and Affective Processes.}
The intuitive stream comprises fast, automatic, and affectively laden evaluations generated by perceptual and subcortical mechanisms. It inherits its computational character from the structures surveyed earlier: the amygdala and anterior insula for rapid affective tagging and aversive appraisal \cite{Phelps2006, Chang2013}; the vmPFC for integrating somatic markers and affective valuations into action-anchored representations \cite{Bechara2000}; and the TPJ–mPFC network for immediate interpretation of agents’ intentions \cite{YoungSaxe2010, SaxeKanwisher2003}. These processes operate at low cognitive cost and generate what can be described, within the topological framework introduced previously, as steep, rapidly forming attractor basins in the evaluative landscape. Intuitive appraisals therefore play a decisive role in shaping the initial direction of behavioural tendencies, often determining which features of a situation are encoded as morally salient before deliberation becomes possible. This mechanism is well documented in Social Signal Processing and Affective Computing, where micro-expressions, gaze cues, posture shifts, and affective markers orient behavioural readiness even in the absence of explicit reasoning \cite{Vinciarelli2012, Scherer2009}. Intuition is thus not a primitive substitute for deliberation but the behaviourally grounded substrate on which moral thought is built.

\paragraph{Controlled and Deliberative Processes.}
The controlled stream is subserved by dorsolateral prefrontal cortex (dlPFC), anterior cingulate cortex (ACC), and lateral parietal networks implicated in rule representation, inhibition, causal reasoning, and long-horizon evaluation \cite{Miller2001, Koechlin2003, Shackman2011}. Controlled processes are engaged when intuitive appraisals conflict with one another or with internalised commitments, when the moral relevance of a situation requires justification, or when environmental ambiguity demands a more structured interpretation. In the topological model, controlled processes reshape intuitive attractors by flattening some gradients and amplifying others, thereby altering the curvature of the evaluative field in ways that enable stable action selection. This modulation is not well captured by the outdated metaphor of a “rational override.” Rather, the controlled system integrates affective, social-cognitive, and normative information into coherent action policies, consistent with philosophical accounts of moral judgment as a species of practical reasoning \cite{Anscombe1957, Korsgaard2009}. Its computational significance extends to Affective Computing: controlled processes supply the representational flexibility required for artificial systems to track context, resolve ambiguity, and form morally relevant action-guiding policies rather than static rule-matching outputs \cite{Picard1997}.

\paragraph{Dynamic Integration.}
Dual-process models thus support the view that moral cognition is an interactive, topologically structured, and dynamically integrated system \cite{Evans2008, Cushman2013}. Intuitive mechanisms generate the initial evaluative landscape—anchored by affect, attention, and perceptual salience—while controlled mechanisms adjust, stabilise, or reinterpret that landscape in light of commitments, norms, or long-term goals. The system’s behaviour is therefore neither fully bottom-up nor fully top-down but emerges from the continual exchange between affective gradients and regulatory constraints. This integrated architecture supplies the mechanistic substrate for the perturbation phenomena examined later in the thesis: when the environment changes, it alters the intuitive gradients that form the initial evaluative field, thereby reshaping the downstream burden on controlled processes \cite{Railton2017}. Crucially, a synthetic agent need not supply explicit reasons to influence moral behaviour; by virtue of its perceptual presence and ambiguous ontological status, it can reconfigure the evaluative field in which reasons become behaviourally operative \cite{Coeckelbergh2010, BigmanGray2018}. 

\noindent
Against this theoretical background, the empirical question addressed later in the thesis becomes more sharply framed. 

\textit{If moral cognition arises from the dynamic integration of fast affective appraisals with slower controlled processes, then perturbations to the perceptual–social environment that alter the initial affective gradients or the subsequent regulatory burden should yield measurable shifts in moral behaviour.}

A humanoid robot constitutes precisely such a perturbation: its perceptual salience, humanomorphic form, and ambiguous ontological status are known to modulate attention, social appraisal, and mind attribution in ways that alter both intuitive and deliberative pathways \cite{BigmanGray2018, Coeckelbergh2010, Zlotowski2015}. These influences operate at the level of the evaluative field itself—reshaping which cues are encoded as morally salient, how affective resonance unfolds, and when controlled processes are recruited to stabilise or reinterpret the situation. Evidence from developmental, affective, and social-cognitive neuroscience suggests that such shifts in salience and ambiguity can reconfigure the timing and accessibility of both intuitive and controlled moral processes \cite{CroneSteinbeis2017, Scherer2009, Vinciarelli2009}. The experiment developed in Chapter \ref{chap:experimental_methods} therefore provides an empirical test of this mechanistic claim: 

\textit{whether the presence of a synthetic observer systematically deforms the evaluative landscape from which prosocial action emerges.}


\section{The Social Intuitionist Model}
While dual-process architectures describe the mechanistic integration of affective and deliberative pathways, Haidt’s \emph{Social Intuitionist Model} (SIM) provides a complementary account of the \emph{social ecology} within which moral judgments are produced and negotiated \cite{Haidt2001, Haidt2007}. SIM stands firmly in the Humean tradition: moral judgment arises first from rapid intuitive appraisals, with explicit reasoning operating primarily as a communicative, justificatory, or reputation-management mechanism. For the purposes of this thesis, its significance is twofold. 

\noindent
First, it anchors moral cognition in perceptual, affective, and socially distributed processes rather than in solitary rational deliberation. Second, it predicts that even minimal, ambiguous, or merely perceptual forms of social presence can reshape the intuitive component of moral judgment—precisely the mechanism probed by the experiment that follows.

\paragraph{Primacy of Intuition.}
According to SIM, intuitive appraisals constitute the generative core of moral judgment. These appraisals arise rapidly—on the order of hundreds of milliseconds—through affective and perceptual pathways that register norm violations, suffering, fairness cues, or socially meaningful stimuli long before conscious deliberation is possible \cite{Luo2006, Sarlo2012}. Neurocognitive evidence shows that harm detection, norm sensitivity, and interpersonal appraisal are instantiated in circuits associated with affective tagging (amygdala, anterior insula), mental-state attribution (TPJ, mPFC), and embodied simulation. Within the topological framework developed earlier, SIM corresponds to the idea that intuitive processes generate the \emph{initial curvature} of the evaluative landscape: they create steep affective gradients that orient subsequent interpretation and action.

This primacy is not merely temporal but structural: intuitive appraisals determine which environmental features are encoded as morally salient. In Social Signal Processing and Affective Computing, analogous mechanisms are observed in the rapid extraction of gaze, posture, and affective micro-signals that guide interpersonal coordination \cite{Vinciarelli2012, Scherer2009}. SIM therefore provides the social–cognitive analogue of the perceptual–affective machinery surveyed earlier: intuitions are not post hoc artefacts but the first-order drivers of moral cognition.

\paragraph{Reason as Interpersonal.}
SIM also reconceptualises reason not as the architect of moral judgment but as a socially situated \emph{modulatory process}. Deliberation is typically invoked after intuitive appraisals have already fixed the valence and direction of judgment. It functions primarily to justify existing intuitions, align with interlocutors, negotiate reputation, or repair social discord \cite{Haidt2001}. Philosophically, this positions moral reasoning closer to an interpersonal practice—akin to Strawsonian norm negotiation or Scanlonian contractual justification—than to a solitary search for objective moral facts.

At the Level of Abstraction operative in this thesis, such reasoning does not generate the moral evaluation; it operates on a pre-structured evaluative field shaped by intuition, affect, and social meaning. SIM therefore aligns with the broader theoretical framework established earlier: moral cognition is fundamentally action-oriented, socially embedded, and sensitive to perturbations in the perceptual–social environment.

\paragraph{Relevance to Synthetic Presence.}
The Social Intuitionist Model acquires particular methodological force in relation to the experiment developed later in the thesis. If moral intuition is tuned to social presence—especially to the mere perception of another mind, agent, or observer—then introducing a humanoid robot with ambiguous ontological status constitutes a direct perturbation of the intuitive machinery itself. Empirical work shows that ambiguous agents modulate attentional allocation, emotional resonance, and intuitive moral appraisal \cite{BigmanGray2018, Roeser2018, Zlotowski2015}. Within SIM’s framework, such modulation occurs \emph{prior} to deliberation: the robot reshapes the intuitive gradients that structure the evaluative field, thereby altering the likelihood that moral salience will translate into prosocial action.

Thus SIM not only complements the dual-process architecture but also provides a socially grounded explanatory lens for the modulation effects measured in the experiment. It predicts precisely the kind of subtle, pre-reflective, yet behaviourally measurable displacement that emerges when a synthetic agent perturbs the affective–social substrate from which moral judgments arise.


\paragraph{Synthetic Presence and Social Perturbation.}
SIM is particularly powerful when considering \emph{minimal sociality}. A humanoid robot constitutes a perceptually social yet ontologically indeterminate entity. Its presence can influence intuitive appraisal by shifting attention, altering affective resonance, or modulating perceived social oversight. These shifts occur at the intuitive stage of moral processing, thereby modifying the evaluative gradients that shape action. SIM thus provides a conceptual bridge between the empirical findings of the experiment and the theoretical claim that synthetic agents restructure evaluative topology even in the absence of explicit communication or normative instruction.


\section{Prosocial Behaviour as Moral Action}

The conclusion reached in the previous section—that minimal or ambiguous social presence can reshape intuitive appraisal—immediately motivates a shift from internal judgment to observable action. At the Level of Abstraction adopted throughout this thesis, moral cognition is defined by its action-guiding role:\textit{ it is a system whose explananda are behavioural commitments rather than verbal reports. }

\noindent
If intuitive appraisal is the first locus at which synthetic presence can deform the evaluative landscape, then the empirically accessible signature of such deformation must be sought not in explicit justification but in the practical enactment of moral cognition. This makes prosocial behaviour—cooperation, helping, and in particular, \textbf{costly resource donation}~\cite{Batson2011, FehrGachter2002, Henrich2005, Tomasello2016, Warneken2015, Baumard2013, Crockett2016, Scanlon1998, Darwall2006}—the principled site at which perturbations to the evaluative topology become experimentally tractable\cite{HaleyFessler2005, ShariffNorenzayan2007, BigmanGray2018, Zlotowski2015, Alfano2013Character, Doris2002, Gintis2014MoralComputation}.

Prosocial behaviour is not merely an altruistic variant of social action; it is one of the most reliable cross-disciplinary indicators of moral engagement. Across behavioural economics, developmental studies, evolutionary game theory, and empirical moral psychology, patterns of helping, sharing, and charitable giving consistently track agents’ sensitivity to fairness, harm, reciprocity, compassion, and need \cite{Batson2011, Henrich2005, FehrGachter2002, Tomasello2016, Warneken2015}. Philosophical accounts converge on the same point. Whether in Scanlon’s framework of interpersonal justifiability \cite{Scanlon1998}, Darwall’s second-personal standpoint \cite{Darwall2006}, or constitutivist and planning-theoretic models of agency \cite{Korsgaard1996, Korsgaard2009, Bratman1987, Velleman2000}, prosocial actions manifest a normatively loaded form of practical identity: 

\textit{they reveal how an agent construes her reasons, acknowledges the claims of others, and binds herself to outcomes she takes to be morally required. In this sense, prosocial behaviour is not merely expressive but} \emph{commitment-realising}. 

It shows how evaluative appraisals become operative in action, rather than hypothetically endorsed in speech.

This distinction between avowed moral attitudes and their embodiment in action has become central to contemporary accounts of moral psychology and agency. Experimental work demonstrates that explicit judgments are weak predictors of real-world helping, whereas behavioural tasks expose the operative structure of moral evaluation \cite{Cushman2013Action, Gintis2014MoralComputation}. Philosophical analyses of responsibility, reactive attitudes, and self-governance likewise locate normativity in enacted agency rather than assent \cite{Strawson1962, Wallace1999, Arpaly2003, Railton2017Agency}. Against this background, the methodological choice of using charitable donation as the dependent variable in the experiment is not an operational convenience but a principled commitment to analysing moral cognition in its practical register.

Prosocial action emerges from a structured sequence already outlined in earlier sections: detection of morally salient cues; intuitive appraisal of their affective and social significance; controlled modulation when competing commitments must be adjudicated; and finally, behavioural execution. Each stage is exquisitely sensitive to the social field. Whether an agent feels watched, whether the observer is human or synthetic, whether the presence is affectively warm, neutral, or indeterminate—all of these modulate the trajectories through the evaluative topology. Neuroimaging studies confirm that prosocial choice draws on the same integrated circuitry that underwrites harm aversion, empathic concern, valuation, and norm enforcement \cite{Moll2002, Decety2004}. Within the topological framework developed earlier, prosocial behaviour marks the \emph{terminal attractor} of a moral trajectory: when salience is strong and unimpeded, the system converges on a stable basin corresponding to prosocial action.

It is precisely this structure that renders prosocial donation an ideal test variable for the experimental paradigm developed in this thesis. If the presence of a humanoid robot—perceptually social yet ontologically ambiguous—alters attentional focus, affective resonance, or the perceived structure of social oversight, then those perturbations need not appear in verbal justifications; they will appear in the \emph{behavioural expression} of moral cognition. The experiment therefore does not treat donation as a proxy for morality in a naïve sense, but as a theoretically grounded behavioural readout of the evaluative topology: a measurable manifestation of how moral salience is transformed into action under conditions of synthetic perturbation. In this view, detecting attenuation in prosocial behaviour is detecting deformation in the underlying evaluative field.


\paragraph{Why Prosocial Behaviour Serves as a Proxy for Moral Action.}
Within the theoretical framework developed throughout this chapter, prosocial behaviour is not an auxiliary behavioural measure but the natural terminus of moral cognition understood as a practical, action-guiding system. Across divergent philosophical traditions, there is agreement on this point: for Aristotle, the telos of ethical reflection is \textit{praxis}~\cite{AristotleNicomacheanEthics, McDowell1979VirtueReason, Burnyeat1980Aristotle}
; for Kant, moral judgment manifests in the capacity to act from obligation~\cite{KantGroundwork, Korsgaard1996Sources, Allison2011KantEthics}
; for Hume, moral distinctions acquire motivational force only through sentiment~\cite{HumeTreatise, HumeEnquiry, Cohon2008Hume}
. Despite their incompatibilities, all three converge on the thesis that the mark of moral cognition is its capacity to reorganise an agent’s field of reasons so as to issue in action. 

In computational terms, a prosocial act such as monetary donation reveals that the evaluative field has reached a locally stable configuration: moral salience has been detected, weighted, and rendered behaviourally operative despite competing self-regarding incentives. Donation therefore provides access to what this thesis calls the \textit{operational core} of moral cognition: the point at which evaluative topology is strong enough to yield observable practical commitment. It is, in this sense, not a surrogate for morality but its empirical footprint. What the agent does with her own resources in a morally charged situation is the clearest behavioural index of how moral meaning has been processed and transformed by the cognitive–affective machinery surveyed in earlier sections \cite{Cushman2013Action, Gintis2014MoralComputation}.

\paragraph{Relevance for Synthetic Perturbation.}
Because prosocial behaviour is the final expression of the evaluative trajectory, it is also the level at which perturbations to the moral field become measurable. Synthetic presence—especially when perceptually social yet ontologically indeterminate—can alter the evaluative topology long before explicit reasoning is recruited. Changes in attentional allocation, reductions in affective resonance, or increased uncertainty in mental-state attribution operate at the intuitive tier and propagate through the system, subtly reshaping gradient structures and attractor dynamics. Such perturbations rarely manifest in explicit moral self-reports, which are coarse, post hoc, and socially filtered; instead, they emerge in the \emph{behavioural expression} of moral cognition.

The experimental design developed in Chapter \ref{chap:experimental_methods} leverages precisely this fact. By embedding a morally salient stimulus (the charity prime) within an environment modulated by a silent humanoid robot, the paradigm tests how ontological ambiguity refracts the integration of intuitive and deliberative processes. A reduction in donation is therefore not interpreted as the failure of moral principle, nor as evidence of diminished norm endorsement, but as a deformation in the evaluative pathway linking moral perception to prosocial behaviour. In topological terms, the presence of the robot shifts the curvature of the moral field, altering the system’s convergence tendencies and lowering the probability of reaching the prosocial attractor.

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Conceptual Synthesis: Why Prosocial Donation Measures Moral Perturbation]
		Prosocial donation is the behavioural endpoint of the evaluative architecture that transforms moral salience into action. Because moral cognition is inherently practical, perturbations to its perceptual, affective, or interpretive components manifest most reliably in the structure of behavioural output. A humanoid robot---perceptually social yet ontologically ambiguous---modulates this architecture not by issuing commands but by reshaping the evaluative field in which moral meaning becomes behaviourally operative. Measuring donations under synthetic perturbation therefore provides a principled, theoretically grounded method for detecting how artificial presence deforms the transition from moral appraisal to prosocial commitment.
	\end{tcolorbox}
\end{center}

\bigskip
\paragraph{Concluding Perspective: Why This Matters for the Thesis.}
The argument developed across the preceding sections now converges: dual-process architectures reveal that moral cognition arises from the continuous integration of intuitive, affect-laden evaluations with controlled interpretive processes; the Social Intuitionist Model emphasises the primacy of intuitive, socially responsive appraisal; dynamic integration shows how perturbations to early evaluative gradients propagate through the system; and the analysis of prosocial behaviour establishes that the appropriate level of empirical access is behavioural rather than declarative.

Placed against this background, the experimental work to follow acquires its precise theoretical meaning. The thesis does not ask whether robots possess moral agency, nor whether they instantiate ethical principles, nor whether they trigger reputational reasoning in any explicit sense. Instead, it investigates how the presence of a synthetic, perceptually social, ontologically ambiguous agent reshapes the evaluative topology through which \emph{human} moral cognition unfolds. The robot is treated not as a quasi-person but as a perturbation operator on the moral field: a source of structural deformation capable of altering the gradients through which moral salience is transformed into prosocial action.

In this light, the experimental question becomes a refined philosophical one: 
\begin{quote}
	\emph{To what extent is moral action sensitive to the situational topology in which it is embedded, rather than to the content of explicit reasoning or principle endorsement?}
\end{quote}
The answer, as subsequent chapters demonstrate, is that even minimal, silent, behaviourally neutral artificial agents can tilt the evaluative landscape, shifting the balance between intuitive and deliberative processes and decreasing the probability that moral salience converges on prosocial action. This finding is not anecdotal but structurally illuminating: it shows that the moral mind, when understood through the lenses of abstraction and topology, is a dynamic, context-responsive system whose behavioural outputs depend on the shape of the environment as much as on internal principles.

This insight anchors the methodological rationale of the experiment, motivates the normative analysis developed in the Ethical Cognition chapter, and frames the broader implications discussed in the General Conclusion. It also sets the stage for a technomoral claim that animates the thesis as a whole: as artificial agents become woven into ordinary human environments, the topology of moral life itself may be subtly, pervasively reshaped—not through explicit persuasion, but through shifts in the evaluative fields within which moral cognition takes place.

%%%THE END!
