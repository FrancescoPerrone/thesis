\chapter{Cognitive–Affective Architecture of Moral Judgment}
\label{chap:moral_primer}
\thispagestyle{pprintTitle}


% Adjusting epigraph settings
\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushright}

The conceptual apparatus developed in this chapter is not an ornamental introduction to moral theory. It is the minimum set of distinctions required to make the research question itself intelligible. The project asks:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Can the mere presence of a synthetic, non-agentic entity perturb the inferential 
			transformation through which morally salient cues are converted into observable moral 
			behaviour?}
	\end{leftbar}
\end{center}

\bigskip
\noindent


that is to say whether the mere presence of a synthetic agent can alter the trajectory by which human beings transform a morally salient perception into a morally relevant action. Such a question does not belong to the domain of ethical theory; it belongs to the domain of moral cognition.

To address it, one must understand the cognitive–affective substrate in which moral judgments are formed, weighted, and enacted. The behaviour observed in the experiment does not arise at the level of explicit reasoning, rule application, or reflective justification. It arises upstream, within the processes that determine what becomes salient, how empathic resonance is allocated, which cues are attended to, and how the felt sense of accountability is modulated. These are the mechanisms through which moral evaluation becomes behaviourally operative; without a precise understanding of them, the central phenomenon of this thesis is not only unexplained but incorrectly described.

Reflective moral theories---Kantian maxims, utilitarian calculus, contractualist reasoning---do not operate at this level. They articulate justificatory relations, not generative mechanisms. They tell us why an action may be defensible, not how the human cognitive system produces the behaviour in the
first place \cite{Korsgaard1996,Scanlon1998,SidgwickMethods}. For this reason, any attempt to explain the experimental effect by appealing to ethical principles is methodologically misaligned. 

It begins at a Level of Abstraction that the phenomenon does not inhabit \cite{Floridi2008,Floridi2011}.

What is required instead is an account of moral cognition as an action-guiding evaluative process: a process in which affect, attention, salience, social interpretation, and contextual meaning jointly determine how moral cues acquire behavioural force. A large body of work in moral psychology and cognitive neuroscience demonstrates that these mechanisms---affective appraisal, empathic
resonance, intuitive evaluation, and attentional modulation---constitute the causal substrate of moral judgment \cite{Haidt2001,Greene2001,Greene2004,Cushman2013,Decety2004,Crockett2016}. Only within such a framework can the influence of synthetic presence be meaningfully specified. Without it, the
experimental result risks being mischaracterised as a change in moral belief or a failure of deliberation, when in fact it is a perturbation of the evaluative field that precedes both~\cite{Conty2016,Nettle2013,Pfattheicher2015}.

The purpose of this chapter is therefore clarificatory in the strictest sense. It isolates the cognitive--affective mechanisms that constitute the causal substrate of moral behaviour; it distinguishes them from the reflective structures of ethical theory; and it establishes the Level of Abstraction at
which the research question resides \cite{Floridi2008,Floridi2011}.

By doing so, it provides the conceptual conditions under which the empirical findings of the thesis can be correctly interpreted. The experiment does not test principles, preferences, or doctrines. It tests the stability of the evaluative machinery through which moral meaning becomes action. Understanding that machinery is the only way to understand the phenomenon under investigation.

This is why the chapter must take the form it does.
Not to broaden the discussion of morality, but to focus it precisely at the level where the phenomenon of synthetic moral perturbation arises.


A recurring theme across the reviewed literature is that failures in Machine Ethics stem from two related errors: \emph{category mistakes} and \emph{LoA conflation}. 

Category mistakes arise when reflective normative principles are
treated as if they described the psychological mechanisms that generate moral behaviour; LoA conflation occurs when descriptive cognitive regularities are mistaken for normative constraints or vice versa \cite{Floridi2008,Floridi2011}. Both errors follow from neglecting the fact that justificatory structures live at
a high Level of Abstraction, whereas moral behaviour is produced at a lower, cognitive--affective LoA documented in moral psychology and social cognition~\cite{Haidt2001,Greene2004,Decety2004}. 

Recognising these distinctions is methodologically essential: without LoA discipline, interpretive models of
moral perturbation become confused, and empirical findings—such as the attenuation effects examined in this thesis (Chapter~\ref{chap:exp_methods})—risk being mischaracterised as
failures of reasoning rather than as deformations of the evaluative field.

\section{Descriptive and Normative Domains}

The term ``morality'' spans at least two analytically distinct domains. The first is \emph{descriptive morality}: 


\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{the empirical study of how humans form moral judgments, experience moral emotions, and engage in normatively salient actions.}
	\end{leftbar}
\end{center}

\bigskip
\noindent

 This includes developmental psychology \cite{Kohlberg1981}, social–cognitive models \cite{Haidt2007, DorisSep2020}, affective neuroscience \cite{Moll2002, Decety2004}, and evolutionary accounts of cooperation and prosociality \cite{Joyce2006, Tomasello2016}. 
 
 The second is \emph{normative morality}: 
 
 \bigskip
 \noindent
 \begin{center}
 	\begin{leftbar}
 		\textit{the domain of ethical theorising concerned with how one ought to act.}
 	\end{leftbar}
 \end{center}
 
 \bigskip
 \noindent
 
This domain encompasses deontological, consequentialist, contractualist, and virtue-theoretic traditions \cite{Hursthouse1999, Hooker2000, Anscombe1957, Korsgaard2009}.

These domains are distinct but interdependent. Descriptive accounts illuminate how agents actually evaluate and respond to situations, while normative theories articulate standards for justified action. Empirical models of moral cognition acquire meaning partly through the normative vocabulary within which moral judgments are articulated, while normative theories must remain constrained by what agents are psychologically capable of performing or understanding.

The distinction between descriptive and normative morality is introduced at this point in the chapter because it provides the final conceptual boundary required before the empirical and theoretical analysis can proceed. Without it, two serious confusions would arise—each of which would undermine the scientific aims of the project.

First, moral terminology in technical disciplines is often used ambiguously. Words like obligation, responsibility, harm, or trust are employed as if their meaning were self-evident, yet researchers oscillate unconsciously between describing how agents in fact behave and prescribing how they ought to behave. This sliding between domains produces conceptual instability: experimental findings are mistaken for ethical insights, and normative claims are misinterpreted as empirical predictions.

Second, the research question of this thesis is strictly descriptive: \textit{Can synthetic presence alter the evaluative processes through which humans convert moral perception into moral action?}

To answer this question, the project must operate within the empirical domain of moral psychology. If this boundary is not explicitly marked, the analysis risks drifting into normative interpretation—treating behavioural attenuation as moral deficiency, or treating reflective theories as mechanistic explanations.

The descriptive–normative distinction therefore performs a crucial clarificatory function:

\noindent
The distinction identifies the \textbf{level at which the thesis operates}. The aim is not to determine what people \emph{should} do in the presence of robots, but to explain what \emph{does} happen within the cognitive--affective architecture when artificial agents enter the evaluative field. Such phenomena require descriptive tools: models of attention, salience, empathy, and social meaning—not principles or moral doctrines \cite{Haidt2001,Greene2004,Decety2004,Pfattheicher2015,Conty2016}. 

\noindent
Second, the distinction prevents the \textbf{misinterpretation of empirical findings} as moral judgments. If a robot’s presence reduces prosocial behaviour, this is a psychological effect, not a moral failure. It does not imply that agents have acted wrongly or that the robot has transgressed any ethical boundary. It reflects a perturbation in the evaluative machinery that gives moral cues their behavioural force \cite{Haley2005,Bateson2006,Dear2019,Kuchenbrandt2011,Malle2016}. 

\noindent
Third, the distinction isolates the \textbf{causally relevant components of morality} for the experiment. The mechanisms at stake—affective resonance, accountability salience, attentional modulation—belong entirely to descriptive cognition \cite{Decety2004,Crockett2016,Vinciarelli2009,Conty2016}. Normative theories are indispensable for understanding the structure of moral reasoning, but they do not generate behaviour \cite{Scanlon1998,Korsgaard1996,SidgwickMethods}. Keeping the domains separate ensures that the phenomenon is examined at the correct Level of Abstraction \cite{Floridi2008,Floridi2011}. 

\noindent
Finally, the distinction prepares the ground for \textbf{integrating normative theory later without conceptual confusion}. Normative materials will reappear, not as behavioural engines, but as structural constraints within the evaluative topology—deontic invariants, consequentialist gradients, virtue-theoretic attractors, sentimentalist vectors, and contractualist equilibria~\cite{Greene2014,Foot2001,Hursthouse1999,Annas2011}. This reinterpretation is only possible once descriptive and normative domains have been clearly disentangled.

\noindent
In sum, the distinction is introduced here because it secures the conceptual boundary conditions of the thesis. It establishes the domain in which the claims are made, prevents methodological conflation, and ensures that the phenomenon under investigation—moral perturbation under synthetic presence—is analysed at the level where it actually occurs. The orientation of the thesis is therefore precise: moral cognition is the object of study; normative theory provides the vocabulary of justification; and coherence requires that these domains remain distinct.


The project now turns to a minimal operational definition of morality. This may appear abrupt, but its placement at this point in the chapter is deliberate. The preceding sections established the conceptual boundaries required to analyse moral cognition without collapsing distinct Levels of Abstraction or importing normative assumptions into descriptive models. Having drawn these boundaries, the thesis now requires a definition precise enough to guide empirical and theoretical analysis, yet modest enough to avoid the philosophical commitments associated with substantive normative theories.

\subsection{Why Definitions Vary}

There is no single universally accepted definition of morality, and this plurality is neither accidental nor superficial. Different research programmes emphasise different elements of the moral domain. Cognitive approaches foreground the mechanisms by which agents form evaluative judgments \cite{Doris2010}; affective traditions emphasise the emotional systems that underpin moral concern \cite{Nussbaum2001}; rationalist accounts privilege normative reasoning \cite{Korsgaard2009}; social-scientific models attend to conventions and cultural norms \cite{Bicchieri2016}; evolutionary frameworks focus on the adaptive functions of cooperation and prosociality \cite{Joyce2006,Tomasello2016}. Philosophical traditions likewise diverge in grounding morality in rationality, sentiment, virtue, utility, social contracts, or evolutionary pressures.

Computational treatments often inherit only one strand of this diversity. They default to rule-based perspectives not because such models accurately describe human moral cognition, but because they are structurally convenient to implement \cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. This convenience has encouraged the misleading interpretation of moral behaviour as rule following and has fostered oversimplified models of moral decision-making that obscure the cognitive–affective architecture through which real moral judgments are produced \cite{Haidt2001,Greene2001,Cushman2013,Decety2004}.

A primary aim of this chapter is therefore corrective: to replace these inherited simplifications with a framework grounded in contemporary moral psychology, cognitive science, and social-signal research \cite{Pentland2007,Vinciarelli2009,Conty2016}. Only such a framework can support the empirical and conceptual analysis required by the research question.


\subsection{Minimal Operational Definition for This Thesis}

Within this clarified landscape, the thesis adopts the following minimal, action-oriented definition of moral cognition:

\begin{quote}
	\emph{Moral cognition is the evaluative process through which agents detect normatively salient features of a situation, generate judgments concerning permissible or obligatory actions, and select behaviour accordingly.}
\end{quote}

This definition is intentionally modest. It avoids entanglement in substantive normative theories while isolating the components necessary for empirical investigation: evaluation, judgment, and action. It reflects contemporary moral psychology, which treats moral cognition as the product of interacting affective and cognitive mechanisms~\cite{Haidt2001,Greene2001,Cushman2013,Decety2004}, and it coheres with the theoretical machinery developed throughout this thesis—evaluative topology, Levels of Abstraction~\cite{Floridi2008,Floridi2011}, and synthetic perturbation as documented in HRI and SSP~\cite{Malle2016,Bremner2022,Vinciarelli2009}.

Under this definition, moral cognition functions as a mapping from situational cues to action policies, shaped by trait-level dispositions \cite{BaronCohen2003,Habashi2016} and by the affective and attentional structures of the evaluative field \cite{Conty2016,Pfattheicher2015}. It provides the minimal conceptual anchor required to examine how synthetic presence modulates the transformation from moral perception to moral action.

Before proceeding to the distinction between factual and normative judgments, it is important to make explicit what has been achieved in the preceding sections. Although these sections are primarily conceptual, they perform essential scientific functions. They do not merely summarise philosophical background; rather, they establish the explanatory conditions under which the empirical and theoretical claims of the thesis become possible. Three achievements are central.

First, we have identifying the correct level of explanation for the research question. The literature review and the clarificatory sections that follow it isolate the cognitive–affective Level of Abstraction as the locus of the phenomenon under investigation. This is not a descriptive flourish: it is a scientific result.
By showing that the perturbation induced by synthetic presence occurs upstream of explicit reasoning, these sections locate the causal substrate that must be modelled if the experiment is to be intelligible. Without this, the observed attenuation could not be interpreted without ideological or normative distortion.

Second, we have eliminated those category errors that distort empirical interpretation. The distinction between descriptive and normative domains, and the clarification of their respective inferential structures, remove a set of systematic mistakes that plague the technical literature. This is not conceptual housekeeping; it is \textbf{methodological decontamination}. By preventing the importation of prescriptive content into cognitive models—or the projection of cognitive regularities into normative claims—the chapter ensures that empirical outcomes are interpreted within the correct domain. This conceptual hygiene is a precondition for generating reliable scientific knowledge.

Third, we have established a minimal, action-guiding definition of moral cognition. The operational definition introduced in the previous section is itself a contribution. It provides the first precise specification of the cognitive object under study: moral cognition understood as an evaluative process connecting situational cues to action selection.
This definition constrains the mechanisms that may legitimately be invoked as explanations—salience, affect, attention, social meaning—and excludes mechanisms that belong to the wrong LoA. It also provides the structural interface between empirical data and the evaluative-topological model developed later.

Collectively, these achievements secure the conceptual foundations of the thesis. They define the explanandum, delimit the explanatory layer, and prevent methodological conflation \cite{Floridi2008,Floridi2011}. Only after completing this work can the project turn to finer distinctions—such as the difference between factual and normative judgments—that further refine the architecture of moral cognition at the level where synthetic 
perturbation takes effect \cite{Scanlon1998,Korsgaard1996}.

This is why the next section follows naturally. Understanding moral perturbation requires understanding which kinds of judgments are being perturbed. Synthetic presence does not alter factual beliefs; it alters the evaluative force that connects normative appraisal to behaviour. The distinction between factual and normative judgment is therefore not decorative: it is the next analytic step in specifying the mechanism through which moral cognition is modulated~\cite{Cushman2013,Haidt2001,Greene2014}.


\section{Judgments: Factual and Normative}

A central distinction for analysing moral cognition—and for understanding the experimental phenomenon at the heart of this thesis—is the difference between factual and normative judgments. Although both concern evaluations of situations, they operate at distinct logical and functional levels. Factual judgments describe states of affairs: they answer questions about what is the case. Normative judgments concern what ought to be done, what is \textit{permissible}, \textit{required}, or \textit{forbidden}. The distinction is classical in philosophy, yet remains frequently blurred in computational and psychological treatments of morality \cite{Black1972, Deigh2010}. Its importance here lies in the fact that:


\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{synthetic perturbation affects normative judgment, even though the factual perception of the situation might remains unchanged.}
	\end{leftbar}
\end{center}

\bigskip
\noindent

Because the synthetic perturbation operates selectively on the normative layer, we must first clarify what distinguishes normative judgment from the factual input on which it depends. Only then can we specify the mechanism that is being modulated.

Factual judgments derive their correctness from empirical features of the world; their truth depends on observation or inference. Normative judgments embed reasons for action—they carry prescriptive force even when tacitly represented \cite{Hare1981, Korsgaard2009}. This is more than a semantic contrast. It marks a functional division within the cognitive architecture: judgments about what is engage classificatory and predictive systems, whereas judgments about what ought to be done recruit mechanisms that assign motivational weight, integrate affective cues, and generate the directional force that links evaluation to action.

This division maps directly onto the psychological conception of moral cognition, understood as the ensemble of perceptual, affective, and inferential processes that register morally salient features and transform them into evaluative representations \cite{Greene2001, Haidt2001}. Moral cognition includes explicit moral judgment as well as the upstream mechanisms that detect salience, encode social meaning, and initiate the transition from appraisal to behaviour \cite{Cushman2013, Young2012}. The descriptive–normative distinction is mirrored in these systems: factual information is processed by mechanisms specialised for representational accuracy, while normative appraisal engages systems that confer action-guiding significance~\cite{Moll2002, Greene2004, Shenhav2017}.

Psychological models therefore treat factual information as input to evaluative appraisal~\cite{Cushman2013ActionOutcomeValue, CushmanYoung2009Beyond, CushmanGreene2012Common, Mikhail2007UMG}. Normative judgment requires an additional mapping: the transformation of descriptive cues into action-guiding evaluations~\cite{Hindriks2015Normative, Greene2014Beyond, Railton2017MoralLearning}. Collapsing normative into factual judgment erases this architecture. For empirical research—and especially for paradigms measuring moral behaviour—maintaining this distinction prevents behavioural outputs from being mistaken for moral endorsement or internalised norms.

This separation also clarifies the mechanism probed by the experiment. Synthetic presence does not alter what participants believe about the scenario. \textit{It alters how strongly normative force is experienced}. The attenuation effect is therefore not a change in factual judgment but a deformation of the evaluative dynamics that convert normative appraisal into action.

Recognising this prepares the ground for the next step. Once factual uptake and normative evaluation are disentangled, it becomes clear that moral judgment cannot be reduced to belief or emotion alone. It arises from the coordinated operation of perceptual, affective, inferential, and motivational systems that jointly confer normative authority and behavioural direction. It is this internal evaluative architecture—linking perception to action—that synthetic presence perturbs. To understand how such perturbation is possible, we now examine the structure of moral judgment itself.


\section{Internal Architecture of Moral Judgment}

Moral judgments are not mere expressions of preference or affective reaction. They exhibit a characteristic structure that combines evaluative content, justificatory grounding, and action-guiding force~\cite{Smith1994MoralProblem, Railton1986MoralRealism, Blackburn1998RulingPassions, Gibbard1990WiseChoices, Korsgaard1996Sources}. For the purposes of this thesis, a moral judgment involves at least three interlocking components:

\begin{enumerate}
	\item \textbf{Salience detection}: the recognition that a situation contains normatively relevant features—harm, fairness, honesty, obligation, care. This process draws upon perceptual, affective, and social-cognitive systems \cite{Moll2002, Decety2004}.
	\item \textbf{Evaluative appraisal}: the assessment of those features in light of internalised norms, dispositions, or reasons. This appraisal may be intuitive or reflective, emotionally charged or deliberative, depending on context and individual differences \cite{Nussbaum2001, Doris2010}.
	\item \textbf{Practical commitment}: the formation of an action-guiding stance, in which the judgment functions as a reason for or against a particular behaviour \cite{Anscombe1957, Korsgaard2009}.
\end{enumerate}

These components distinguish moral judgments from other evaluative acts—such as aesthetic impressions or strategic choices—and ground the thesis’s operational conception of moral cognition as an \textbf{evaluative mapping} from situational cues to action policies. They also clarify why synthetic perturbation can alter behaviour without altering factual beliefs: the perturbation targets the mechanisms that assign motivational weight, not the mechanisms that register empirical information.

This tripartite structure accommodates both intuitive and deliberative models of moral judgment. Intuitive processes typically dominate in everyday moral encounters; yet even when reasons are not explicitly articulated, these judgments retain justificatory form~\cite{Haidt2001, Greene2014Beyond, Railton2017MoralLearning, Mikhail2007UMG}. Conversely, deliberative processes involve explicit reasoning, counterfactual consideration, and appeals to principles or character traits \cite{Hursthouse1999}. This duality reflects not two kinds of morality, but two modes of access to the same evaluative architecture.

This distinction between intuitive and deliberative processes is not merely taxonomic; it initiates a deeper inquiry into the mechanisms that make moral judgment possible. To understand why certain stimuli reliably elicit prosocial behaviour whereas others disrupt or attenuate it, we must examine the architecture through which moral salience is perceived, represented, and acted upon. The transition from perception to appraisal, and from appraisal to action, is mediated by identifiable affective, perceptual, and executive systems, each contributing distinct computational roles within the broader evaluative ecology.

As the next section shows, contemporary psychological and neuroscientific research converges on a model of moral cognition as a distributed, dynamically interactive network. This framework clarifies how humans ordinarily navigate morally charged environments and provides the conceptual foundation for understanding how these processes may be perturbed—subtly yet systematically—by the presence of agents whose social and ontological status is ambiguous, such as humanoid robots. In this sense, the empirical foundations surveyed below serve as the substrate upon which the subsequent experimental analysis is built.

Understanding the internal architecture of moral judgment is not an abstract philosophical exercise. It is a methodological necessity imposed by the research question and the experimental paradigm developed in later chapters. The phenomenon under investigation—the attenuation of prosocial behaviour in the presence of a silent humanoid robot—occurs precisely within the architecture just described. Without a clear account of this architecture, the empirical effect would be unintelligible or, worse, misinterpreted.

The experiment demonstrates that the presence of a humanoid robot does not alter what participants believe about the situation. The factual content of the scenario remains stable. What changes is the normative force experienced in response to it: the directional pressure that transforms evaluative appraisal into action. Such a shift can only be understood if moral judgment is recognised as a composite process involving salience detection, affective appraisal, and practical commitment. The attenuation effect reveals a perturbation in one or more of these components—the curvature of the evaluative field—rather than any alteration in belief or principle.

This analysis also clarifies why the ontological ambiguity of the robot is central rather than incidental. The NAO robot used in the experiment possesses no beliefs, goals, or communicative intentions. Yet it is perceptually agentic: its morphology, gaze posture, and embodied presence activate social-cognitive mechanisms ordinarily reserved for human agents. This ambiguous status—more than an object, less than a person—positions the robot uniquely within the evaluative architecture. It can recruit salience-detection systems, modulate affective appraisal, or reshape perceived accountability without supplying any of the intentional content associated with genuine agency.

In other words, the robot functions not as a locus of moral claims but as a perturbation operator acting on the substrate that generates moral judgment. Recognising this requires precisely the distinctions drawn in this chapter: between descriptive and normative domains, between factual and normative judgments, and between intuitive and deliberative processes. These distinctions allow us to see what the empirical effect is—a deformation of the evaluative field—and what it is not: a change in belief, a failure of reasoning, or an abandonment of moral principle.

For the reader who has progressed to this point in the thesis, the significance should now be clear. The conceptual machinery developed in this chapter is not preparatory ornamentation; it is the explanatory foundation upon which the entire project rests. The experiment measures subtle changes in prosocial behaviour, but the theoretical contribution lies in explaining why such changes occur and how artificial agents exert influence within the cognitive–affective ecology of moral judgment. Only with a precise account of the internal architecture can the thesis articulate, diagnose, and ultimately theorise the phenomenon of synthetic moral perturbation.

This is the point where philosophical analysis, cognitive science, and experimental design converge. And it is within this convergent space that the remainder of the thesis will operate.


\subsection{Psychological and Neuroscientific Foundations of Moral Decision-Making}

A substantial body of cognitive neuroscience demonstrates that moral decision-making does not arise from a single “moral centre.” Instead, it emerges from coordinated activity across affective, social-cognitive, and executive networks. These systems jointly determine how agents detect morally salient cues, generate evaluative appraisals, and select behaviour. The architecture is therefore inherently practical: the neural substrates implicated in moral judgment are also those responsible for valuation, behavioural control, and action selection.\footnote{This stands in contrast to folk-psychological depictions of moral judgment as passive contemplation of moral facts. Neuroscientific evidence overwhelmingly shows that moral cognition is organised around action guidance.} Contemporary research thus situates moral judgment within a distributed computational system whose governing question is not “What is right?” but “What should I do here?”~\cite{Bechara2000, Moll2002, Greene2014Beyond}.

\paragraph{Affective and Value-Based Systems.}
The ventromedial prefrontal cortex (vmPFC) and orbitofrontal cortex (OFC) compute affective and motivational value, integrating emotional information with anticipated outcomes. Lesions to vmPFC disrupt the incorporation of social and emotional consequences into decision-making, producing choices that appear normatively inappropriate or insensitive to harm~\cite{Bechara2000}. Functional imaging reveals vmPFC engagement during judgments involving interpersonal harm, care, and empathic concern \cite{Moll2002}. Together, these findings show that moral judgments depend on mechanisms that encode the valence of behavioural options.

The amygdala and anterior insula provide early affective tagging for morally salient stimuli~\cite{Garrigan2016, Eres2018, Fede2020}. The amygdala detects threat, intentional aggression, and aversive outcomes~\cite{LeDoux1998, Phelps2006}, while the anterior insula responds to disgust, norm violations, and aversive interoception~\cite{Moll2005, Sanfey2003, Chang2013}. Electrophysiological studies indicate that these affective signals often precede conscious deliberation~\cite{Sarlo2012, Luo2006}, functioning as rapid gating mechanisms for downstream moral appraisal.

\paragraph{Social-Cognitive and Interpretive Systems.}
Moral judgments frequently hinge on beliefs, intentions, and reasons~\cite{Mikhail2007, YoungSaxe2011}. The temporo-parietal junction (TPJ), medial prefrontal cortex (mPFC), and posterior superior temporal sulcus (pSTS) form a network specialised for mental-state attribution~\cite{Saxe2003, SaxeKanwisher2003, Pelphrey2004, VanOverwalle2009}. TPJ activation, for example, is reliably observed when distinguishing intentional from accidental harms or attributing blame or forgiveness~\cite{YoungSaxe2010, Cushman2013}. These systems ensure that moral cognition tracks reasons and intentions, not merely outcomes.

The anterior cingulate cortex (ACC) monitors conflict between competing evaluative signals~\cite{Botvinick2004, Shackman2011}. Classic moral dilemmas recruit ACC activity when intuitive emotional responses and reflective considerations collide~\cite{Greene2004, Decety2012}. This conflict-monitoring function indicates that moral cognition involves arbitration among multiple evaluative forces~\cite{Shenhav2013, Etkin2011}.

\paragraph{Executive and Action-Guidance Systems.}
The dorsolateral prefrontal cortex (dlPFC) supports controlled cognitive operations, including inhibition of affective impulses, representation of rules, and evaluation of long-term consequences~\cite{Miller2001, Koechlin2003}. Disruption of dlPFC activity via TMS alters willingness to endorse instrumental harm \cite{Tassy2012, Greene2004TMS}, demonstrating that this region contributes to structuring action policies that integrate affective, deontic, and goal-directed considerations \cite{Shenhav2017, Cushman2013ValueIntegration}.

Crucially, the dlPFC does not operate in isolation. Its interactions with vmPFC, ACC, and parietal regions reveal an integrated system in which valuation, social interpretation, and executive control jointly shape moral decisions~\cite{Hare2009, Mansouri2009, Bressler2010}. Recent accounts describe this network as computing action-guiding commitments rather than abstract evaluations~\cite{Shenhav2013ControlValue, Gintis2014MoralComputation}.

\noindent
This distributed architecture demonstrates a key claim that motivates the project: 

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{moral decision-making is inherently action-oriented and computationally grounded in mechanisms of valuation, salience, and behavioural control.}
	\end{leftbar}
\end{center}

\bigskip
\noindent

The experiment later introduced does not perturb beliefs, rules, or principles. It perturbs this action-guidance machinery—the very substrate through which moral salience becomes behaviour.

The neuroscientific evidence therefore provides the empirical foundation for the thesis’s central argument: a silent humanoid robot does not need beliefs or intentions to influence moral behaviour. Its ambiguous social presence modulates the affective, attentional, and interpretive systems that constitute the architecture of moral judgment.

This is why the neuroscience matters, and why it belongs here in the argument: it shows, at the biological level, that morality is a process of evaluative action selection, and therefore vulnerable to the kinds of perturbation artificial agents can introduce.

\paragraph{Functional Integration and Practical Orientation.}
Across these subsystems, a coherent picture emerges: moral cognition is not a contest between “emotion” and “reason,” but a dynamically integrated process in which affective valuation, social interpretation, and executive control jointly determine behaviour \cite{Haidt2001, Greene2014Integration, Cushman2013DualSystems}. This integration is fundamentally practical. The vmPFC and OFC compute the affective value of potential actions \cite{Ongur2000, Rangel2008}; the TPJ and mPFC generate intention-sensitive interpretations of agents’ behaviour \cite{SaxeKanwisher2003, YoungSaxe2010}; the ACC detects conflict between competing behavioural tendencies \cite{Botvinick2004, Shackman2011}; and the dlPFC regulates whether intuitive impulses should be suppressed, enacted, or balanced against normative constraints \cite{Miller2001, Tassy2012}. Even primary affective structures such as the amygdala and insula contribute to behavioural readiness by producing rapid somatic markers and prioritising morally relevant cues in the environment \cite{Phelps2006, Chang2013}.

Lesion studies, electrophysiological evidence, and neuroimaging findings converge on a single conclusion: moral judgment is an action-guidance mechanism operating under conditions of social meaning. On this view, moral cognition constitutes a form of evaluative control—a mapping from cue detection to practical commitment—rather than a detached assessment of abstract moral truths~\cite{Greene2014Beyond, Cushman2013Action}. This interpretation aligns with philosophical accounts emphasising the intrinsically action-directed nature of moral evaluation \cite{Anscombe1957, Korsgaard2009}, while grounding those commitments in empirical evidence about the neural architecture of agency, valuation, and control.

\section{From Moral Architecture to Perturbation by Synthetic Agents}

The integrated picture that emerges from cognitive neuroscience and psychology provides the conceptual bridge to the central phenomenon examined in this thesis. If moral judgment operates through distributed systems that compute \emph{salience}, \emph{affective weight}, and \emph{behavioural readiness}, then \textbf{moral behaviour can be perturbed without altering beliefs or principles}. A humanoid robot need not issue commands or express intentions to exert influence: by reshaping the affective and attentional substrates of moral appraisal, it can modulate the likelihood that moral perception culminates in prosocial action.

This follows directly from the practical orientation of the moral architecture described earlier. Moral cognition is not an abstract exercise in principle-identification; it is a mechanism for transforming perceptual and affective cues into behaviour. Any alteration to the social or perceptual environment---particularly one involving the presence of an entity with ambiguous social status---can shift the evaluative computations that guide action. Later chapters develop this claim empirically, showing how synthetic presence attenuates the behavioural expression of moral salience (see Hypothesis~\ref{hyp:synthetic_perturbation} in Chapter~\ref{chap:exp_methods}).

A humanoid robot is especially revealing as a perturbation. It is \emph{perceptually social} (in virtue of humanoid form), yet \emph{ontologically indeterminate} (neither fully agentic nor behaviourally irrelevant). Such indeterminacy can disrupt attentional allocation, dampen affective resonance, and introduce uncertainty in mind attribution. These upstream shifts alter the weighting, timing, and accessibility of evaluative signals. In short: \textbf{the robot changes the evaluative conditions under which moral appraisal becomes moral action}.

Understanding this architecture is therefore indispensable for interpreting the empirical findings. The experiment does not measure abstract moral judgments but the \emph{practical enactment} of moral cognition in an environment subtly transformed by synthetic presence. The neuroscientific foundations surveyed here provide the scaffolding for explaining how a silent observer can attenuate prosocial behaviour in stable, measurable ways.

A final conceptual step is required. If moral cognition is an architecture for transforming evaluative information into action, then \textbf{any alteration to the informational field is, in principle, a moral intervention}. A humanoid robot---an entity shaped like a person, yet not one---constitutes such an intervention. It does not supply new moral content; it \emph{reconfigures the conditions under which content becomes operative}. The moral landscape is therefore not defined only by principles or dispositions, but by the \emph{topology of the environment} in which they are enacted.

This insight has two consequences that structure the remainder of the thesis.

First, it shifts the explanatory centre of gravity: from conscious deliberation to the \emph{situated dynamics of evaluative processing}. The experiment asks how moral cognition functions when confronted with an entity whose social meaning is ambiguous.

Second, it reframes the normative question. The significance of artificial agents lies not merely in what they do, but in how their \emph{mere presence} modifies the normative affordances of a shared environment. Artificial agents reshape the moral field long before any explicit moral reasoning occurs.

In this way, the Moral Primer prepares two convergent lines of inquiry. The empirical chapters show how minimal synthetic presence modulates the behavioural expression of moral cognition. The normative chapters argue that this modulation exposes a structural oversight within classical Machine Ethics: the assumption that moral agency can be understood independently of the \emph{environmental scaffolds} that shape human evaluation.

These threads suggest a view of artificial agents not as moral subjects or mere tools, but as \emph{operators on moral space}---entities capable of bending the pathways through which moral meaning becomes action. The full implications of this perspective emerge only once the empirical and philosophical analyses are brought into dialogue. For now, it suffices to note that understanding moral decision-making under conditions of social and ontological ambiguity is not preparatory background; it is the \emph{conceptual linchpin} of the entire thesis.

This conceptual foundation also illuminates the methodological commitments that follow: the \emph{Level of Abstraction} at which moral cognition is analysed, and the \emph{topological structure} of evaluative processes under perturbation. An LoA, in Floridi’s sense, fixes the informational distinctions that matter for explanation. Here, our LoA does not concern the metaphysics of moral agency nor the justification of principles, but the \emph{functional transformation} of perceptual and affective cues into action-guiding evaluation. At this LoA, robots are not modelled as moral agents but as \emph{modulators of the evaluative field}.\footnote{On LoA as a methodological device for analysing informational systems, see Floridi 2010, 2011, 2013.}

Once this LoA is fixed, moral cognition can be modelled topologically: as a system mapping inputs to behavioural outputs through a structure shaped by salience, attention, affective resonance, and interpretive inference. Changing the environment---in this case by introducing a synthetic observer---can therefore be understood as a \emph{deformation} of the evaluative landscape. The experiment developed later investigates precisely such a deformation.

This topological perspective also clarifies why synthetic agents matter ethically even when behaviourally inert \cite{Coeckelbergh2010, Alfano2013MoralEnvironment}. At our operative LoA, the morally relevant property of a robot is its ability to \emph{warp attentional and affective gradients} that structure human appraisal \cite{Zlotowski2015SocialCues, BigmanGray2018}. A robot can function as a normative deflector or semantic attractor, subtly redistributing the vectors through which moral salience exerts its pull \cite{Alfano2013Trust, Railton2017Scaffolding}. Later empirical chapters document these redistributions; later normative chapters examine how they challenge Machine Ethics, which typically locates moral significance in the agent rather than the \emph{perturbation it induces} \cite{FloridiSanders2004, Coeckelbergh2020}.

Seen through this joint lens of LoA and moral topology, the empirical question at the heart of the thesis takes clear shape:

\bigskip
\noindent
\begin{center}
	\begin{leftbar}
		\textit{Does the presence of a synthetic agent reshape the evaluative field in which humans convert moral perception into prosocial action?}
	\end{leftbar}
\end{center}

\bigskip
\noindent

The formalism
\[
f: \Sigma \to \Delta, \qquad \mathcal{P}_{\mathscr{R}}:\Sigma\to\Sigma', \qquad f_{\mathscr{R}}=f\circ\mathcal{P}_{\mathscr{R}}
\]
offers a conceptual anchor---nothing more than a vocabulary---for expressing this claim: robotic presence functions as a perturbation operator on the evaluative field.

\subsection{Philosophical Synthesis}

This framework reframes perennial philosophical disputes. A Kantian model locates moral authority in rational principle; an Aristotelian model situates it in cultivated perception; a Humean model grounds it in sentiment and intuitive appraisal. The cognitive--affective architecture described earlier aligns most closely with the Humean--Aristotelian hybrid: moral judgment is rooted in \emph{evaluative sensitivity}, not detached rationality. When the social world is reconfigured---when its cues are displaced or reframed---the moral response shifts accordingly.

\subsection{Concluding Perspective: Why This Matters for the Thesis}

The preceding analysis converges on a single insight: \textbf{robots reshape the evaluative topology of moral life}, not by reasoning, nor by instructing, but by altering the perceptual--social gradients through which moral meaning becomes behaviour.

The experimental chapters test this claim; the normative chapters show why it challenges the foundational assumptions of Machine Ethics. What emerges is a technomoral thesis: as artificial agents permeate human environments, they will inevitably reshape the \emph{topology of moral experience}---subtly, silently, and often without intention. This is why synthetic presence matters. This is why the experiment matters. And this is why the conceptual groundwork laid in this chapter is essential for everything that follows.

The claim that artificial agents will reshape the \emph{topology of moral experience} may at first seem tailored to embodied, physically present robots. But its significance extends directly to the contemporary landscape dominated by large language models. As the earlier discussion of LLMs and the ``post--Machine Ethics'' era makes clear, modern AI systems no longer resemble the rule-based architectures that shaped the first wave of Machine Ethics. They operate through statistical patterning, implicit social modelling, and affectively charged conversational exchanges. They recalibrate attention, shape expectations, influence interpretation, and modulate interpersonal stance.

In other words, even without bodies, \textbf{LLMs are already perturbation operators on the evaluative field}. What varies is the channel of perturbation. Robots perturb \emph{perceptual} and \emph{embodied} salience. LLMs perturb \emph{semantic}, \emph{discursive}, and \emph{interpersonal} salience. Both influence the intuitive layer of moral cognition---the layer that precedes deliberation and shapes the evaluative landscape in which reasons and principles gain behavioural traction.

Seen from this perspective, the technomoral thesis is not limited to robotics. It is a general claim about how artificial systems---embodied or disembodied---reconfigure the cognitive--affective conditions under which human moral judgment unfolds. The role of this chapter is precisely to make this conceptual shift visible. Without a clear account of moral cognition as an \emph{action-guiding}, \emph{field-sensitive}, and \emph{LoA-dependent} architecture, discussions about LLM ``moral competence'' or ``machine virtue'' become methodologically ungrounded.

Classical Machine Ethics imagined that the moral significance of AI lay in the principles encoded into the machine.  
The present analysis shows that the real significance lies in the \emph{perturbations AI induces in us}.

Thus the technomoral thesis challenges Machine Ethics not because LLMs solve the old problems of rule-encoding, but because they demonstrate the irrelevance of those problems. If moral behaviour is shaped at the level of salience, affect, and social meaning, then the central question is no longer:

\begin{quote}
	\textit{``Can a machine follow an ethical principle?''}
\end{quote}

but rather:

\begin{quote}
	\textit{``How does the machine’s presence---physical, linguistic, or social---alter the evaluative field in which human agents form moral judgments?''}
\end{quote}

The role of this chapter, therefore, is foundational. It provides the cognitive, psychological, and philosophical machinery required to see why this reframing is necessary. Without the distinctions introduced here---between descriptive and normative domains, factual and moral judgment, intuitive and deliberative processing, and above all, between Levels of Abstraction---one could easily mistake the current success of LLMs at producing coherent moral-sounding text for evidence of genuine moral cognition.

The chapter prevents this mistake. It equips the reader with the conceptual discipline needed to interpret both robotic and linguistic systems not as moral agents in any substantive sense, but as \emph{environmental modifiers}: systems that reshape salience, meaning, and behaviour by transforming the evaluative topologies within which human moral cognition is enacted.

Thus the link back to the earlier discussion is straightforward: the technomoral thesis is the correct answer to the question of AI’s moral significance in the LLM era---not because machines have become moral, but because our \emph{moral environment} is being continuously reshaped by artificial systems whose influence operates beneath the threshold of reflective judgment.



%%THE END!