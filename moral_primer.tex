\chapter{Cognitive Affective Architecture of Moral Judgement}
\label{chap:moral_primer}
\thispagestyle{pprintTitle}

Moral phenomena begin before explicit deliberation and the experiment in Chapter~\ref{chap:exp_methods} captures a behavioural expression of this pre-reflective evaluative machinery. Moral behaviour is indeed the observable endpoint of a much earlier process: it does not begin with principles, arguments, or rules but in the quiet architecture through which we register what matters in a situation—what draws attention, what feels salient, and what acquires weight before any deliberate reasoning takes shape.

If this thesis asks whether the mere presence of a humanoid robot can alter moral behaviour, it must first offer an account of the evaluative machinery through which such behaviour is formed. Before anything else can be said about synthetic moral modulation, the central problem must be stated plainly. The experiment does not begin with behaviour,robots, or statistics; it begins with a conceptual uncertainty about the structure of moral cognition itself. If moral evaluation is shaped upstream—by attention, affect, salience, and social interpretation—then even a silent artificial body might, in principle, shift how those evaluative processes unfold. The project therefore opens with a single question, and everything that follows is an attempt to answer it:

\bigskip
\begin{center}
	\begin{leftbar}
		\textit{Can the presence of a synthetic, non-agentic entity reshape the cognitive--affective transformation through which morally salient cues become action?}
	\end{leftbar}
\end{center}
\bigskip

Providing an answer is, in truth, an immense task. Questions about how perception acquires moral significance, how emotions and intuitions guide judgement, and how evaluative meaning crystallises have been debated for centuries across Moral Psychology, Phenomenology, Ethics, and Cognitive Science. No single thesis can reproduce or resolve that tradition. The aim here is more circumscribed and methodologically precise: to identify, at the appropriate Level of Abstraction, the cognitive--affective mechanisms that are \emph{causally operative} in the phenomenon under investigation. What follows is therefore not a philosophical survey but a functional outline of the evaluative processes that operate upstream of deliberation and through which any perturbation—including synthetic presence—must pass in order to influence moral action.

Within this cognitive–affective frame, moral judgement emerges through a layered evaluative sequence: attention filters the scene, affective appraisals colour what is noticed, intuitive evaluations establish a sense of “how things stand,” and only then do reflective reasons enter the picture. These mechanisms belong to a Level of Abstraction beneath the familiar structures of moral theory. They do
not specify what agents \emph{ought} to do; they determine how moral meaning becomes behaviourally actionable in the first place.

Robotic presence, if it influences moral judgment at all, must do so here—within the processes that govern salience, resonance, and social interpretation. The experiment described later in this thesis examines precisely such an influence, but its interpretation depends on identifying the evaluative substrate through which moral cues acquire force. Without that substrate clearly articulated, the
phenomenon risks being misdescribed as a change in belief or a failure of reasoning, when it is instead a deformation of the field from which both arise.

\noindent
Reflective moral theories—Kantian maxims, utilitarian calculus, contractualist reasoning— are all ethical framewors that do not operate at this level. They articulate justificatory relations, not generative mechanisms. They tell us why an action may be defensible, not how the human cognitive system produces the behaviour in the first place~\cite{Korsgaard1996,Scanlon1998,SidgwickMethods}. Any attempt to explain the experimental effect by appealing to ethical principles therefore begins at a Level of Abstraction the phenomenon does not inhabit~\cite{Floridi2008,Floridi2011}.

What is required instead is an account of moral cognition as an action-guiding evaluative process: one in which affect, attention, salience, social interpretation, and contextual meaning jointly determine how moral cues acquire behavioural force. Although rationalist traditions in moral psychology treat judgment as the outcome of reflective reasoning, a large body of empirical work 
demonstrates that mechanisms such as affective appraisal, empathic resonance, intuitive evaluation, and attentional modulation constitute the causal substrate of moraljudgement~\cite{Haidt2001,Greene2001,Greene2004,Cushman2013,Decety2004,Crockett2016}. Only within such a framework can the influence of synthetic presence be meaningfully specified. Without it, the experimental result would risk being mischaracterised as a shift in belief or a failure of deliberation, rather than a perturbation of the evaluative field that precedes both~\cite{Conty2016,Nettle2013,Pfattheicher2015}.


The purpose of this chapter is therefore clarificatory in the strictest sense. It isolates the cognitive–affective mechanisms that constitute the causal substrate of moral behaviour; it distinguishes these mechanisms from the reflective structures of ethical theory; and it establishes the Level of Abstraction at which the central research question resides. Only on this foundation can the 
experiment be interpreted correctly. The study does not test principles, preferences, or doctrines. It probes the stability of the evaluative machinery through which moral meaning becomes action.

A recurrent theme across both the philosophical and empirical literature is that difficulties in Machine Ethics may arise from two related forms of misalignment: \emph{category errors} and \emph{LoA conflation}. Category errors occur when reflective normative principles are treated as if they described the psychological mechanisms that generate behaviour; LoA conflation arises when justificatory standards are interpreted as causal processes, or when descriptive regularities are taken to impose normative constraints~\cite{Floridi2008,Floridi2011}. These misalignments can obscure the possibility that moral behaviour is shaped at a lower, cognitive–affective LoA documented in Moral Psychology and Social Cognition~\cite{Haidt2001,Greene2004,Decety2004}. Without clarity about which LoA is operative, empirical patterns—such as the attenuation effects examined in Chapter~\ref{chap:exp_methods}—may be misread as failures of reasoning rather than as potential deformations in the evaluative conditions through which moral salience becomes action.


The remainder of the chapter therefore proceeds in a principled order. Before we can explain how synthetic presence perturbs the evaluative substrate of moral action, we must clarify the two domains that moral inquiry straddles: the descriptive domain concerned with how moral judgments are \emph{generated}, and 
the normative domain concerned with how actions can be \emph{justified}. Only by keeping these domains distinct can the phenomenon of synthetic moral perturbation be located with conceptual precision. Before turning to that distinction, it is worth pausing for a moment. Much of what follows is technical, but the motivation is simple: moral life is fragile in ways
our theories do not always capture. A small shift in how a scene is perceived can change what feels salient, what feels asked of us, and how we respond. It is this subtle architecture—the quiet machinery beneath judgement—that the next sections aim to bring into focus, one layer at a time.


\section{Descriptive and Normative Domains}
The term ``morality'' spans two analytically distinct domains, which must be kept
separate if the questions in this thesis are to be made precise. The first is \emph{descriptive morality}: the empirical study of how human beings in fact form moral judgments, experience moral emotions, and act in ways that carry normative significance.  This domain includes developmental accounts of the emergence of moral 
judgment \cite{Kohlberg1981}, social–intuitionist and dual-process models of evaluative response \cite{Haidt2007, DorisSep2020}, affective and cognitive neuroscience of empathy and valuation \cite{Moll2002, Decety2004}, and evolutionary explanations of cooperation and prosocial motivation \cite{Joyce2006,Tomasello2016}. These approaches seek to describe the mechanisms that generate moral behaviour. They aim to explain \emph{how} people come to judge, feel, or act, without making claims about whether those responses are justified.
 
The second domain is \emph{normative morality}: the philosophical study of how one \emph{ought} to act, and which moral principles one has most reason to accept. Here we find deontological theories that articulate constraints on action, consequentialist views that evaluate outcomes, contractualist accounts grounded in justifiability to others, and virtue-theoretic traditions emphasising character and cultivated dispositions~\cite{Hursthouse1999, Hooker2000, Anscombe1957, Korsgaard2009}. These theories do not describe psychological processes. They offer standards by which beliefs, emotions, and actions may be assessed as justified or unjustified.

Keeping these domains distinct is not a matter of terminology but of explanatory discipline. Descriptive theories trace the \emph{causal pathways} through which moral behaviour is generated—how salience is allocated, how affect is recruited, how evaluative weight takes shape before any explicit reasoning occurs. Normative
theories, by contrast, articulate the \emph{standards of justification} by which actions may be assessed or criticised.

These domains are distinct but interdependent. Descriptive accounts reveal how agents in fact perceive and respond to a situation, while normative theories rely on that psychological architecture to remain action-guiding rather than fictional. Likewise, empirical models of moral cognition acquire meaning partly through the normative vocabulary within which moral judgments are articulated, even as those normative frameworks must remain constrained by what human agents are psychologically capable of performing or understanding.

The phenomenon investigated in this thesis—whether synthetic presence can perturb the formation of moral judgments—belongs squarely to the descriptive domain. Yet it can only be understood with conceptual clarity when set against this normative background. Without the separation, one risks mistaking psychological mechanisms
for ethical reasons, or treating justificatory principles as predictors of behaviour. The analyses that follow rely on this distinction to interpret the experimental findings at the correct Level of Abstraction.






The descriptive–normative distinction is introduced here because it marks the final
conceptual boundary required before the thesis can turn to a precise account of
moral cognition. Without it, two confusions would repeatedly undermine the scientific
aims of the project.

First, moral terminology in technical disciplines is often used with unexamined
ambiguity. Terms such as \emph{obligation}, \emph{responsibility}, \emph{harm}, or
\emph{trust} circulate freely in HRI, AI ethics, and behavioural science, but their
usage slides—often imperceptibly—between describing how people in fact respond to
a situation and prescribing how they ought to. When these domains are not kept
distinct, conceptual instability follows: empirical results are mistaken for ethical
insights, and normative claims are misread as behavioural predictions.

Second, the research question of this thesis is strictly descriptive:

\bigskip
\begin{center}
	\begin{leftbar}
		\textit{Can synthetic presence alter the evaluative processes through which humans convert 
			moral perception into moral action?}
	\end{leftbar}
\end{center}

Answering this question requires working at the LoA where moral behaviour is
\emph{generated}, not where it is \emph{justified}. If this boundary is not drawn
explicitly, one risks treating behavioural attenuation as a moral failure or
misinterpreting reflective theories as if they were mechanistic explanations.

The descriptive–normative distinction therefore performs three indispensable
functions.

\begin{enumerate}
	\item It identifies the \textbf{level at which the thesis operates}.  
	The aim is not to determine what people \emph{should} do in the presence of robots,
	but to explain what \emph{does} happen within the cognitive–affective architecture
	when an artificial agent enters the evaluative field. Such phenomena demand
	descriptive tools: models of salience, affect, attention, and social meaning
	\cite{Haidt2001,Greene2004,Decety2004,Pfattheicher2015,Conty2016}.
	
	\item It prevents the \textbf{misinterpretation of empirical effects as moral judgments}.  
	If a robot’s presence reduces prosocial behaviour, this is a psychological
	modulation—not an ethical lapse. It indicates a perturbation in the evaluative
	machinery that gives moral cues their behavioural force
	\cite{Haley2005,Bateson2006,Dear2019,Kuchenbrandt2011,Malle2016}.
	
	\item It isolates the \textbf{causally relevant components of morality} for the
	experiment.  
	The mechanisms at stake—affective resonance, accountability salience,
	attentional modulation—belong entirely to descriptive cognition
	\cite{Decety2004,Crockett2016,Vinciarelli2009,Conty2016}. Normative theories
	explain justificatory standards \cite{Scanlon1998,Korsgaard1996,SidgwickMethods},
	but they do not generate behaviour. Keeping the domains apart secures the LoA at
	which the empirical phenomenon actually occurs \cite{Floridi2008,Floridi2011}.
\end{enumerate}

Finally, clarifying this boundary ensures that normative theory can re-enter the
discussion later—without conceptual conflation. When deontic invariants,
consequentialist gradients, virtue-theoretic attractors, or contractualist
equilibria appear in later chapters \cite{Greene2014,Foot2001,Hursthouse1999,Annas2011},
they will do so not as behavioural engines, but as structural constraints within the
evaluative topology. This reinterpretation is only coherent when the descriptive and
normative domains have been separated with care.

\medskip

\noindent
In sum, the distinction introduced here is not a philosophical detour but a
boundary condition. It specifies the domain in which the thesis makes its claims,
prevents methodological conflation, and secures the conceptual precision required
to study moral perturbation under synthetic presence. The orientation is now clear:
moral cognition is the object of analysis; normative theory provides the vocabulary
of justification; and coherence requires that these domains remain distinct.

The project therefore turns, at this point, to a minimal operational definition of
morality. This may feel abrupt, but its placement is deliberate. Having established
the conceptual boundaries of the inquiry, we now require a definition that is
precise enough to support empirical analysis yet modest enough to avoid the
commitments of substantive normative theory. What follows provides that definition,
and with it, the foundation on which the remainder of the thesis will build.


\subsection{Why Definitions Vary}

There is no single universally accepted definition of morality, and this plurality is neither accidental nor superficial. Different research programmes emphasise different elements of the moral domain. Cognitive approaches foreground the mechanisms by which agents form evaluative judgments \cite{Doris2010}; affective traditions emphasise the emotional systems that underpin moral concern \cite{Nussbaum2001}; rationalist accounts privilege normative reasoning \cite{Korsgaard2009}; social-scientific models attend to conventions and cultural norms \cite{Bicchieri2016}; evolutionary frameworks focus on the adaptive functions of cooperation and prosociality \cite{Joyce2006,Tomasello2016}.

Philosophical traditions likewise diverge in grounding morality in rationality, sentiment, virtue, utility, social contracts, or evolutionary pressures. Rationalist approaches trace their lineage to Kant’s account of morality as grounded in pure practical reason and universal maxims \cite{Kant1785,Kant1788}, later developed in pluralistic form by Ross \cite{Ross1930} and in contemporary constructivist terms by Korsgaard \cite{Korsgaard1996}. Sentimentalist traditions locate the foundations of morality in affective experience, drawing on the classical work of Hume and Smith \cite{Hume1739,Hume1751,Smith1759} and extended by modern accounts of emotion as evaluative perception \cite{Nussbaum2001,Churchland2011}. Virtue-ethical approaches interpret morality through character and practical wisdom, originating in Aristotle’s treatment of virtue as cultivated sensitivity to salience \cite{AristotleNE} and revitalised by Foot, MacIntyre, and Hursthouse \cite{Foot1978,MacIntyre1981,Hursthouse1999}. Consequentialist theories instead ground normativity in outcomes and aggregated welfare, from Bentham and Mill’s classical utilitarianism \cite{Bentham1789,Mill1861} to Sidgwick’s formal unification \cite{Sidgwick1874} and later developments by Singer and Parfit \cite{Singer1979,Parfit1984}. Contractualist and social-contract traditions ground morality in principles that no one could reasonably reject or that would be chosen under conditions of fairness, tracing back to Hobbes, Locke, and Rousseau \cite{Hobbes1651,Locke1689,Rousseau1762}, and given contemporary articulation by Rawls and Scanlon \cite{Rawls1971,Rawls1993,Scanlon1998}. Evolutionary approaches ground morality in the adaptive functions of cooperation, altruism, and shared intentionality, beginning with Darwin’s account of sympathy and moral sense \cite{Darwin1871}, and extended through foundational work by Trivers, Wilson, Boehm, and Tomasello \cite{Trivers1971,Wilson1975,Boehm2012,Tomasello2016}.


Computational treatments often inherit only one strand of this theoretical diversity. They tend to default to \textit{rule-based perspectives}, particularly those associated with rationalist and deontological traditions in moral philosophy. These include Kantian approaches, in which moral evaluation proceeds through the application of universal maxims and categorical imperatives~\cite{Kant1785,Kant1788}, and Rossian pluralism, which frames moral judgement as the resolution of prima facie duties through principled deliberation~\cite{Ross1930}. Classical utilitarian, decision-theoretic accounts also lend themselves to rule-like operationalisation insofar as they cast moral evaluation as the application of calculable decision procedures over outcomes~\cite{Mill1861,Sidgwick1874}. It is these rule-governed structures—deontic imperatives, duty taxonomies, and outcome-based decision rules—that have historically been adopted within Machine Ethics as if they captured the generative machinery of human moral cognition~\cite{Moor2006,Anderson2011,Bringsjord2006}. This adoption appears to reflect not claims about descriptive accuracy, but the structural convenience such models offer for computational implementation~\cite{Bringsjord2006,Anderson2011,Arkin2009,Wallach2008}. This structural convenience has, at times, encouraged an oversimplified picture of moral behaviour as though it were principally a matter of rule following. Such an interpretation can obscure the cognitive–affective processes through which moral judgements are formed, processes documented across moral psychology, cognitive neuroscience, and behavioural research~\cite{Haidt2001,Greene2001,Cushman2013,Decety2004}.

A central aim of this chapter is therefore to provide a corrective orientation: to articulate a framework informed by contemporary findings in Moral Psychology, Cognitive Science, and Social Signal Research~\cite{Pentland2007,Vinciarelli2009,Conty2016}. Such a framework offers a more appropriate basis for the empirical and conceptual analyses required by the research question developed in this thesis.


\subsection{Minimal Operational Definition for This Thesis}

The survey above does not deliver a single, unified theory of morality—nor could it. Instead, it reveals the depth and heterogeneity of the concept. Across philosophical, psychological, evolutionary, and computational traditions, the term “morality” designates different objects: rules, reasons, emotions, virtues,
practices, social contracts, adaptive strategies. What this overview provides, however, is the necessary backdrop for the next conceptual move. 

To study how synthetic presence perturbs moral behaviour, the thesis must shift from the broad question of \emph{what morality is} to the more specific and empirically tractable question of \emph{how moral evaluations are generated}.

This is where the notion of \emph{moral cognition} enters. The term does not replace the philosophical debates surveyed above, nor does it adjudicate between them. Rather, it identifies the cognitive–affective machinery through which moral concern becomes psychologically operative. If “morality” names a family of
normative and cultural frameworks, “moral cognition” names the set of processes that allow an agent to recognise normatively salient features, form evaluative judgments, and translate these into behavioural dispositions. It is at this level—
the level of perception, salience, affect, and intuitive appraisal—that the phenomenon of synthetic moral perturbation must be located.

With this shift of focus, the thesis can now adopt the minimal,
action-oriented definition required for empirical analysis. The definition is not intended to capture the philosophical richness of moral theory; it is intended to identify the causal substrate that the experiment must probe.

\bigskip
\begin{center}
	\begin{leftbar}
		\textit{Moral cognition is the evaluative process through which agents detect normatively salient features of a situation, generate judgments concerning permissible or obligatory actions, and select behaviour accordingly.}
	\end{leftbar}
\end{center}

Two clarifications follow immediately. First, this definition is deliberately \emph{non-substantive}: it does not commit the thesis to any particular normative theory. It specifies a cognitive role—detecting, evaluating, acting—rather than a
moral content. Second, the definition locates moral behaviour within an information-processing architecture rather than within a system of rules. This orientation is what allows the empirical work of the thesis to proceed. To understand how synthetic presence perturbs moral action, we must describe the mechanisms that convert perceptual and affective input into evaluative output.

The remainder of this chapter develops precisely that architecture. Having surveyed the diversity of moral theories and clarified the sense in which the thesis is concerned with \emph{moral cognition}, we can now turn to the cognitive--affective processes that make moral evaluation behaviourally operative.It is within this machinery—not within ethical doctrines—that the phenomenon of synthetic moral perturbation will be found.

This definition is intentionally modest. It avoids entanglement with substantive normative theories while isolating the components necessary for empirical investigation: evaluation, judgment, and action. It aligns with contemporary Moral Psychology, which treats moral cognition as the product of interacting
affective and cognitive mechanisms~\cite{Haidt2001,Greene2001,Cushman2013,Decety2004},
and it coheres with the theoretical scaffolding developed across the thesis: evaluative topology, Levels of Abstraction~\cite{Floridi2008,Floridi2011}, and the perturbative role of synthetic presence as documented in HRI and SSP
\cite{Malle2016,Bremner2022,Vinciarelli2009}.

Under this definition, moral cognition functions as a mapping from situational cues to action policies, shaped by dispositional traits
\cite{BaronCohen2003,Habashi2016} and by the attentional and affective structures of the evaluative field \cite{Conty2016,Pfattheicher2015}. It supplies the minimal conceptual anchor required to analyse how synthetic presence modulates the transformation from moral perception to moral action.

Before turning to the distinction between factual and normative judgments, it is worth making explicit what has been achieved in the preceding sections. Although the discussion has been conceptual, its role has been scientific rather than merely preparatory. Three contributions are central.

The clarificatory work carried out so far serves three related functions. First, it locates the phenomenon under investigation at the correct Level of Abstraction. The literature review shows that the effects of synthetic presence occur within the cognitive–affective processes that precede deliberation; identifying this level is not a matter of presentation but a substantive result. Only by situating the phenomenon upstream of explicit reasoning can the experimental attenuation be interpreted without normative distortion or speculative inference.

Second, clarifying the distinction between descriptive and normative domains eliminates a set of category errors that routinely distort empirical interpretation. The aim is not conceptual tidiness but methodological discipline. Without this distinction, prescriptive content is too easily imported into cognitive models, and
descriptive regularities too easily mistaken for ethical conclusions. Avoiding these forms of conflation is a necessary condition for generating reliable explanatory
claims.

Third, the chapter establishes a minimal and action-guiding definition of moral  cognition. By specifying the evaluative process that links situational cues to action  selection, it identifies the mechanisms that may legitimately be invoked—salience, 
affect, attention, social meaning—and excludes those belonging to the wrong Level  of Abstraction. This definition also provides the conceptual interface through which the empirical findings connect to the evaluative–topological framework developed later in the thesis.

Taken together, these contributions supply the explanatory architecture required to interpret the experimental results correctly and to explain how synthetic presence perturbs the evaluative field from which moral behaviour emerges.


Collectively, these achievements secure the conceptual foundations of the project. They define the explanandum, delimit the operative explanatory layer, and prevent Level-of-Abstraction conflation~\cite{Floridi2008,Floridi2011}. Only once this groundwork is established can the thesis introduce finer distinctions—such as that between factual and normative judgments—that refine the
architecture of moral cognition at the point where synthetic perturbation takes effect~\cite{Scanlon1998,Korsgaard1996}.

This is why the next section follows naturally. To understand how synthetic presence modulates moral behaviour, we must first understand \emph{which kind of judgment is being modulated}. Synthetic presence does not alter factual beliefs; it alters the evaluative force through which normative appraisals acquire
behavioural significance. Distinguishing factual from normative judgment is therefore not an ornamental philosophical exercise: it is the next analytic step in identifying the mechanism through which the evaluative field is reshaped~\cite{Cushman2013,Haidt2001,Greene2014}.


\section{Judgments: Factual and Normative}

A central distinction for analysing moral cognition—and for understanding the experimental phenomenon at the heart of this thesis—is the difference between factual and normative judgments. Although both concern evaluations of situations, they operate at distinct logical and functional levels. Factual judgments describe
states of affairs: they answer questions about what is the case. Normative judgments concern what ought to be done, what is \emph{permissible}, \emph{required}, or \emph{forbidden}. The distinction has a long and influential history in moral philosophy, yet it is frequently blurred in computational and psychological treatments of morality \cite{Black1972,Deigh2010}. Its importance
in the present context lies in the fact that:

\begin{center}
	\begin{leftbar}
		\textit{synthetic perturbation affects normative judgment, even though the factual perception of the situation might remains unchanged.}
	\end{leftbar}
\end{center}

Because the synthetic perturbation operates selectively on the normative layer, we must first clarify what distinguishes normative judgment from the factual input on which it depends. Only then can we specify the mechanism that is being modulated.

Factual judgments derive their correctness from empirical features of the world; their truth depends on observation or inference. Normative judgments embed reasons for action—they carry prescriptive force even when tacitly represented \cite{Hare1981, Korsgaard2009}. This is more than a semantic contrast. It marks a functional division within the cognitive architecture: judgments about what is engage classificatory and predictive systems, whereas judgments about what ought to be done recruit mechanisms that assign motivational weight, integrate affective cues, and generate the directional force that links evaluation to action.

This division maps directly onto the psychological conception of moral cognition, understood as the ensemble of perceptual, affective, and inferential processes that register morally salient features and transform them into evaluative representations \cite{Greene2001, Haidt2001}. Moral cognition includes explicit moral judgment as well as the upstream mechanisms that detect salience, encode social meaning, and initiate the transition from appraisal to behaviour \cite{Cushman2013, Young2012}. The descriptive–normative distinction is mirrored in these systems: factual information is processed by mechanisms specialised for representational accuracy, while normative appraisal engages systems that confer action-guiding significance~\cite{Moll2002, Greene2004, Shenhav2017}.

This division maps directly onto our operational definition of moral cognition. Under this definition, the cognitive system performs at least two analytically distinct functions. First, it forms \emph{factual judgments} about how the situation stands 
on the basis of perceptual and descriptive input. Second, it performs a further \emph{normative transformation}: it evaluates those facts in terms of what ought to be done, producing action-guiding judgments that structure behaviour. These two 
processes operate at different functional levels within moral cognition and must be kept distinct for the empirical questions of this thesis to be intelligible~\cite{Greene2001, Haidt2001, Moll2002, Cushman2013, Shenhav2017}.

This separation also clarifies the mechanism probed by the experiment. Synthetic presence does not alter what participants believe about the scenario. \textit{It alters how strongly normative force is experienced}. The attenuation effect is therefore not a change in factual judgment but a deformation of the evaluative dynamics that convert normative appraisal into action.

Recognising this prepares the ground for the next step. Once factual uptake and normative evaluation are disentangled, it becomes clear that moral judgment cannot be reduced to belief or emotion alone. It arises from the coordinated operation of perceptual, affective, inferential, and motivational systems that jointly confer normative authority and behavioural direction. It is this internal evaluative architecture—linking perception to action—that synthetic presence perturbs. To understand how such perturbation is possible, we now examine the structure of moral judgment itself.


\section{Internal Architecture of Moral Judgment}

Moral judgments are not mere expressions of preference or affective reaction. They exhibit a characteristic structure that combines evaluative content, justificatory grounding, and action-guiding force~\cite{Smith1994MoralProblem, Railton1986MoralRealism, Blackburn1998RulingPassions, Gibbard1990WiseChoices, Korsgaard1996Sources}. For the purposes of this thesis, a moral judgment involves at least three interlocking components:

\begin{enumerate}
	\item \textbf{Salience detection}: the recognition that a situation contains normatively relevant features—harm, fairness, honesty, obligation, care. This process draws upon perceptual, affective, and social-cognitive systems \cite{Moll2002, Decety2004}.
	\item \textbf{Evaluative appraisal}: the assessment of those features in light of internalised norms, dispositions, or reasons. This appraisal may be intuitive or reflective, emotionally charged or deliberative, depending on context and individual differences \cite{Nussbaum2001, Doris2010}.
	\item \textbf{Practical commitment}: the formation of an action-guiding stance, in which the judgment functions as a reason for or against a particular behaviour \cite{Anscombe1957, Korsgaard2009}.
\end{enumerate}

These components distinguish moral judgments from other evaluative acts—such as aesthetic impressions or strategic choices—and ground the thesis’s operational conception of moral cognition as an \textbf{evaluative mapping} from situational cues to action policies. They also clarify why synthetic perturbation can alter behaviour without altering factual beliefs: the perturbation targets the mechanisms that assign motivational weight, not the mechanisms that register empirical information.

This tripartite structure accommodates both intuitive and deliberative models of moral judgment. Intuitive processes typically dominate in everyday moral encounters; yet even when reasons are not explicitly articulated, these judgments retain justificatory form~\cite{Haidt2001, Greene2014Beyond, Railton2017MoralLearning, Mikhail2007UMG}. Conversely, deliberative processes involve explicit reasoning, counterfactual consideration, and appeals to principles or character traits \cite{Hursthouse1999}. This duality reflects not two kinds of morality, but two modes of access to the same evaluative architecture.

This distinction between intuitive and deliberative processes is not merely taxonomic; it initiates a deeper inquiry into the mechanisms that make moral judgment possible. To understand why certain stimuli reliably elicit prosocial behaviour (more on this point will follow in Chapter~\ref{chap:tools_new}) whereas others disrupt or attenuate it, we must examine the architecture through which moral salience is perceived, represented, and acted upon. The transition from perception to appraisal, and from appraisal to action, is mediated by identifiable affective, perceptual, and executive systems, each contributing distinct computational roles within the broader evaluative ecology.

As the next section shows, contemporary psychological and neuroscientific research converges on a model of moral cognition as a distributed, dynamically interactive network. This framework clarifies how humans ordinarily navigate morally charged environments and provides the conceptual foundation for understanding how these processes may be perturbed—subtly yet systematically—by the presence of agents whose social and ontological status is ambiguous, such as humanoid robots. In this sense, the empirical foundations surveyed below serve as the substrate upon which the subsequent experimental analysis is built.

Understanding the internal architecture of moral judgment is not an abstract philosophical exercise. It is a methodological necessity imposed by the research question and the experimental paradigm developed in later chapters. The phenomenon under investigation—the attenuation of prosocial behaviour in the presence of a silent humanoid robot—occurs precisely within the architecture just described. Without a clear account of this architecture, the empirical effect would be unintelligible or, worse, misinterpreted.

The experiment in Chapter~\ref{chap:exp_methods} demonstrates that the presence of a humanoid robot does not alter what participants believe about the situation. The factual content of the scenario remains stable. What changes is the normative force experienced in response to it: the directional pressure that transforms evaluative appraisal into action. Such a shift can only be understood if moral judgment is recognised as a composite process involving salience detection, affective appraisal, and practical commitment. The attenuation effect reveals a perturbation in one or more of these components—the curvature of the evaluative field—rather than any alteration in belief or principle.

This analysis also clarifies why the ontological ambiguity of the robot is central rather than incidental. The NAO robot used in the experiment possesses no beliefs, goals, or communicative intentions. Yet it is perceptually agentic: its morphology, gaze posture, and embodied presence activate social-cognitive mechanisms ordinarily reserved for human agents. This ambiguous status—more than an object, less than a person—positions the robot uniquely within the evaluative architecture. It can recruit salience-detection systems, modulate affective appraisal, or reshape perceived accountability without supplying any of the intentional content associated with genuine agency~\cite{Conty2016,Dear2019,Haley2005,Groom2010,Zlotowski2015,Malle2016,Coeckelbergh2020}.


In other words, the robot functions not as a locus of moral claims but as a perturbation operator acting on the substrate that generates moral judgment. Recognising this requires precisely the distinctions drawn in this chapter: between descriptive and normative domains, between factual and normative judgments, and between intuitive and deliberative processes. These distinctions allow us to see what the empirical effect is—a deformation of the evaluative field—and what it is not: a change in belief, a failure of reasoning, or an abandonment of moral principle.

For the reader who has progressed to this point in the thesis, the significance should now be clear. The conceptual machinery developed in this chapter is not preparatory ornamentation; it is the explanatory foundation upon which the entire project rests. The experiment measures subtle changes in prosocial behaviour, but the theoretical contribution lies in explaining why such changes occur and how artificial agents exert influence within the cognitive–affective ecology of moral judgment. Only with a precise account of the internal architecture can the thesis articulate, diagnose, and ultimately theorise the phenomenon of synthetic moral perturbation.

This is the point where philosophical analysis, cognitive science, and experimental design converge. And it is within this convergent space that the remainder of the thesis will operate.


\subsection{Psychological and Neuroscientific Foundations of Moral Decision-Making}
\label{sub:neuro}
Before moving further into technical terrain, it is worth pausing to recall the shape of the question guiding this chapter. If moral cognition is the process by which agents register what matters in a situation and translate it into action, then any adequate account must eventually touch the mechanisms that realise this
process in the mind and brain. The philosophical distinctions just developed prepare the ground; what follows identifies the cognitive architecture that makes those distinctions behaviourally meaningful.

A substantial body of cognitive neuroscience demonstrates that moral decision-making does not arise from a single “moral centre.” Instead, it emerges from coordinated activity across affective, social-cognitive, and executive networks. These systems jointly determine how agents detect morally salient cues, generate evaluative appraisals, and select behaviour. The architecture is therefore inherently practical: the neural substrates implicated in moral judgment are also those responsible for valuation, behavioural control, and action selection.\footnote{This stands in contrast to folk-psychological depictions of moral judgment as passive contemplation of moral facts. Neuroscientific evidence overwhelmingly shows that moral cognition is organised around action guidance.} Contemporary research thus situates moral judgment within a distributed computational system whose governing question is not “What is right?” but “What should I do here?”~\cite{Bechara2000, Moll2002, Greene2014Beyond}.

\paragraph{Affective and Value-Based Systems.}
The ventromedial prefrontal cortex (vmPFC) and orbitofrontal cortex (OFC) compute affective and motivational value, integrating emotional information with anticipated outcomes. Lesions to vmPFC disrupt the incorporation of social and emotional consequences into decision-making, producing choices that appear normatively inappropriate or insensitive to harm~\cite{Bechara2000}. Functional imaging reveals vmPFC engagement during judgments involving interpersonal harm, care, and empathic concern \cite{Moll2002}. Together, these findings show that moral judgments depend on mechanisms that encode the valence of behavioural options.

The amygdala and anterior insula provide early affective tagging for morally salient stimuli~\cite{Garrigan2016, Eres2018, Fede2020}. The amygdala detects threat, intentional aggression, and aversive outcomes~\cite{LeDoux1998, Phelps2006}, while the anterior insula responds to disgust, norm violations, and aversive interoception~\cite{Moll2005, Sanfey2003, Chang2013}. Electrophysiological studies indicate that these affective signals often precede conscious deliberation~\cite{Sarlo2012, Luo2006}, functioning as rapid gating mechanisms for downstream moral appraisal.

\paragraph{Social-Cognitive and Interpretive Systems.}
Moral judgments frequently hinge on beliefs, intentions, and reasons~\cite{Mikhail2007, YoungSaxe2011}. The temporo-parietal junction (TPJ), medial prefrontal cortex (mPFC), and posterior superior temporal sulcus (pSTS) form a network specialised for mental-state attribution~\cite{Saxe2003, SaxeKanwisher2003, Pelphrey2004, VanOverwalle2009}. TPJ activation, for example, is reliably observed when distinguishing intentional from accidental harms or attributing blame or forgiveness~\cite{YoungSaxe2010, Cushman2013}. These systems ensure that moral cognition tracks reasons and intentions, not merely outcomes.

The anterior cingulate cortex (ACC) monitors conflict between competing evaluative signals~\cite{Botvinick2004, Shackman2011}. Classic moral dilemmas recruit ACC activity when intuitive emotional responses and reflective considerations collide~\cite{Greene2004, Decety2012}. This conflict-monitoring function indicates that moral cognition involves arbitration among multiple evaluative forces~\cite{Shenhav2013, Etkin2011}.

\paragraph{Executive and Action-Guidance Systems.}
The dorsolateral prefrontal cortex (dlPFC) supports controlled cognitive operations, including inhibition of affective impulses, representation of rules, and evaluation of long-term consequences~\cite{Miller2001, Koechlin2003}. Disruption of dlPFC activity via TMS alters willingness to endorse instrumental harm \cite{Tassy2012, Greene2004TMS}, demonstrating that this region contributes to structuring action policies that integrate affective, deontic, and goal-directed considerations \cite{Shenhav2017, Cushman2013ValueIntegration}.

Crucially, the dlPFC does not operate in isolation. Its interactions with vmPFC, ACC, and parietal regions reveal an integrated system in which valuation, social interpretation, and executive control jointly shape moral decisions~\cite{Hare2009, Mansouri2009, Bressler2010}. Recent accounts describe this network as computing action-guiding commitments rather than abstract evaluations~\cite{Shenhav2013ControlValue, Gintis2014MoralComputation}.

\noindent
This distributed architecture demonstrates a key claim that motivates the project: 

\begin{center}
	\begin{leftbar}
		\textit{Moral decision-making is inherently action-oriented and computationally grounded in mechanisms of valuation, salience, and behavioural control.}
	\end{leftbar}
\end{center}

The experiment later introduced (Chapter~\ref{chap:exp_methods}) does not perturb beliefs, rules, or principles. It perturbs this action-guidance machinery—the very substrate through which moral salience becomes behaviour.

The neuroscientific evidence therefore provides the empirical foundation for the thesis’s central argument: 

\begin{center}
	\begin{leftbar}
		\textit{A silent humanoid robot does not need beliefs or intentions to influence moral behaviour.}
	\end{leftbar}
\end{center}

Its ambiguous social presence modulates the affective, attentional, and interpretive systems that constitute the architecture of moral judgment.

This is why the neuroscience matters, and why it belongs here in the argument: it shows, at the biological level, that morality is a process of evaluative action selection, and therefore vulnerable to the kinds of perturbation artificial agents can introduce.

\subsection{Functional Integration and Practical Orientation.}
Across these subsystems, a coherent picture emerges: moral cognition is not a contest between “emotion” and “reason,” but a dynamically integrated process in which affective valuation, social interpretation, and executive control jointly determine behaviour \cite{Haidt2001, Greene2014Integration, Cushman2013DualSystems}. This integration is fundamentally practical. The vmPFC and OFC compute the affective value of potential actions \cite{Ongur2000, Rangel2008}; the TPJ and mPFC generate intention-sensitive interpretations of agents’ behaviour \cite{SaxeKanwisher2003, YoungSaxe2010}; the ACC detects conflict between competing behavioural tendencies \cite{Botvinick2004, Shackman2011}; and the dlPFC regulates whether intuitive impulses should be suppressed, enacted, or balanced against normative constraints \cite{Miller2001, Tassy2012}. Even primary affective structures such as the amygdala and insula contribute to behavioural readiness by producing rapid somatic markers and prioritising morally relevant cues in the environment \cite{Phelps2006, Chang2013}.

Lesion studies, electrophysiological evidence, and neuroimaging findings converge on a single conclusion: moral judgment is an action-guidance mechanism operating under conditions of social meaning. On this view, moral cognition constitutes a form of evaluative control—a mapping from cue detection to practical commitment—rather than a detached assessment of abstract moral truths~\cite{Greene2014Beyond, Cushman2013Action}. This interpretation aligns with philosophical accounts emphasising the intrinsically action-directed nature of moral evaluation \cite{Anscombe1957, Korsgaard2009}, while grounding those commitments in empirical evidence about the neural architecture of agency, valuation, and control.

\section{From Moral Architecture to Perturbation by Synthetic Agents}

The integrated picture that emerges from cognitive neuroscience and psychology provides the conceptual bridge to the central phenomenon examined in this thesis. If moral judgment operates through distributed systems that compute \emph{salience}, \emph{affective weight}, and \emph{behavioural readiness}, then \textbf{moral behaviour can be perturbed without altering beliefs or principles}. A humanoid robot need not issue commands or express intentions to exert influence: by reshaping the affective and attentional substrates of moral appraisal, it can modulate the likelihood that moral perception culminates in prosocial action.

This follows directly from the practical orientation of the moral architecture described earlier. Moral cognition is not an abstract exercise in principle-identification; it is a mechanism for transforming perceptual and affective cues into behaviour. Any alteration to the social or perceptual environment---particularly one involving the presence of an entity with ambiguous social status---can shift the evaluative computations that guide action. Later chapters develop this claim empirically, showing how synthetic presence attenuates the behavioural expression of moral salience (see Hypothesis~\ref{hyp:synthetic_perturbation} in Chapter~\ref{chap:exp_methods}).

A humanoid robot is especially revealing as a perturbation. It is \emph{perceptually social} (in virtue of humanoid form), yet \emph{ontologically indeterminate} (neither fully agentic nor behaviourally irrelevant). Such indeterminacy can disrupt attentional allocation, dampen affective resonance, and introduce uncertainty in mind attribution. These upstream shifts alter the weighting, timing, and accessibility of evaluative signals. In short: \textbf{the robot changes the evaluative conditions under which moral appraisal becomes moral action}.

Understanding this architecture is therefore indispensable for interpreting the empirical findings. The experiment does not measure abstract moral judgments but the \emph{practical enactment} of moral cognition in an environment subtly transformed by synthetic presence. The neuroscientific foundations surveyed here provide the scaffolding for explaining how a silent observer can attenuate prosocial behaviour in stable, measurable ways.

A final conceptual step is required. If moral cognition is an architecture for transforming evaluative information into action, then \textbf{any alteration to the informational field is, in principle, a moral intervention}. A humanoid robot---an entity shaped like a person, yet not one---constitutes such an intervention. It does not supply new moral content; it \emph{reconfigures the conditions under which content becomes operative}. The moral landscape is therefore not defined only by principles or dispositions, but by the \emph{topology of the environment} in which they are enacted. This insight has two consequences that structure the remainder of the thesis.

First, it shifts the explanatory centre of gravity: from conscious deliberation to the \emph{situated dynamics of evaluative processing}. The experiment asks how moral cognition functions when confronted with an entity whose social meaning is ambiguous.

Second, it reframes the normative question. The significance of artificial agents lies not merely in what they do, but in how their \emph{mere presence} modifies the normative affordances of a shared environment. Artificial agents reshape the moral field long before any explicit moral reasoning occurs.

In this way, the chapter establishes the conceptual foundations on which the remainder of the thesis depends. The experimental analysis that follows examines how minimal synthetic presence alters the evaluative conditions under which moral behaviour is formed, while the theoretical chapters (chapters~\ref{chap:dis}, \ref{chap:ethics_s}, \ref{chap:general_discussion}, \ref{chap:conclusion}) articulate the structural 
consequences of this finding for our understanding of moral agency, normative theory, and the design of artificial systems. What unifies these strands is the recognition that moral action cannot be understood in abstraction from the cognitive–affective and social scaffolds that make it possible.

Seen through this lens, artificial agents are neither moral subjects nor passive tools. They function as \emph{operators on the evaluative field}: entities capable of shifting salience, displacing empathic resonance, and modulating the pathways through which moral meaning becomes behaviourally operative. The full significance 
of this perspective emerges only when the empirical and philosophical analyses are brought into alignment, but the core insight is already visible.

Understanding how moral behaviour arises under conditions of social and ontological ambiguity is not ancillary to the thesis; it is the \emph{conceptual linchpin} that renders the central research question intelligible. Only with this architecture in place can the influence of synthetic presence be explained with the clarity and precision the phenomenon requires.




This conceptual foundation also illuminates the methodological commitments that follow: the \emph{Level of Abstraction} at which moral cognition is analysed, and the \emph{topological structure} of evaluative processes under perturbation. A LoA, in Floridi’s sense, fixes the informational distinctions that matter for explanation. Here, our LoA does not concern the metaphysics of moral agency nor the justification of principles, but the \emph{functional transformation} of perceptual and affective cues into action-guiding evaluation. At this LoA, robots are not modelled as moral agents but as \emph{modulators of the evaluative field}.\footnote{On LoA as a methodological device for analysing informational systems, see Floridi 2010, 2011, 2013.}

Once this LoA is fixed, moral cognition can be modelled topologically: as a system mapping inputs to behavioural outputs through a structure shaped by salience, attention, affective resonance, and interpretive inference. Changing the environment---in this case by introducing a synthetic observer---can therefore be understood as a \emph{deformation} of the evaluative landscape. The experiment developed later investigates precisely such a deformation.

This topological perspective also helps to clarify why synthetic agents may exert moral relevance even when they remain behaviourally minimal~\cite{Coeckelbergh2010,Alfano2013MoralEnvironment}. At the operative LoA adopted here, the salient property of a robot is its potential to \emph{modulate attentional and affective gradients} that guide human appraisal~\cite{Zlotowski2015SocialCues,BigmanGray2018}. In this capacity, a robot can act as a normative deflector or semantic attractor, subtly reshaping the pathways through which moral salience gains behavioural traction~\cite{Alfano2013Trust,Railton2017Scaffolding}. Chapter~\ref{chap:exp_methods} examines these possible redistributions and illustrates how they invite a shift from locating moral significance solely in the artificial agent to considering the \emph{perturbations its presence may induce}~\cite{FloridiSanders2004,Coeckelbergh2020}.

Seen through this joint lens of LoA and evaluative topology, the empirical 
question at the heart of the thesis takes clear shape:

\begin{center}
	\begin{leftbar}
		\textit{Does the presence of a synthetic agent reshape the evaluative field in 
			which humans convert moral perception into prosocial action?}
	\end{leftbar}
\end{center}

\noindent
To state this more precisely, it is helpful to introduce a minimal formalism:

\[
f : \Sigma \to \Delta, \qquad 
\mathcal{P}_{\mathscr R} : \Sigma \to \Sigma', \qquad 
f_{\mathscr R} = f \circ \mathcal{P}_{\mathscr R}.
\]

\noindent
The symbols here name the components already implicit in the discussion:

\begin{itemize}
	\item $\Sigma$ is the \emph{evaluative input space}: the perceptual, 
	affective, and contextual cues available to the agent in a situation.
	
	\item $f : \Sigma \to \Delta$ is the \emph{baseline evaluative mapping}: the 
	cognitive--affective process by which those cues are transformed into 
	downstream moral responses (for example, a decision to donate).
	
	\item $\mathcal{P}_{\mathscr R} : \Sigma \to \Sigma'$ represents the 
	\emph{perturbation induced by the robot’s presence}: it deforms the 
	evaluative field by shifting which cues are salient and how they are 
	weighted.
	
	\item $f_{\mathscr R} = f \circ \mathcal{P}_{\mathscr R}$ is the 
	\emph{robot-conditioned evaluative mapping}: the overall transformation from 
	cues to action once the field has been perturbed by synthetic presence.
\end{itemize}

\noindent
Nothing stronger is claimed. This formalism serves as a conceptual anchor for 
the central thesis: the humanoid robot does not introduce new moral content or 
explicit reasons; it acts as a \emph{perturbation operator} on the evaluative 
field. The experiment asks whether $\mathcal{P}_{\mathscr R}$ is empirically 
detectable---that is, whether the silent presence of the robot measurably 
alters the mapping from moral perception to prosocial action.


\subsection{Philosophical Synthesis}

This framework reframes perennial philosophical disputes. A Kantian model locates moral authority in rational principle; an Aristotelian model situates it in cultivated perception; a Humean model grounds it in sentiment and intuitive appraisal. The cognitive--affective architecture described earlier aligns most closely with the Humean--Aristotelian hybrid: moral judgment is rooted in \emph{evaluative sensitivity}, not detached rationality. When the social world is reconfigured---when its cues are displaced or reframed---the moral response shifts accordingly.

\section{Concluding Perspective: Why This Matters for the Thesis}

The preceding analysis points toward a common insight: \textbf{robots may reshape the evaluative topology of moral life}, not by reasoning or instruction, but by modulating the perceptual–social gradients through which moral meaning acquires behavioural relevance. 

Chapter~\ref{chap:exp_methods} examines this possibility empirically, and the normative chapters explore how it intersects with, and in some cases complicates, the assumptions that underlie classical Machine Ethics. Taken together, these strands suggest a technomoral thesis: as artificial agents become embedded in human environments, their presence may contribute—often quietly and without intention—to shifts in the \emph{topology of moral experience}. This is the sense in which synthetic presence matters, and it is why the conceptual groundwork developed in this chapter is needed for what follows.

Although the claim that artificial agents may influence the \emph{topology of moral experience} is illustrated here through embodied robots, its scope extends to the contemporary landscape shaped by large language models. As the earlier discussion of LLMs and the ``post–Machine Ethics'' era indicates, modern AI systems differ markedly from the rule-based architectures that motivated earlier theoretical frameworks. They operate through statistical patterning, implicit social modelling, and affectively charged conversational dynamics; in doing so, they can recalibrate attention, shape expectations, and influence interpretive stance. In this broader context, the perturbational role attributed to synthetic presence becomes a general feature of artificial systems that participate in human moral environments.

In this sense, even without bodies, \textbf{LLMs can be understood as potential perturbation operators on the evaluative field}. What differs is the channel through which the perturbation arises. Robots tend to influence \emph{perceptual} and \emph{embodied} salience, whereas LLMs act through \emph{semantic}, \emph{discursive}, and \emph{interpersonal} forms of salience. Both may interact with the intuitive layer of moral cognition—the layer that precedes explicit deliberation and structures the evaluative background in which reasons and principles acquire behavioural significance.

Viewed from this perspective, the technomoral thesis is not confined to robotics. It is a broader claim about how artificial systems—whether embodied or disembodied—may reconfigure the cognitive–affective conditions under which human moral judgement unfolds. The task of this chapter is to render this conceptual shift explicit. Without a clear account of moral cognition as an \emph{action-guiding}, \emph{field-sensitive}, and \emph{LoA-dependent} architecture, discussions of LLM ``moral competence'' or ``machine virtue'' risk losing contact with the processes that actually shape moral behaviour.

Classical Machine Ethics tended to locate the moral significance of artificial systems in the principles encoded within them. The present analysis suggests that a more informative focus lies in the \emph{perturbations such systems may induce in us}.

In this light, the technomoral thesis challenges Machine Ethics not because LLMs resolve the traditional problems of rule encoding, but because they indicate that those problems may be peripheral to the dynamics that matter. If moral behaviour is shaped at the level of salience, affect, and social meaning, then the central question is no longer:

\begin{quote}
	\textit{``Can a machine follow an ethical principle?''}
\end{quote}

but rather:

\begin{quote}
	\textit{``How does the machine’s presence---physical, linguistic, or social---alter the evaluative field in which human agents form moral judgments?''}
\end{quote}

The role of this chapter is therefore foundational. It assembles the cognitive, psychological, and philosophical resources required to motivate the reframing that guides the thesis. The distinctions developed here—between descriptive and normative domains, between factual and moral judgement, between intuitive and deliberative processing, and the Levels of Abstraction that organise them—offer
the conceptual discipline needed to interpret the experiment without drift or overreach. Taken together, these distinctions suggest that synthetic systems matter not as bearers of reasons or values, but as \emph{environmental operators}: entities whose presence may modulate the patterns of salience, affect, and accountability through which moral cognition gains behavioural traction.

With these boundaries in place, the central phenomenon of the thesis becomes clearer. The attenuations in prosocial behaviour which we will observe under robotic co-presence in the experimental setting in Chapter~\ref{chap:exp_methods} is not readily
understood as a shift in belief or principle-application, but as a possible deformation of the evaluative field that precedes both. 

What follows therefore turns to this cognitive–affective substrate—the level at which synthetic perturbation is most plausibly understood to take effect.
