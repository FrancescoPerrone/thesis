\chapter{Abstract}

Moral behaviour emerges not from isolated cognitive modules or explicit reasoning, but from a structured evaluative field shaped by attention, affect, salience, and dispositional architecture. This thesis develops a field-theoretic account of moral cognition grounded in empirical data, formal topology, and the philosophy of information. It argues that artificial systems—particularly humanoid robots—interact with this evaluative field in ways that conventional, rule-based Machine Ethics fails to capture.

A controlled behavioural experiment tested whether the silent presence of a humanoid robot (NAO) modulates prosocial giving under a strong moral cue (the Watching-Eye paradigm). Bayesian estimation and distribution-sensitive regression models reveal a modest but directionally consistent attenuation: participants donated less in the robot’s presence despite identical moral affordances. Psychometric measures (EQ, SQ, BFI-10) were used to construct latent dispositional ecologies, which exhibited *heterogeneous* susceptibility to the perturbation. The Prosocial–Empathic cluster showed the strongest attenuation; the Analytical–Structured cluster showed little to none; the Emotionally Reactive cluster showed high variability. These findings confirm that synthetic presence interacts with evaluative topology rather than exerting a uniform behavioural effect.

Interpreted within the mapping $f(\alpha_E, \beta_C, \gamma_R)$ and read at the operative Level of Abstraction, the behavioural pattern is consistent with a structural account in which synthetic presence acts as a perturbation on the evaluative field. NAO’s embodied ambiguity appears to modulate intuitive appraisal by shifting attentional weighting and weakening the affective resonance ordinarily elicited by morally salient cues, thereby redistributing salience prior to explicit deliberation. The robot does not contribute moral content; rather, it modifies the perceptual and affective scaffolds through which such content becomes action-guiding.

The thesis proposes that artificial systems, even when devoid of agency or intent, can operate as modifiers of the moral environments in which human decisions unfold. The behavioural patterns observed here are consistent with the view that moral action is field-dependent, topologically structured, and responsive to synthetic co-presence. This perspective highlights a limitation of strictly top-down approaches in Machine Ethics and motivates an ecological orientation—one that emphasises the governance of the evaluative environments shaped through human–machine interaction. The work offers a methodological basis for a computational and topological investigation of moral cognition under conditions increasingly defined by artificial presence.
