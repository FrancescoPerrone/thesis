\chapter{Abstract}

This thesis investigates the cognitive, affective, and computational foundations of 
moral decision-making, with a particular focus on how the presence of synthetic agents 
perturbs the evaluative processes that underlie prosocial action. The work begins by 
clarifying the conceptual structure of morality, distinguishing between intuitive and 
deliberative forms of moral judgment, and framing moral cognition as an inherently 
action-oriented system: a distributed architecture that maps perceptual, affective, and 
interpretive cues onto behavioural commitments. Drawing on contemporary research in 
cognitive neuroscience, Social Signal Processing, and Affective Computing, the thesis 
argues that moral judgment emerges from the dynamic integration of fast affective 
appraisals and slower controlled processes, organised within a topologically structured 
evaluative field. This framework challenges monolithic, rule-based approaches in 
Machine Ethics and motivates a human-centric, discovery-oriented paradigm for 
understanding how artificial systems participate in morally relevant contexts.

Building on this theoretical foundation, the thesis examines whether and how the 
perceptual presence of a humanoid robot—silent, unprogrammed, and ontologically 
ambiguous—modulates prosocial behaviour in a controlled experimental setting. 
Participants were exposed either to a control environment or to an identical 
environment containing the robot, and their donation behaviour was unobtrusively 
measured. The results demonstrate a reliable attenuation of prosocial action in the 
robot condition: participants donated less frequently and in smaller amounts, an effect 
that persisted across frequentist, robust, and Bayesian analyses. Personality traits 
from the Big Five, as well as empathizing and systemizing scores, did not differ 
between groups and did not predict or moderate the donation effect, indicating that the 
attenuation arises not from dispositional differences but from a perturbation of the 
evaluative context itself. Latent dispositional structures revealed through PCA and 
clustering further supported this interpretation: while distinct psychometric profiles 
were identifiable, the robot’s influence cut across them, suggesting a broad modulation 
of intuitive affective pathways rather than trait-contingent effects.

Taken together, these findings offer empirical evidence that humanoid robotic presence 
reconfigures the intuitive and deliberative dynamics of moral cognition, altering 
salience, affective resonance, and behavioural readiness even in the absence of explicit 
interaction. The thesis concludes by arguing that such perturbation effects carry 
significant implications for the design of morally capable artificial agents, the 
deployment of robots in socially sensitive contexts, and the development of computational 
models that reflect the true structure of human moral cognition. Rather than treating 
morality as the encoding of rules or principles, this work advances a view of moral 
intelligence—biological or artificial—as a process grounded in the dynamic, 
action-guiding coordination of evaluative systems, responsive to the social and 
ontological contours of the environments in which agents act.
