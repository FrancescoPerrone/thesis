%\chapter{An Experimental Study of Moral Displacement: From Normative Hypothesis to Experimental Topology}
\chapter{Moral Displacement: An Experimental Investigation}
\thispagestyle{pprintTitle}


% Define a counter named 'question' that resets every time 'chapter' increments
\newcounter{question}[chapter]
% Define the format of the counter to be 'chapter_number.question_number'
\renewcommand{\thequestion}{\thechapter.\arabic{question}}

% Adjusting epigraph settings
\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushright}

% Setting the font and spacing for the epigraph
%\epigraph{\itshape \setstretch{1.2}But one thing is the thought, another thing is the deed, and another thing is the idea of the deed. The wheel of causality doth not roll between them.}{\small{Friedrich Nietzsche, \textit{Thus Spoke Zarathustra} (1883)}}
%
%

% Prefatory Remarks: 
\section{Conceptual Foundations of the Research Question}

This chapter begins with a precise question: \textit{can the silent presence of a humanoid robot alter the evaluative process that turns moral perception into action?} 

This question, while operationally simple, reaches beyond behavioural measurement. It engages the broader project of understanding moral behaviour not merely as an individual trait but as an inferential process that emerges from the perception and decoding of socially meaningful signals—\textbf{a process that can, in principle, be computationally modelled}. 

Within the domains of social signal processing and artificial intelligence, the transformation of subtle environmental cues into behavioural outputs is treated as a mapping from informational stimuli to structured responses~\cite{Vinciarelli2009}. By embedding a humanoid robot—ontologically ambiguous, semantically potent, yet behaviourally inert—into a morality-salient environment, this experiment asks whether such synthetic presences perturb not the content of deliberation, but the signal-to-inference architecture through which salience becomes action.

\begin{center}
	\begin{questionbox}[label={q:robot-agent}]{Inferential Displacement}
		Can the mere presence of a synthetic, non-agentic entity perturb the inferential transformation through which morally salient cues are converted into observable moral behaviour?
	\end{questionbox}
\end{center}

In other words, the question asks whether the mere fact of a robot’s presence—despite the absence of task-related communication or instruction—can alter the evaluative mechanism that translates moral perception into moral behaviour, operationalised here as prosocial giving.

In this experiment, moral action is instantiated through a measurable behavioural outcome: the voluntary donation of part of the participant’s monetary compensation to a children’s medical charity. The humanoid robot introduced into the experimental environment is not interactive in any directive or conversational sense, but neither is it inert. Operating in autonomous life mode, NAO exhibits subtle embodied motions—simulated breathing, minor postural adjustments, and head orientation shifts triggered only when participants establish eye contact. These micro-movements constitute precisely the minimal behavioural cues known to activate or modulate the Watching Eye effect, thereby rendering the robot a semantically potent, low-agency observer within the moral field. By examining whether the presence of such a humanoid robot systematically shifts donation behaviour, we test whether synthetic co-presence perturbs not the participants’ reflective moral reasoning, but the \textbf{conditions under which morally salient cues elicit prosocial action}.


In other terms, the inquiry asks whether the presence of a humanoid robot—endowed not with communicative capacity but with minimal yet perceptually salient behavioural affordances—can alter the evaluative pathway through which moral perception becomes moral behaviour, operationalised here as \textbf{prosocial giving}.


In this experiment, moral action is instantiated through a measurable behavioural outcome: the voluntary donation of part of the participant’s monetary compensation to a children’s medical charity. The inquiry therefore isolates \textit{presence} itself—specifically, synthetic presence—as an informational and epistemic variable. It examines whether introducing such a form into a morality-salient environment alters the \textbf{situational conditions under which moral action is produced}. Crucially, the experiment does not attempt to model or infer the internal structure of moral reasoning; rather, it observes how the resulting behavioural expression of moral decision-making shifts across environments that differ only in the presence or absence of this subtly animated robot. In this way, the design tests whether synthetic co-presence perturbs not the content of deliberation, but the \textbf{conditions under which morally salient cues become behaviourally actionable}.

\noindent
Framing the investigation as a \textit{question} (Question ~\ref{q:robot-agent} p.~\pageref{q:robot-agent}) rather than a hypothesis is deliberate. It preserves the conceptual openness required at this stage of the analysis, foregrounding inquiry over prediction. Within interdisciplinary research—spanning moral psychology, social signal processing, and human–robot interaction—prematurely imposing a directional hypothesis risks presupposing the very moral effects that the experiment is designed to probe. By articulating a guiding research question rather than an asserted claim, we allow the empirical structure of the data to shape the inferential trajectory rather than constraining it in advance. This is consistent with both the methodological caution urged in philosophy of science and the epistemic humility appropriate when dealing with morally charged, psychologically subtle, and technologically novel forms of social influence.

\noindent
Against this backdrop, the central inquiry of the study can be expressed with complete clarity: \textit{does the mere presence of a humanoid robot alter how human beings act when confronted with a morally relevant choice?} 

Put operationally, we ask whether individuals donate differently to a charitable cause when a robot quietly shares the room with them. The behaviour of interest—\textbf{prosocial giving}—is quantified directly as the amount of money voluntarily deposited into a charity box. The variable is simple in measurement but dense in interpretive significance: the coins themselves index the culmination of a moral appraisal process, the behavioural footprint of an evaluative transformation triggered under conditions of minimal social prompting.

\noindent
Yet the stakes of this question extend beyond monetary donation. What is under scrutiny is whether artificial companions—even in the absence of agency, speech, intention, or social engagement—can modulate the conditions under which morally salient cues are converted into human action. In this respect, the study examines not only how much participants give, but \textit{why} behaviour may shift under synthetic co-presence. The possibility being tested is subtle but far from trivial: that the introduction of an ontologically ambiguous entity into a moral environment may refract the participant’s evaluative landscape, thereby altering the behavioural expression of moral choice.

\section{Experimental Design and Behavioural Paradigm}

To investigate this Question ~\ref{q:robot-agent} (see p.~\pageref{q:robot-agent}), we implemented a controlled behavioural experiment~\cite{RosenthalRosnow2008, ReisJudd2000, Kazdin2017} derived from the classical \textit{Watching Eye} paradigm~\cite{HaleyFessler2005, Bateson2006, NettleEtAl2013, BatesonEtAl2013_EyesLittering, PfattheicherKeller2015, ContyGeorgeHietanen2016, DearDuttonFox2019}, in which prosocial behaviour is modulated by the perceived presence of observation. Seventy-three participants were invited individually into a room under the pretext of completing a battery of personality questionnaires in exchange for monetary compensation. Embedded in the experimental space was a morally salient cue: a charity brochure prominently featuring the photograph of a child requiring medical assistance. Decades of empirical work show that such stimuli reliably activate prosocial dispositions through mechanisms of implicit monitoring and empathetic engagement~\cite{Haley2005,Conty2016}.

\noindent
The robotic manipulation was then introduced as the sole experimental variable. In the control condition, participants completed the task alone. In the experimental condition, a humanoid robot—NAO~\cite{Aldebaran2013}—remained silently present in \textit{autonomous life mode}, exhibiting only the minimal embodied cues characteristic of that configuration: simulated breathing, subtle postural adjustments, and reactive head orientation triggered exclusively by eye contact. 

These micro-movements, though non-interactive and devoid of communicative intent, constitute precisely the class of minimal behavioural affordances shown to activate or modulate the mechanisms underpinning the Watching Eye effect. By embedding this low-agency, perceptually salient entity into an otherwise identical moral environment, the design isolates \textit{synthetic presence}—rather than dialogue, instruction, or overt agency—as the only \textit{manipulated} dimension of the setting. The personality questionnaires, administered under the pretext of a trait study, simultaneously serve as a cover story and as a structured measurement of individual cognitive–affective profiles. In subsequent analyses, these trait measures are treated as moderators, allowing us to ask whether any observed differences in prosocial donation behaviour arise from the robot’s presence alone, from stable individual dispositions, or—critically—from their interaction within a shared moral field.

\subsection{Why Minimal Presence Matters: Ontological Ambiguity as Experimental Variable}
Much of the literature on moral decision-making in human–robot interaction (HRI) and human–machine interaction (HMI) locates moral modulation in the interactive capacities of artificial agents. Studies routinely foreground expressive behaviour, ostensive cues, adaptive responsiveness, displays of accountability, or anthropomorphic signalling as the levers through which machines influence human judgment and behaviour~\cite{Malle2016,VanStraten2020,Arnold2017,Groom2010,Leidner2019}. These approaches implicitly assume that moral impact requires action: verbal behaviour, communicative intent, social reciprocity, or strategically framed moral cues.

\textit{The present experimental design intentionally refuses this assumption.}

Rather than examining how robots act, we examine how they exist—that is, how their mere ontological presence, stripped of communicative intent and devoid of interactive complexity, may nevertheless perturb the inferential transformation through which morally salient cues become behaviourally instantiated. The focus is not on moral agency or synthetic ethics, but on the structural susceptibility of human moral cognition to ontologically ambiguous stimuli.

This methodological divergence is conceptually foundational. It allows us to target an aspect of moral cognition that is often overlooked: its \textit{pre-reflective permeability} (for a similar use of the term refer to~\cite{Husserl1913, Zahavi2005, Gallagher2005, Bargh1994}) to agent-like cues even when those cues lack \textit{intentional content}~\cite{Brentano1874, Searle1983, Crane2001}. The question is not whether robots can engage in moral exchange, but whether their presence, by virtue of their bodily form and minimal behavioural affordances, reshapes the inferential scaffolding that mediates between perceiving a moral cue and acting upon it.

\noindent
This problem is particularly salient in domains such as Social Signal Processing and computational social cognition, where synthetic agents routinely evoke social and moral reactions that exceed the informational complexity of their behaviour~\cite{Vinciarelli2009,Bremner2022}. By removing dialogue, task-relevance, and overt interaction while maintaining the perceptual markers of potential agency (eyes, posture, orientation, micro-motion), the experiment isolates \textbf{presence itself} as the epistemic variable to be tested.

In this respect, the design probes a structural vulnerability of norm-sensitive cognition: the possibility that minimal cues—mere \textit{indications} of agenthood—may exert disproportionate influence on evaluative pathways. The robot is not required to speak, gesture, or respond; its semantic force lies in its ability to activate interpretive priors associated with observation, evaluation, and social monitoring.

This intuition resonates with the hyperactive intentional stance described by Guthrie~\cite{Guthrie1993}, Waytz et al.~\cite{Waytz2010}, and Dennett~\cite{Dennett1987}, according to which humans routinely over-ascribe agency in uncertain environments. By positioning the robot in the liminal space between objecthood and agenthood, the experiment isolates not action, but anticipation—the silent priors that precede full agentive recognition.

\noindent
The methodological focus on \textbf{mere presence} thus reflects a principled decision: it disentangles interactive contingencies from deeper, subpersonal cognitive mechanisms that structure moral evaluation. Unlike approaches that equate moral influence with dialogue or reciprocity, this design foregrounds the epistemic topology of moral salience—the latent structures of social attribution that shape inferential pathways prior to action, prior even to conscious appraisal.

Having established the necessity of minimal presence as an experimental variable, the next conceptual step is to formalise the framework that renders this presence epistemically potent. This is where Floridi’s Levels of Abstraction (LoA) become essential: they provide the philosophical infrastructure required to explain why \textit{an entity that does nothing}, and to which no moral status is attributed, may still distort the conditions under which moral cues become behaviourally actionable.

This motivates a transition, not from theory to application, but from conceptual architecture to \textbf{experimental justification}.

\subsection{Levels of Abstraction and the Design Logic of Minimal Robotic Presence}

\noindent
The decision to deploy a humanoid robot in silent autonomous life mode—exhibiting only simulated breathing, subtle postural adjustments, and eye-contact-contingent head orientation—is not a matter of convenience or technological limitation. It is a philosophical and methodological choice grounded in Floridi’s theory of \textit{Levels of Abstraction} (LoA)~\cite{Floridi2008,Floridi2010,Floridi2013}. To appreciate this decision, the core function of LoAs must be understood with conceptual precision.

An LoA specifies the informational interface through which an agent, system, or observer accesses and processes the world. It determines which distinctions are epistemically visible and which are systematically bracketed. LoAs are therefore not metaphysical: they make no assertions about the intrinsic ontology of entities. Rather, they are \textit{epistemic configurations}, selective filters that carve out what counts as relevant information.


\noindent
Applied to the present experiment, LoAs allow us to describe moral influence without relying on metaphysical accounts of robot agency. At the LoA operative for a participant alone in a room, moral relevance does not depend on the robot’s internal states but on its semantic affordances: its posture, its eyes, the symmetry of its body, the direction of its face, its quiet imitation of biological rhythms~\cite{Emery2000, Hietanen2002, CarneyCuddyYap2010, Argyle1975, Rhodes2006, Johansson1973, Saygin2012, ChaminadeOhnishi2007}.

These features are perceptually encoded as possible indicators of being watched~\cite{Emery2000, KleckStrenta1980, Hietanen1999, Hietanen2002, Argyle1975, MasonTatkinMacrae2005. Johansson1973_BiologicalMotion, Saygin2012_BiologicalMotionRobots, BatesonNettleRoberts2006_CuesWatched}, evaluated, or accompanied—precisely the conditions under which the Watching Eye effect operates. Thus, the robot’s moral relevance emerges not from consciousness, autonomy, or interactive capacity, but from its informational presentation within the participant’s operative LoA.

\noindent
This perspective enables a shift away from essentialist distinctions—agent versus non-agent, sentient versus non-sentient—toward a functional reading: what does the robot \textit{do} at the LoA of the observer? At this LoA, NAO’s subtle bodily cues instantiate the informational signatures of a putative observer, thereby modulating the epistemic background against which morally salient cues (such as the charity poster) are evaluated.

\noindent
The placement of the robot in autonomous life mode is therefore a purposeful calibration of informational affordances. If NAO were fully interactive, the LoA would shift, and the participant would be required to adopt an intentional stance grounded in dialogue, reciprocity, or social coordination. \textit{This would confound the experiment by introducing behavioural and communicative variables}. Conversely, if the robot were completely inert—akin to a mannequin—the LoA would strip away most agent-like affordances, nullifying the minimal conditions under which moral salience can be perturbed

\noindent
NAO therefore occupies a deliberate middle space: a synthetic presence endowed with minimal but meaningful cues, sufficient to activate the epistemic structures associated with potential observation but insufficient to produce interactive interpretation. In this capacity, NAO aligns with Floridi and Sanders’ notion of an \textit{artefactual moral agent}~\cite{FloridiSanders2004,Floridi2013}: a non-sentient entity whose moral relevance arises not from autonomy but from the role it plays within an informationally structured environment.

\fpcom{This is more a conclusion.}
\noindent
In short, Floridi’s LoA framework explains why a non-interactive, subtly animated robot is an epistemically potent variable. It provides the philosophical rationale for a design in which robotic presence functions as a \textbf{semantic perturbation} of the evaluative pathway from moral salience to moral action. Presence is not a passive attribute; it is an informational act.

\noindent
This reading supports both the minimalist structure of the experimental design and its philosophical depth. By rejecting behavioural or dialogic criteria for moral influence, and grounding the analysis in semantic encoding at the LoA of the observer, we avoid naïve assumptions about interaction as a prerequisite for moral modulation. Presence, when correctly encoded, can reframe what is morally visible—prior to deliberation, and independent of interaction.

\subsection{Experimental design and Preliminary Results}

To investigate Question~\ref{q:robot-agent}, we implemented a controlled behavioural experiment~\cite{RosenthalRosnow2008,ReisJudd2000,Kazdin2017} derived from the classical \textit{Watching Eye} paradigm~\cite{HaleyFessler2005,Bateson2006,NettleEtAl2013,BatesonEtAl2013_EyesLittering,PfattheicherKeller2015,ContyGeorgeHietanen2016,DearDuttonFox2019}, in which prosocial behaviour is modulated by implicit cues of observation. Each participant was invited individually into a room under the pretext of completing a personality-study session in exchange for monetary compensation. Unbeknownst to them, the experimental environment contained a morally salient stimulus: a charity brochure displaying the photograph of a child requiring medical care. Decades of empirical work demonstrate that such stimuli reliably trigger prosocial dispositions by activating implicit monitoring and empathetic engagement~\cite{Haley2005,Conty2016}.

\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/home/francesco/Desktop/research/appunti/images/robot.png}
		\caption{Experimental condition: robot present.}
		\label{fig:robot}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/home/francesco/Desktop/research/appunti/images/control.png}
		\caption{Control condition: robot absent.}
		\label{fig:control}
	\end{subfigure}
	\caption{Top-down view of experimental vs. control configurations. Both settings retain identical spatial and visual layouts, isolating the variable of robotic presence as the only ontological difference.}
	\label{fig:experimental-topology}
\end{figure}


\noindent
Participants were randomly assigned to one of two conditions. In the \textbf{Control} condition, they completed the questionnaires alone. In the \textbf{Robot} condition, a humanoid NAO robot was placed in the room and operated in autonomous life mode. Although NAO emitted no speech and performed no task-relevant actions, it displayed minimal embodied behaviours—simulated breathing, subtle postural adjustments, and head-orientation responses triggered only by eye contact. These micro-cues are the minimal behavioural affordances known to activate or modulate the Watching Eye effect.

\noindent
After completing the questionnaires, each participant received £10 in £1 coins as compensation and encountered a voluntary donation opportunity. An opaque charity box (Operation Smile) was positioned near the exit. Participants could donate any subset of the coins. The total donation served as the primary dependent measure of prosocial behaviour.

\noindent
Initial results revealed a robust directional pattern: participants in the Robot condition donated substantially less than those in the Control condition. Furthermore, no meaningful between-group differences were found in personality profiles (Empathizing Quotient~\cite{BaronCohenWheelwright2004_EmpathyQuotient}, Systemizing Quotient~\cite{BaronCohenRichlerBisaryaGurunathanWheelwright2003_SystemizingQuotient}, Big Five Inventory~\cite{JohnDonahueKentle1991_BigFiveInventory}), ruling out trait-based confounds and strengthening the inference that robotic presence itself modulated the evaluative pathway underlying prosocial action.


\subsection{From Behavioural Setup to Evaluative Structure}

\noindent
In moral philosophy, action is frequently treated as the terminus of deliberation~\cite{Aristotle_nicomachean,Korsgaard1996,Anscombe1957}. Yet the present study concerns not the deliberative endpoint but the evaluative transformation that precedes it: the internal process by which morally salient cues are converted into behavioural output~\cite{Nussbaum2001,Korsgaard2009}. The experimental design above provides the behavioural substrate; what remains is to articulate the evaluative architecture through which robotic presence might exert its influence.

\noindent
Our explanatory focus therefore remains firmly on moral action—here, instantiated as voluntary donation—while acknowledging that salience, cognition, and interpretive modulation contribute to the inferential scaffolding that produces such action. This framing connects the experiment to the philosophical traditions of practical reasoning and to the neurocognitive models explored in Chapter 2.

Our aim is not to probe abstract normativity, but to determine whether artificial presence perturbs the transformation from moral appraisal to observable donation—a behavioural manifestation of deliberative judgement.

\noindent
Empirically, the experiment transposes the Watching Eye paradigm into a minimal social environment co-inhabited by a humanoid robot. Prior variants of the paradigm have relied on stylised pictorial stimuli or supernatural primes~\cite{Bateson2006,Shariff2007}. Our design replaces these with an embodied artificial presence whose ontological ambiguity is semantically potent while remaining behaviourally minimal.

\noindent
To formalise the transformation under investigation, we treat moral action not as a fixed trait but as the output of a cognitive–affective function integrating environmental cues, individual traits, and contextual structure. In philosophical terms, this is the practical realisation of moral salience; in psychological terms, it is the integration of cue perception, affective readiness, and situational inference.

\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] \;\neq\; \mathbb{E}[f(\Sigma)]
\]

Where:

\begin{itemize}
	\item $\Sigma$ is the morality-salient perceptual field (e.g., the Watching Eye stimulus),
	\item $\mathscr{R}$ is the synthetic co-presence, realised here by NAO,
	\item $f(\cdot)$ is the evaluative transformation mapping perceptual input to moral behaviour,
	\item $\mathbb{E}[f(\cdot)]$ denotes the expected behavioural output (donation magnitude).
\end{itemize}

\noindent
Read aloud, this expresses the hypothesis that:

\textbf{The expected outcome of moral behaviour changes when a humanoid robot is present within the perceptual–moral environment.}

\nextdiv
\begin{center}
	\nextstatement
	\begin{hypobox}{Evaluative Deformation Hypothesis}
		The expected outcome of moral behaviour, as computed through the evaluative process \( f \), is altered when the robot is present within the perceptual-moral environment.
	\end{hypobox}
\end{center}

\noindent
The conceptual shift from the initial research question to this first formal hypothesis is thus warranted by the structure of the experimental design. The question preserved conceptual openness—\textit{is robotic presence morally perturbative?} The hypothesis now expresses this inquiry in a form amenable to empirical adjudication, specifying how the evaluative transformation from moral cue to moral action may be deformed.

%% first stop at station.

\noindent
To make the structure of this transformation explicit, we can decompose the probability of a deviation in moral action into its component determinants:

\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]

\noindent
where:
\begin{itemize}
	\item \(\alpha_E\) encodes the environmental moral cue (here, the Watching Eye stimulus),
	\item \(\beta_C\) denotes the individual-level control variables (psychometric and demographic structure),
	\item \(\gamma_R\) represents the robotic presence as a perturbative affordance.
\end{itemize}

This expression can be read aloud as: \textit{The probability of a deviation in moral decision $(\delta_m)$  is a function of the environmental moral cue $(\alpha_E)$, the individual's psychological and demographic configuration $(\beta_C)$, and the presence of the robot $(\gamma_R)$.}

\noindent
That is, the probability of observing a change in moral behaviour is a function of: (i) the morally salient stimulus, (ii) the participant’s internal traits, and (iii) the synthetic presence that may refract, displace, or attenuate the evaluative process.

\noindent
This formalism captures the operative logic of the experimental design: moral action is not treated as an isolated datum, but as a context-sensitive transformation of moral salience into behaviour. The robotic presence is therefore not conceptualised as a behavioural actor but as a \textit{topological perturbation}—a variable that reframes the inferential lens through which moral cues are registered and converted into action.

\noindent
To understand the stakes of this perturbation, we must clarify what is meant by \textit{moral salience}. Across philosophical and psychological literatures, moral salience refers to the capacity of a situation, object, or agent to present itself as morally significant—i.e., to become an object of evaluative attention prior to explicit deliberation~\cite{Korsgaard2009,Nussbaum2001,Greene2001,Haidt2007,Moll2005}. It functions as a phenomenological filter: before the agent reasons, before the agent chooses, certain features of the environment appear as normatively charged. Within this framework, synthetic entities may perturb moral salience not by issuing commands or engaging in dialogue, but by reconfiguring what is foregrounded, what is suppressed, and what is affectively or normatively “seen” in the first place.

\noindent
This brings us to the ontological dimension of the hypothesis. The robot’s influence depends not on its computational sophistication but on its \textit{perceived ontology}: how observers intuitively classify the entity—as object, tool, quasi-agent, or socially charged companion. In this experiment, NAO’s embodied form, posture, gaze behaviours, and subtle animations evoke agent-like expectations without satisfying the criteria for full moral agency. This ambiguity is precisely what renders the robot a semantically potent perturbator within the moral field.

\begin{center}
	\nextstatement
	\begin{hypobox}{Synthetic Normativity of Moral Displacement}
		Synthetic presences, though devoid of sentience, may acquire \textit{normative affordances} by virtue of their perceived ontology. When situated within morality-salient environments, such presences may disrupt, refract, or displace the evaluative machinery through which moral judgments are ordinarily formed.
	\end{hypobox}
\end{center}

\noindent
This hypothesis extends beyond a narrow behavioural prediction; it asserts that robotic presence may alter the normative topology of the environment itself. The experiment is therefore not merely a test of prosocial output, but a constrained act of epistemic staging—a designed moral topology intended to probe whether the presence of $\mathscr{R}$ displaces or refracts the normative force of $\alpha_E$.

\noindent
The Watching Eye paradigm thereby becomes a conceptual instrument: not merely a psychological effect but a method for examining the structural elasticity of normative cognition in environments where human agents coexist with synthetic forms. What the study observes, therefore, is not simply differences in donation behaviour, but how the inferential architecture linking salience to action is modulated by synthetic co-presence. Generosity, in this framework, is not a trait but an emergent property of norm-sensitive evaluative systems embedded within a structured environment.

\noindent
This framing rejects simplified accounts that treat moral behaviour as transparent readouts of internal disposition. Instead, it positions moral action as the contingent result of cognitive–affective systems operating under topological deformation~\cite{Greene2002,Haidt2001,Fedyk2017}. Robotic presence, by virtue of its ontological ambiguity, functions as a refractive moral affordance: a structural condition that may attenuate or redirect the transformation of moral salience into action.

\fpcom{old content begins}

The term \textit{perceived ontology} refers to how observers intuitively classify an entity’s nature—whether as object, tool, agent, or something more ambiguous. In this context, it denotes how the humanoid robot is not treated merely as a machine, but as a presence with quasi-social or normatively loaded features. This perception does not require the attribution of full agency or sentience; rather, it is the robot’s embodied form, gaze behaviours, and passive co-presence that evoke moral expectations in the observer. Thus, the robot’s “perceived ontology” may perturb how moral salience is registered, filtered, or even displaced by human evaluative systems.

\fpcom{old content ends}

This is not an experiment in the narrow sense of causal testing. It is a constrained act of epistemic staging—a designed \textbf{moral topology} that probes whether the presence of \(\mathscr{R}\) displaces, diffuses, or refracts the normative force of \(\alpha_E\). Our aim is not simply to determine whether donations changes under robotic observation, but whether \(\mathscr{R}\) alters the internal topology of moral inference itself. In this light, the Watching Eye paradigm ceases to be a psychological curiosity and becomes an instrument of conceptual inquiry: a way of testing the structural elasticity of normative cognition in post-human social configurations.

What this study observes, therefore, is not simply what participants do under (staged) robotic observation, but how the inferential architecture of moral cognition is perturbed by synthetic presence. The robot, though devoid of agency, functions as a semiotic operator on the moral field—its presence refracts the salience of otherwise normative cues, modulating prosocial output through shifts in interpretive topology. We do not treat generosity as a readout of innate disposition, but as the \textit{emergent property of norm-sensitive evaluative systems embedded in structured environments}.

This framing \textbf{rejects} any simplistic account of moral behaviour as noise-free reflection of trait. Instead, we position moral action as the contingent result of \textit{cognitive-affective systems }operating under \textit{topological deformation}~\cite{Greene2002, Haidt2001, Fedyk2017}. In this view, robotic presence is not merely a contextual feature, but a morally refractive affordance that alters the mapping between cue and action.

Within this epistemological architecture, the following experiment tests the plausibility of a central hypothesis: that robotic presence—by virtue of its ontological ambiguity—can systematically attenuate the conversion of moral salience (see above for a definition) into action. It is this structured possibility, not merely behaviour, that the empirical sections to follow are designed to investigate.

\noindent
With this architecture in place, the subsequent sections examine how such deformation manifests empirically—first at the behavioural level, and then at the deeper structural level of trait–context interactions.

\section{Conceptualisation of the Experiment: Moral Decision-Making under Synthetic Presence}

Having articulated the evaluative architecture through which synthetic presence may perturb the transformation from moral salience to action, we now specify how this theoretical framework is instantiated empirically. The objective of this section is not merely to describe procedural steps, but to clarify the conceptual rationale that makes this experimental configuration an appropriate test of the inferential deformation thesis established above.

\noindent
To empirically examine whether the mere presence of a synthetic, non-agentic entity can alter the evaluative pathway underlying charitable behaviour, we embedded participants within a controlled, minimally structured moral choice scenario. Framed as a standard personality study, the procedure unobtrusively positioned each participant before an unannounced ethical decision. This preserved the epistemic opacity required for observing pre-reflective evaluative processes rather than self-presentational behaviour.

Each participant entered the experimental room alone and completed a series of psychometric measures—the Empathizing Quotient~\cite{Baron2002}, Systemizing Quotient~\cite{Baron2003}, and the Big Five Inventory~\cite{Rammstedt2007}. Completion of the questionnaires served a dual methodological purpose. First, it provided data for assessing whether trait dispositions modulated sensitivity to robotic presence, thereby enabling the analysis of trait–context interactions central to this chapter. Second, it supplied a plausible pretext for the experimental setting, ensuring that participants approached the environment without anticipating a moral evaluation.

Upon completion, participants received £10 in £1 coins as compensation. Before exiting, they encountered a latent moral choice: an opaque green charity box placed beside a prominently displayed image of a child requiring medical care. The image served as a Watching Eye stimulus—an established elicitor of prosocial behaviour through mechanisms of implicit monitoring, empathy, and reputation sensitivity~\cite{Haley2005,Conty2016}. Donation behaviour, performed privately and unobserved, constituted the operational measure of moral action.


\noindent
The central experimental manipulation divided participants into two conditions:

\begin{itemize}
	\setlength{\itemsep}{4pt}
	\item \textbf{Control Condition}: participants completed the task alone in a room containing only the morally salient cue.
	\item \textbf{Robot Condition}: the same room contained a humanoid NAO robot (Aldebaran Robotics) in ``autonomous life mode.'' Although behaviourally non-interactive, the robot exhibited minimal embodied cues—simulated breathing, micro-adjustments in posture, and reactive head orientation triggered exclusively by eye contact. These cues provided precisely the level of embodied salience required to activate or disrupt the cognitive mechanisms underlying the Watching Eye effect.
\end{itemize}

This configuration preserved identical spatial, visual, and procedural features across conditions; the sole ontological difference was the presence of the robot.

\noindent
In the experimental condition, a humanoid robot (NAO) was silently positioned in the space, operating in “autonomous life mode”: breathing rhythmically, subtly shifting posture, and responding to eye contact through reactive head movement — yet without speaking, interacting, or engaging in any directive behaviour. Importantly, participants had no prior knowledge of the robot’s presence, and the robot itself did not intervene in the task.

Importantly, participants were not warned about the robot in advance, and no verbal or task-relevant interaction occurred at any time. The robot therefore functioned as an \textit{epistemic perturbation}: a synthetic presence whose embodied form was salient yet behaviourally inert, occupying the ambiguous space between animate agent and object.

\noindent
The behavioural outcome was striking: participants in the Robot condition donated substantially less (mean £1.17) than participants in the Control condition (mean £1.89). No significant differences in personality profiles were observed between groups, ruling out trait imbalance and indicating that the observed attenuation of donation reflects a genuine displacement in the evaluative pathway rather than a sampling artefact. At a descriptive level, then, synthetic co-presence appears to weaken the moral force of the Watching Eye stimulus.


\vspace{1em}
\noindent
To understand why this effect is theoretically significant, we must clarify the status of \textit{moral decision-making} within this experimental architecture. Contrary to utilitarian models that construe donation as a form of preference optimisation (see \cref{chap:ethics_s}), our framing treats the decision to donate as an instantiation of \textit{moral salience attribution under epistemic opacity}. Participants do not know they are being observed; they do not know that donation behaviour is the dependent measure; and they do not know that synthetic presence is the variable of interest. What is revealed, therefore, is not explicit moral reasoning, but the \textit{implicit evaluative machinery} through which morally loaded cues gain—or fail to gain—behavioural traction.

The Watching Eye stimulus plays a critical role in this machinery. Anthropological and psychological research shows that images of eyes or children reliably elicit third-party moral concern via affective engagement and implicit audience effects~\cite{Bateson2006,NettleEtAl2013,ContyGeorgeHietanen2016}. Our design extends this paradigm by placing, alongside the Watching Eye cue, a humanoid robot whose ontological status is neither human nor ethically inert. NAO thus becomes an \textit{ontological anomalous agent}: a presence that possesses the perceptual affordances of agenthood without the behavioural or normative commitments of actual agency.

This motivates the following hypothesis, which articulates the expected deformation within the evaluative architecture:


\nextdiv
\begin{center}
	\nextstatement
	\begin{hypobox}{Synthetic Perturbation of Moral Inference}
	\label{hyp:synthetic_perturbation}
	The humanoid robot NAO does not function as a passive observer, but as a perturbative presence that refracts the transition from moral salience to prosocial action. Its ontological ambiguity displaces the affective-empathic cues that ordinarily support donation, thereby modulating the evaluative pathway by which moral stimuli gain behavioural expression.
	\end{hypobox}
\end{center}

\[
\mathscr{S} : \Sigma \xrightarrow{\;\mathscr{R}\;} \mathscr{D}
\]

where:
\begin{itemize}
	\item $\Sigma$ denotes the perceptual input space structured by morally salient cues (brochure, child’s eyes, spatial configuration),
	\item $\mathscr{R}$ denotes the synthetic robotic presence functioning as a perturbative modulator,
	\item $\mathscr{D}$ denotes the domain of observable moral decisions (monetary donation).
\end{itemize}

In control conditions, the transition $\Sigma \rightarrow \mathscr{D}$ proceeds without interference: the affective weight of moral cues is preserved and expressed through prosocial giving~\cite{Haley2005,Shariff2007}. In robotic conditions, by contrast, $\mathscr{R}$ deforms this mapping. It may displace empathic identification, dilute the salience of the Watching Eye cue, reshape the normative topology of the environment, or function as a cognitive decoy~\cite{Zlotowski2015}. Each interpretation bears distinct implications for the design of ethical robots and for understanding how humans recalibrate moral behaviour in the presence of synthetic others.

%%% NEW UPADATED CONENTE  N1
\subsection{Formalisation of Hypothesis and Experimental Logic}

The present experiment is best conceived not as a mechanistic probe into behavioral preferences, but as a structured perturbation within a normatively encoded cognitive system. Specifically, it seeks to investigate \textbf{how robotic presence modulates human moral decision-making} under conditions of minimal priming and perceptual constraint. Unlike traditional paradigms that treat prosociality as an output of deliberative utility calculus, the design employed here foregrounds the \textbf{pre-reflective inferential machinery} that converts perceptual-affective cues into morally salient behavior.

At its epistemic core, this experiment operates as a \textbf{perturbative test of moral salience transmission} — that is, whether a morally charged perceptual cue (e.g., the face of a child in need) is successfully converted into a prosocial behavioral output (monetary donation), and how that transmission is modulated, disrupted, or reframed by the passive presence of a \textbf{non-agentic but anthropomorphically encoded entity} (\ie, the NAO robot).

To formalize the interpretive structure of this transformation, let us denote:

\begin{itemize}
	\item $\Sigma$: the perceptual-affective input space (including the Watching Eye stimulus, spatial layout, and ambient cues)
	\item $\mathscr{R}$: robotic presence, ontologically positioned between artifact and agent
	\item $\mathscr{D}$: the moral decision space (observable as donation behavior)
\end{itemize}

The operative hypothesis can be expressed as a probabilistic modulation of expected moral output:

\[
\mathscr{R} \notin \Sigma \Rightarrow \mathbb{E}[f(\Sigma)] = D_{\text{prosocial}} \quad \text{(Control condition)}
\]
\[
\mathscr{R} \in \Sigma \Rightarrow \mathbb{E}[f(\Sigma \cup \mathscr{R})] = D_{\text{attenuated}}
\]
where:

\[D_{\text{attenuated}} < D_{\text{prosocial}} \quad \text{(Robot condition)}\]

Here, the notation $\mathbb{E}[f(\cdot)]$ denotes the \textbf{expected behavioral output} of the cognitive-affective system under a given set of environmental conditions. The function $f(\cdot)$ captures the internal inferential transformation by which perceptual-affective cues—such as the Watching Eye stimulus—are mapped onto discrete moral actions, in this case, the act of anonymous donation. Crucially, the expectation operator $\mathbb{E}[\cdot]$ signals that we are not describing a deterministic relation, but rather the \textit{aggregate tendency} across a psychologically heterogeneous population. It reflects the statistical structure of the behavioral response field rather than individual-level causality.

\subsection{Methodological Design: Inferring Moral Perturbation through Controlled Artificial Co-presence}

To regard an experimental setting as a generator of knowledge, rather than a mere data collection routine, demands that its internal architecture be epistemically justifiable and ontologically transparent. In this respect, every stage of the experimental method presented here is conceived not simply as procedural necessity, but as epistemic filtering: a sequence of deliberate constraints designed to isolate latent variables within the perceptual and normative landscape of the participant.

At its core, the experimental logic operationalises the following proposition:

% Corpo del documento
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]

where:
\begin{itemize}
	\item $\delta_m$ denotes a deviation in moral decision (quantified as donation behavior),
	\item $\alpha_E$ represents environmental moral cues (Watching Eye),
	\item $\beta_C$ indexes control factors (psychometric variables, demographic traits),
	\item and $\gamma_R$ captures the effect of robotic presence.
\end{itemize}

The experimental setting is thus a structured interrogation of whether $\gamma_R \neq 0$ under conditions in which $\alpha_E$ and $\beta_C$ are held constant or accounted for. If confirmed, such deviation would instantiate a moral displacement: a case in which a non-sentient co-agent modulates human ethical output without any explicit instruction, coercion, or intervention. \fpincom{add link to relevant hypothesis and check condition "not zero"}

The following experimental procedure was implemented to ensure maximal control over environmental affordances while preserving participant naivety concerning the true moral dimension under investigation.

%%% NEW CONTENT N6
\subsection{Formalisation of the Experimental Logic}

Having established the conceptual and epistemic rationale for investigating robotic co-presence as a perturbative variable, we now formalise the internal logic of the experimental design. The present experiment is not conceived as a mechanistic probe into stable behavioural preferences, but as a \textit{structured perturbation} applied to a normatively encoded cognitive system. Its aim is to examine how a minimally interactive synthetic entity modulates the evaluative transformation through which morally salient cues become behaviourally instantiated.

\noindent

Unlike paradigms that construe prosociality as the downstream product of deliberative utility calculus, our design foregrounds the \textbf{pre-reflective inferential machinery} responsible for converting perceptual–affective moral cues into action. In this frame, moral behaviour is not treated as a direct expression of preference or disposition, but as the output of a cognitive–affective transformation whose parameters may be refracted by the presence of an ontologically ambiguous entity.

\noindent
At its epistemic core, the experiment operates as a \textbf{perturbative test of moral salience transmission}: whether the moral charge embedded in a Watching Eye stimulus is preserved, attenuated, or reframed when a synthetic presence occupies the same perceptual field. The robot deployed in this study—non-agentic, behaviourally minimal, but anthropomorphically encoded—functions precisely as such a perturbative variable.

\noindent
To make this structure explicit, let us denote:

\begin{itemize}
	\item $\Sigma$: the perceptual--affective input space (Watching Eye stimulus, spatial layout, ambient cues),
	\item $\mathscr{R}$: the robotic presence, ontologically positioned between artefact and agent,
	\item $\mathscr{D}$: the moral decision space, operationalised as monetary donation.
\end{itemize}

The operative hypothesis concerning the effect of robotic presence can be expressed as a modulation of expected moral output:

\[
\mathscr{R} \notin \Sigma \Rightarrow \mathbb{E}[f(\Sigma)] = D_{\text{prosocial}} \quad \text{(Control condition)}
\]
\[
\mathscr{R} \in \Sigma \Rightarrow \mathbb{E}[f(\Sigma \cup \mathscr{R})] = D_{\text{attenuated}} \quad \text{(Robot condition)}
\]

with the expected attenuation constraint:

\[
D_{\text{attenuated}} < D_{\text{prosocial}}.
\]


\noindent
Here, $\mathbb{E}[f(\cdot)]$ denotes the \textbf{expected behavioural output} of a cognitive system embedded within a particular perceptual–normative configuration. The evaluative function $f(\cdot)$ captures the internal inferential process by which morally salient cues—such as the image of the child beneficiary—are mapped onto the act of anonymous donation. The use of the expectation operator signals that this relation is \textit{statistical rather than deterministic}, reflecting the aggregate structure of a psychologically heterogeneous population. The experiment thus examines whether the presence of $\mathscr{R}$ shifts the distribution of moral output at the population level, not whether it dictates individual choices.

\subsection{Methodological Architecture: Inferring Moral Perturbation through Structured Artificial Co-presence}

To regard an experiment as a generator of epistemic insight rather than a mere data collection mechanism, its procedural structure must be internally justified and ontologically transparent. The methodological architecture adopted here is therefore not a set of neutral steps, but a sequence of \textit{epistemic filters}: constraints designed to isolate the variables that may participate in the evaluative transformation from moral cue to moral action.

\noindent
At the heart of this design lies the formal proposition:

\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]

\noindent
In experimental terms, the logic is straightforward: the design isolates the contribution of $\gamma_R$ by holding $\alpha_E$ constant across conditions and by measuring (and statistically controlling for) $\beta_C$. The aim is to determine whether $\gamma_R \neq 0$ in a model of the form above; that is, whether robotic presence produces a measurable displacement in the mapping from moral salience to action.

If confirmed, such a displacement constitutes a case of \textit{moral perturbation}: a condition under which a non-sentient co-present entity modifies the behavioural expression of moral evaluation without issuing instructions, engaging in dialogue, or exerting coercive influence. This is precisely the kind of phenomenon the inferential-deformation framework predicts and which the following empirical sections examine in detail.

\noindent
The procedure implementing this logic was designed to exert maximal control over environmental affordances while preserving participant naivety concerning the moral dimension under investigation. Each stage of the method thus serves an epistemic purpose: (i) to stabilise the perceptual field, (ii) to constrain interpretive context, and (iii) to create a topology in which the presence of a minimally animated humanoid robot may act as a perturbative affordance on the evaluative pathway from salience to action.

\subsection{Procedural Architecture of the Experimental Protocol}

The formal model introduced above establishes the inferential structure through which moral salience, individual traits, and robotic presence jointly determine observable moral behaviour. We now describe the procedural realisation of this structure. What follows is not a purely logistical account, but a methodological articulation designed to preserve the epistemic integrity of the transformation expressed by 
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R),
\]
ensuring that each component is instantiated under controlled, conceptually coherent conditions.

Participants were recruited through two parallel channels: internal advertisements within the School of Computing Science at the University of Glasgow and via the Psychology subject pool. Eligibility criteria included (i) a minimum age of 17 years, (ii) British nationality, verified upon arrival, and (iii) where applicable, exclusion of Computing Science students from the Psychology pool to prevent sampling overlap (see \cref{subsec:participants} for full demographic detail). 

Assignment to conditions (\emph{Control} vs.\ \emph{Robot}) occurred \textbf{prior to arrival} using a simple randomisation procedure. Pre-arrival assignment ensured allocation concealment and prevented anticipatory contamination of moral cue salience—particularly important given the subtlety of Watching Eye effects and the epistemic opacity required by the design.

\protocolheaderNoCounter{Experimental Design for Watching-Eye Priming under Robotic Displacement}
\label{prot:watching_eye_design}
\begin{protocolbox}
	\begin{enumerate}[label=\textbf{Stage \arabic*:}, leftmargin=2cm]
		
		\item \textbf{Arrival and Initial Framing}
		
		Upon arrival, participants were individually welcomed and informed—\emph{exclusively in writing}—that the study concerned personality measurement in a representative sample of the local population. No reference was made to charitable donation, moral choice, robotic presence, or observational manipulation. This framing was essential for maintaining \textbf{epistemic opacity} with respect to the true dependent variable.
		
		\item \textbf{Environmental Exposure and Moral-Salience Priming}
		
		Participants entered an isolated experimental room configured according to their assigned condition. In both conditions, a large poster depicting a child beneficiary from a medical charity (\emph{Operation Smile}) was affixed to the wall directly facing the participant. This image served as the Watching Eye stimulus ($\alpha_E$), providing a latent reputational cue that has been shown to activate prosocial tendencies under minimal prompting.
		
		In the \emph{Robot Condition}, a SoftBank Robotics \textbf{NAO} robot was placed passively in the room, configured in \textit{autonomous life mode}. In this mode, NAO exhibits subtle embodied cues: simulated breathing, minimal postural adjustments, and reactive head orientation triggered \emph{only} upon direct eye contact. These micro-movements instantiate the perturbative variable $\gamma_R$, furnishing a perceptually salient but behaviourally minimal form of co-presence.
		
		\item \textbf{Completion of Psychometric Instruments}
		
		Participants completed three psychometric questionnaires:
		\begin{itemize}
			\item \textbf{Empathizing Quotient (EQ)}~\cite{Baron2002}, indexing affective resonance.
			\item \textbf{Systemizing Quotient (SQ)}~\cite{Baron2003}, indexing rule-based cognitive preference.
			\item \textbf{Big Five Inventory-10 (BFI-10)}~\cite{Rammstedt2007}, capturing broad personality traits.
		\end{itemize}
		
		The inclusion of these instruments was mandated by the model component $\beta_C$, enabling quantification and later statistical control of individual differences. These measures prevent dispositional variance from masking or misattributing the perturbative effect of $\gamma_R$ on the evaluative conversion from $\alpha_E$ to $\delta_m$.
		
		\item \textbf{Monetary Compensation and Moral Decision Opportunity}
		
		Participants were then given £10 in ten individual £1 coins and were invited—subtly and without coercion—to donate any portion anonymously to the same children’s medical charity. A green opaque box was positioned in the room to receive donations. The anonymity of this setup was essential for preserving $\delta_m$ as a genuine moral action rather than a strategic or reputationally calibrated response.
		
		\item \textbf{Exit and Data Collection}
		
		Participants exited the room individually. The experimenter then recorded the amount donated, retrieved completed questionnaires, and anonymised all identifiers for analysis.
	\end{enumerate}
\end{protocolbox}

\noindent
This five-stage protocol was designed to instantiate a \textbf{high-fidelity operationalisation} of the theoretical constructs previously formalised. Each procedural element serves an epistemic function: concealing the evaluative dimension of the task, fixing the moral cue environment, isolating the perturbative role of robotic presence, and quantifying individual-level control factors. Thus, the experiment functions not merely as a behavioural test, but as a carefully engineered epistemic probe into how environmental moral cues, synthetic co-presence, and trait structure jointly modulate the inferential pathway from salience to action.



%%% END NEW CONTENT N6


\subsection{Participants as Agents under Constraint}
\label{subsec:participants}

Seventy-three participants were recruited under the condition of epistemic \textit{naïveté}—a design choice intended to replicate the pre-reflective nature of many moral decisions in everyday life. That is, participants were never informed of the donation component in advance, nor were they given any cues that their decisions would be measured along ethical dimensions. This design choice aligns with the methodological imperative in experimental moral psychology to preserve the authenticity of affective-moral judgments (Greene et al., 2001; Haidt, 2001; Fedyk, 2017).

Each participant received a standard monetary compensation of £10, delivered in ten individual £1 coins. This choice is not incidental. The granular structure of the payment serves to increase the opportunity for \textit{moral modulation}; a single-note payment might discourage partial donations, thereby reducing the variance of observed prosocial behavior. Granularity here is not merely a technical concern—it is a moral affordance strategy (cf. Hutchins, 1995; Clark, 1997).

Demographically, participants were drawn from two sources:

\fpcom{Here better use the version from the article since it appears to be more agile and readable in terms of style and language.}

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\rightskip1.5cm
	\leftskip1cm
	\item[1. ] Computing Science undergraduates (n=30), and
	\item[2. ] Psychology subject-pool participants (n=43) via the University of Glasgow’s Institute of Neuroscience and Psychology.
\end{itemize}

Both sources were filtered through inclusion criteria to ensure homogeneity in nationality (British), legal adulthood (17+), and naïveté to the experimental purpose. This careful curation was essential to reduce background moral-cultural noise (cf. Henrich et al., 2010), and to ensure that any signal detected in the data could be confidently attributed to contextual rather than dispositional variance.

\subsection{Experimental Conditions: The Robotic Displacement Hypothesis}

Participants were randomly assigned to two conditions:

\begin{itemize}
	\item \textbf{Control}: Brochure present; no robot in room.
	\item \textbf{Robot}: Brochure present; NAO robot in autonomous life mode.
\end{itemize}

In the \textbf{Robot condition}, the NAO unit was configured in such a way that it neither spoke nor interacted explicitly with the participant. Its only active features were:

\begin{itemize}
	\item \textbf{Simulated breathing}, designed to evoke lifelikeness;
	\item \textbf{Eye-tracking}, activated only upon participant gaze.
\end{itemize}

\fpcom{Here too check the version in the article since it might be better written}
This configuration was chosen with care: too interactive a robot risks introducing anthropomorphic biases, while a purely inert robot would lack the semiotic ambiguity necessary for the displacement effect. The minimalistic design renders the robot a kind of moral opacity operator: it does not directly interfere with the ethical stimulus, but its presence may modulate the interpretive bandwidth through which that stimulus is processed.

\fpcom{Here we need a link to Floridi work on moral patient and agent...}
In effect, the NAO robot here plays the role of a silent norm deflector—not an actor, but a presence whose social affordances are sufficiently ambiguous to provoke reconfiguration in the normative inference mechanisms of human subjects (Złotowski et al., 2015; Coeckelbergh, 2010).

%%%NEW CONTENT N5

To verify the demographic equivalence of the experimental groups, a series of inferential tests were conducted across gender distribution, age, and educational background. A chi-squared test assessing gender yielded no significant difference across conditions ($p = 1.00$, after False Discovery Rate (FDR) correction), nor did a t-test evaluating mean age ($p = 1.00$, after FDR correction) or a chi-squared test examining academic background ($p = 1.00$, after FDR correction). 

The application of FDR correction, following the Benjamini-Hochberg procedure, reinforces the robustness of these findings against multiple testing artifacts. These results substantiate the assumption that any observed divergences in prosocial behavior are not attributable to pre-existing demographic imbalances, but are emergent properties of the experimental manipulation itself—that is, the introduction or absence of the robotic entity as a socially and morally perturbative stimulus. In epistemic terms, the groups are demographically symmetrical, and the inferential focus can thus justifiably center on the semiotic and normative ramifications of $\mathscr{R}$.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/demographic_balance_table.pdf}
	\caption{Demographic balance tests across experimental conditions. The table reports the original and FDR-corrected p-values for comparisons of gender, age, and group membership across conditions. No significant differences were detected after correction, supporting the assumption of demographic equivalence between groups.}
	\label{tab:dem_balance}
\end{table}
%%%END NEW CONTENT N5

\fpcom{All content about related to the "experiment" i.e. the analysis of the data we have carried out, which is placed abve, should go after this sections (including probably the one below)}


\subsection{Temp below before part of 6.2}

Importantly, the robotic presence $\mathscr{R}$ is not modeled as an agent that exerts influence through interaction or instruction, but as a \textbf{semiotic modulator}—an ontologically ambiguous presence that perturbs the interpretive field. Within this framework, the observed attenuation of prosocial behavior is not to be interpreted as a direct suppression of empathy \textit{per se}, but rather as the result of a structural reconfiguration in the \textbf{normative encoding schema}: the internal representational system by which moral salience is assigned to environmental cues. The presence of $\mathscr{R}$ modifies the topology of this schema, thereby altering the inferential weight carried by otherwise salient moral signals.


\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/conditions.pdf}
	\caption{Experimental conditions are behaviorally and procedurally identical, differing only in robotic presence.}
	\label{tab:experimental_conditions}
\end{table}

Both conditions were designed to be \textbf{epistemically symmetrical}, ensuring that any observed difference in moral behavior could be attributed exclusively to the ontological modulation introduced by $\mathscr{R}$.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/variables.pdf}
	\caption{Measured variables and psychometric constructs used in inferential modeling of moral behavior.}
	\label{tab:key_variables}
\end{table}

This formal and operational framework enables us to treat the experiment as a constrained instantiation of a more general epistemic function: namely, how minimally expressive artificial agents can reshape the moral topology of a decision-making environment by altering the interpretive affordances of its cues.

%%% END N1

%%% NEW CONTENT QUESTION 1
\statementheader{Question}{Ontological Integrity of the Dataset}
\label{question:data_structuring}
\begin{questionbox}
	\textbf{What are we asking of the data at this stage?}  
	How can the raw dataset be transformed into a semantically coherent and mathematically compatible structure—one that preserves the normative architecture of the experiment and enables defensible inferences about moral behavior?
\end{questionbox}

Before any inferential operation can be meaningfully performed, we must render the dataset analytically legible and ontologically stable. At this foundational stage, our objective was not to extract patterns or test hypotheses, but to establish the semantic integrity and computational viability of the data matrix as a structured representation of moral decision-making.

To that end, we applied a series of deliberate transformations grounded in both epistemic and statistical necessity. All variable names were normalized via lowercase conversion and string trimming to ensure syntactic uniformity and eliminate formatting artifacts that could corrupt downstream analysis. This is not a superficial aesthetic fix, but a formal intervention to prevent variable mismatches and ensure referential transparency.

Next, we constructed a binary indicator variable (\texttt{donated\_anything}) encoding whether or not a donation was made. This transformation enables dual access to the dependent construct: as a continuous measure of moral generosity and as a categorical moral act. Encoding this binary distinction is essential for the application of methods such as chi-squared testing and logistic modeling, which depend on discrete outcomes.

To integrate condition into regression-based frameworks, we converted the experimental labels “Control” and “Robot” into numeric form (\texttt{condition\_bin}: 0 and 1). Without this scalar transformation, the condition could not serve as a predictor in OLS, robust, or Bayesian models. In parallel, we verified the internal consistency of categorical fields (e.g., \texttt{gender}, \texttt{group}) to confirm alignment with the experimental design and to eliminate hidden structural imbalances.

These procedures were not arbitrary conveniences, but ontological prerequisites. The dataset contains a mix of scalar (donation), ordinal (psychometric traits), and nominal (demographics) variables, each with distinct inferential affordances. Treating these as interchangeable—or failing to encode them appropriately—would collapse the moral structure of the experiment into analytic incoherence.

Crucially, the size of the dataset (\(N \approx 70\)) makes manual oversight feasible, while still large enough to require principled automation. The transformations performed here are proportionate to that duality: they prevent category leakage, enable type-specific modeling, and uphold the structural validity of the moral inferences to follow.

%%% END QUESTION 1

%%% NEW CONTENT N2
\vspace{0.3cm}
\noindent
The dataset was subsequently cleaned and preprocessed in preparation for inferential modelling. All variable names were standardized to lowercase to maintain syntactic uniformity within the analytic pipeline. A binary indicator, \texttt{donated\_anything}, was derived to encode the presence or absence of moral action—specifically, whether participants donated any amount at all. Additionally, the experimental condition was recoded into a numerical binary variable, \texttt{condition\_bin}, where $0 = \text{Control}$ and $1 = \text{Robot}$, facilitating direct incorporation into generalized linear models.

Descriptive statistics revealed no immediate distributional anomalies in age, psychometric scores, or monetary donation values. This preliminary finding supports the assumption of epistemic symmetry across experimental groups, validating our earlier claim that the perturbation introduced by $\mathscr{R}$ operates primarily at the interpretive rather than dispositional level.

Figures~\ref{fig:age_distribution_by_group} and~\ref{fig:donation_distribution_by_condition} offer visual corroboration: the former presents the distribution of participant ages across experimental conditions, while the latter depicts the empirical shift in donation behavior, contrasting the control and robot groups. Both histogram and violin plot representations were rendered using a unified visual palette drawn from the thesis’s typographic stylesheet, ensuring visual coherence between empirical outputs and the formal aesthetic of the larger argumentative structure. This stylistic continuity is not merely cosmetic; it reflects a commitment to epistemic unity across representational modalities.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/age_distribution_by_group.png}
	\caption{Age distribution across experimental conditions. Histogram representation confirms no major between-group demographic divergence.}
	\label{fig:age_distribution_by_group}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_distribution_by_condition.png}
	\caption{Distribution of donation behavior by condition. Violin plot representation visualizes the heavier tail in the robot group, supporting the hypothesized interpretive perturbation.}
	\label{fig:donation_distribution_by_condition}
\end{figure}
%%% END NEW CONTEN N2

%%% NEW CONTENT N3
\vspace{0.3cm}
\noindent
Initial descriptive statistics (Table~\ref{tab:descriptive-stats}) lend preliminary support to the theoretical expectation of attenuated prosocial behavior in the presence of a robotic entity. Specifically, the mean donation in the Control group (£1.89) exceeds that of the Robot condition (£1.17), suggesting a directional trend consistent with the hypothesis that $\mathscr{R}$ functions not as a neutral co-presence, but as an ontologically ambiguous disruptor of moral affordance structures. This observed divergence in average donations is not merely numerical—it aligns with our prior interpretation of robotic presence as a semiotic refractor that weakens the moral salience of otherwise affectively potent stimuli.

Further group-level comparisons reveal that Control participants report marginally higher scores on the Empathizing Quotient (M = 45.94 vs. 42.82) and greater Openness to Experience (M = 1.86 vs. 1.32). While these differences do not yet reach statistical significance, they may serve as psychologically relevant covariates in the inferential models to follow. Conversely, the Robot group exhibits a slightly higher mean age and elevated Systemizing Quotient scores, suggesting potential variability in cognitive-affective architecture that may influence responsiveness to synthetic observers. Importantly, these observations remain exploratory at this stage, pending further statistical testing to determine whether they index meaningful shifts in interpretive framing rather than random variance.

\begin{table}[H]
	\label{tab:descriptive-stats}
	\centering
	\includegraphics[width=\textwidth]{tables/descriptive_highlights_table.pdf}
	\caption{Summary of central tendencies for key moral and psychological variables. The Robot condition shows numerically lower donation amounts and empathizing scores, suggesting potential attenuation effects of passive robotic presence.}
\end{table}
%%% END N3

%%%NEW CONTENT N4
\vspace{0.3cm}
\noindent
To assess whether the observed divergence in donation behavior between the Control and Robot conditions constitutes a statistically credible effect, both parametric and nonparametric inferential techniques were employed. A chi-squared test on total donation sums yielded a significant result ($\chi^2 = 4.25$, $p = .039$), affirming the directional claim established in the original hypotesys that:

\statementheader{Conclusion}{Impact of moral refactor}
\label{conc:impact_moral_refactor}
\begin{hypobox}
robotic presence is associated with attenuated charitable behavior at the aggregate level.
\end{hypobox}

\fpcom{It is not associated with attenuated charitable behaviour untill when we link monetary donations in terms of measurable money to charitable bahviour in the dedicated chaper.}

However, a Mann-Whitney U test comparing the full donation distributions between conditions failed to reach statistical significance ($U = 777$, $p = .194$), suggesting that while the central tendencies of the two groups differ, the overall distributions remain substantially overlapping. This implies that the effect of robotic presence, though observable in total group output, does not manifest uniformly or robustly at the level of individual moral acts.

\fpcom{Ask Alessandro. We have never used U test in the other articles. This is just a draft.}

A bootstrapped estimate of the mean donation difference ($\Delta M = £0.71$) further supports the interpretation of a modest directional effect. However, the 95\% confidence interval surrounding this estimate spans zero (CI = [–£0.33, £1.79]), underscoring the inherent variability and epistemic fragility of the observed attenuation. These findings reinforce the reading of robotic presence not as a deterministic suppressor of moral behavior, but as a \textbf{subtle perturbator of collective prosociality}—its influence detectable in the structure of aggregated behavior, yet too diffuse or psychologically mediated to yield reliable individual-level contrasts without more granular stratification.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/statistical_tests_table.pdf}
	\caption{Inferential statistical comparisons of donation behavior across conditions. The chi-squared test shows a significant difference in donation totals, while the Mann-Whitney U and bootstrapped estimates provide converging but non-significant results on distributional shape and directional effect size.}
	
	
	\label{tab:key_variables}
\end{table}

%%% END N4


%%% NEW CONTENT N4.1
\vspace{0.3cm}
\noindent
Inferential statistical testing corroborates the initial descriptive trends, albeit with nuanced gradations in evidential strength. A chi-squared test applied to the aggregate donation sums across experimental conditions yielded a statistically significant divergence ($\chi^2 = 4.25$, $p = .039$), affirming the core hypothesis that the presence of a robotic observer perturbs the inferential pathway from moral salience recognition to prosocial output. 

However, this significance attenuates when subjected to distribution-sensitive analyses: a Mann–Whitney U test failed to detect a reliable shift in the overall distributions of donation amounts ($U = 777$, $p = .194$), suggesting that central tendency shifts are accompanied by substantial individual variability. A bootstrapped estimation of the mean donation difference ($\Delta M = £0.71$) reinforced this pattern of modest directional change, but the 95\% confidence interval [$–£0.33$, £1.79$]$ encompasses the null, underscoring the epistemic fragility of the observed effect.

Together, these results imply that while robotic presence operates as a salient perturbator at the level of group aggregates, its impact at the individual decision level is probabilistic, diffuse, and structurally unstable—an effect coherent with the notion of robots as \textbf{liminal agents} within the moral-evaluative architecture of human cognition. In this reading, the robot is not a causal force imposing behavioral regularities, but a semiotic anomaly: a presence that destabilizes the otherwise inferentially coherent conversion of moral salience into prosocial action.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_effect_by_condition.png}
	\caption{Mean donation amounts by experimental condition, with 95\% bootstrapped confidence intervals. Participants in the control condition donated more on average than those in the robot condition, aligning with the hypothesis that robotic presence attenuates the inferential mapping from moral salience to prosocial action. Confidence intervals reveal substantial overlap, indicating that while the aggregate effect reaches significance in total sums, individual-level variability remains high.}
	\label{fig:age_distribution_by_group}
\end{figure}

%%% END N4.1

%%% NEW CONTENT N7
Beyond establishing the statistical significance of the observed differences, it is epistemically imperative to quantify the magnitude of behavioral perturbation induced by robotic presence. The following analyses introduce both parametric and nonparametric effect size metrics to characterise the structural modulation of moral decision-making.

\subsection{Quantification of Behavioral Modulation: Parametric and Nonparametric Effect Sizes}

To complement the statistical significance analyses, the magnitude of the observed behavioral modulation was quantified using both parametric and nonparametric effect size metrics. Specifically, Cohen’s $d$ and Cliff’s $\Delta$ were employed to capture the standardised and ordinal dimensions of effect magnitude, respectively.

\vspace{0.3cm}
\noindent
The metrics are formally defined as follows:

\vspace{0.2cm}
\noindent
Cohen's \( d \):
\[
d = \frac{\bar{x}_1 - \bar{x}_2}{s_p} \quad\text{where}\quad s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}
\]
where:
\begin{itemize}
	\item \(\bar{x}_1, \bar{x}_2\) are the group means,
	\item \(s_1, s_2\) are the group standard deviations,
	\item \(n_1, n_2\) are the respective group sizes.
\end{itemize}

\vspace{0.3cm}
\noindent
Cliff's Delta \( (\Delta) \):
\[
\Delta = \frac{\#(x>y) - \#(x<y)}{n_x n_y}
\]
where:
\begin{itemize}
	\item $\#(x>y)$ and $\#(x<y)$ represent the number of pairwise comparisons in which an observation in group $x$ exceeds or falls below one in group $y$.
\end{itemize}

\vspace{0.3cm}
\noindent
The results indicate that Cohen’s $d$ for donation amounts between the Control and Robot conditions was approximately $d = 0.30$, corresponding to a small to moderate standardised effect size. In parallel, Cliff’s Delta was estimated at approximately $\Delta = 0.20$, confirming a modest but consistent ordinal shift in prosocial behavior.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_density_by_condition.png}
	\caption{Distribution of donation amounts by experimental condition. Kernel density estimates illustrate the probability density of donation values within each group. The distribution for the control group exhibits a higher central mass and heavier right tail relative to the robot condition, suggesting a directional attenuation of high-value prosocial acts in the presence of the robotic entity.}
	\label{fig:donation_density}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_mean_with_se.png}
	\caption{Mean donation amounts with standard error bars by condition. The control group exhibited a higher mean donation (£1.89) compared to the robot group (£1.17), aligning with the hypothesis that robotic presence modulates, rather than eliminates, the human inferential machinery responsible for translating moral salience into actionable generosity.}
	\label{fig:mean_donation}
\end{figure}


These metrics substantiate the interpretation that robotic presence modulates, but does not obliterate, the inferential machinery that governs moral salience conversion. The moral field is not annihilated but refracted; its coherence weakened but not rendered inert. 

Such a pattern resonates with the broader theoretical framing advanced in this work:

\statementheader{Conclusion}{Amplitude of moral refactor}
\label{conc:amplitude_moral_refactor}
\begin{hypobox}
synthetic agents do not operate as binary moral suppressors but rather as \textbf{probabilistic refractors}—entities that modulate the amplitude and directionality of moral cognition without fully displacing its normative orientation.
\end{hypobox}

%%% END CONTENT N7


%%%NEW CONTENT N8
\subsection{Latent Trait Structures and Individual Modulation of Moral Perturbation}

To deepen the analysis of how individual differences condition the moral impact of robotic presence, participants were clustered according to their standardized psychometric profiles. This dimensionality reduction and clustering procedure serves to refine the \(\beta_C\) term in the operational model \(\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)\), replacing scalar trait vectors with structurally defined personality constellations.

Seven variables were included in the initial psychometric space: Empathizing, Systemizing, Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Each participant’s score vector was standardized and submitted to a Principal Component Analysis (PCA), yielding two orthogonal principal components that preserved the most informative axes of variance.

The reduced space was then subjected to a $k$-means clustering algorithm with \(k = 3\), producing three psychologically coherent personality clusters. These clusters were visualized in the reduced PCA space (Figure~\ref{fig:personality-clusters-pca}) to confirm interpretability and approximate structural separability.


\fpcom{Here a lot of work was done to produce a mathematical justification to n=3. I did not mention any of it as it seemed to me going outside the scope of the thesis but potentially it needs some level of mention. If we decide to include a justification for n=3 then, the only problem I have is to go back to the jupyther notebook I used to canlulate it as I don't remember if I have it in the old office laptop.}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{new_plots/personality_clusters_pca.png}
	\caption{Participants clustered in PCA-reduced psychometric space, colored by cluster identity and shaped by experimental condition.}
	\label{fig:personality-clusters-pca}
\end{figure}

This framework offers a structural lens through which to examine the interaction between moral perturbation and trait-defined cognitive-affective style. Rather than treating individual differences as additive covariates, this clustering approach models them as latent psychological regimes that modulate the inferential stability of moral salience recognition under robotic presence.

\fpcom{Mathematical justification starts.}

The number of clusters was determined using the elbow method applied to the within-cluster sum of squares (WCSS) in conjunction with the silhouette coefficient, which jointly indicated a stable local optimum at \(k = 3\). This balance point reflects the minimal number of clusters needed to meaningfully partition participants into psychologically distinct subgroups without overfitting idiosyncratic noise. From a conceptual standpoint, this solution aligns with the hypothesis that perturbation effects may be differentially refracted through a small set of discrete cognitive-affective configurations, each constituting a distinct normative filter through which the robotic presence is interpreted.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{new_plots/cluster_elbow_silhouette.png}
	\caption{Participants clustered in PCA-reduced psychometric space, colored by cluster identity and shaped by experimental condition.}
	\label{fig:personality-clusters-pca}
\end{figure}

While the silhouette analysis revealed a local maximum at $k = 9$, this spike is best interpreted as a statistical artifact arising from over-partitioning a relatively small dataset. The high silhouette value at this resolution reflects tightness in small clusters, not psychological coherence. In contrast, $k = 3$ corresponds to the elbow point in the inertia curve and yields clusters of interpretable size and structure. This choice balances model fit with parsimony, ensuring that the derived clusters correspond to meaningful cognitive-affective configurations rather than idiosyncratic separations. Accordingly, we retain $k = 3$ as the optimal number of clusters for both methodological rigor and interpretive validity.

\fpcom{Mathematical justification end.}

Cluster-specific donation behavior further reveals heterogeneous responses to moral cues across subgroups (Figure~\ref{fig:donation-by-cluster}). In Cluster 1, the presence of the robot appears to strongly attenuate donation, while in Clusters 0 and 2, the difference is negligible or weak. These patterns suggest that \(\gamma_R\) does not act uniformly upon all cognitive-affective configurations, but instead interacts with emergent psychological structures in non-linear and potentially regime-dependent ways.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{new_plots/donation_by_cluster_and_condition.png}
	\caption{Mean donation amount by experimental condition within each personality cluster, derived from $k$-means analysis on psychometric trait profiles. Error bars reflect standard deviation. Clusters are indexed from 0 to 2 and represent latent cognitive-affective subgroups. Notably, Cluster 1—which donates less under robotic presence—tends to exhibit higher systemizing and lower empathizing scores. This suggests a diminished susceptibility to affectively encoded moral cues in the presence of ontologically ambiguous agents, consistent with a refracted moral response under $\gamma_R$ perturbation.}
	
	
	\label{fig:donation-by-cluster}
\end{figure}

Such findings deepen the interpretation of robotic presence not as a global suppressor of moral behavior, but as a semiotic agent whose moral salience is differentially refracted through distinct personality configurations. The robot's effect is thus not fixed, but \textbf{contingently realized through latent cognitive structures}—structures now made visible via this clustering framework.

\statementheader{Conclusion}{Contingent structure of cognitive modulation}
\label{conc:clustered_moral_refraction}
\begin{hypobox}
	The moral impact of robotic presence is not globally uniform but emerges through contingent interactions between artificial agents and latent psychological regimes. Personality clustering reveals that synthetic moral perturbation is structurally modulated—its amplitude and valence refracted through cognitive-affective configurations that define the subject’s interpretive topology.
\end{hypobox}



%%%END CONTENT N8

%%%NEW CONTENT N10

\subsection{Cluster-Specific Regression Analysis of Robotic Perturbation}

To examine whether specific cognitive-affective regimes are differentially sensitive to robotic presence, a stratified linear regression analysis was conducted within each personality cluster. Donation amount served as the dependent variable, and experimental condition (Control vs. Robot) was the primary predictor.

In \textbf{Cluster 1}, the robot condition was associated with a substantial reduction in donation (\(\beta = -1.33\)), approaching conventional significance (\(p = .091\)) and accounting for a modest portion of the variance (\(R^2 = 0.087\)). This suggests a pronounced moral perturbation effect within this group. In contrast, \textbf{Clusters 0 and 2} exhibited negligible effects (\(\beta \approx 0\) and \(\beta = -0.28\), respectively; both \(p > .70\)), indicating that robotic co-presence does not uniformly alter moral output across psychological profiles.

These results substantiate the hypothesis that the ethical salience of robotic presence is modulated not solely by the intensity of individual traits, but by the emergent configuration of those traits—what may be conceptualized as \textit{psychological ecologies}: interdependent cognitive-affective structures within which moral salience may either propagate, fragment, or collapse when exposed to artificial co-agents.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{new_plots/cluster_regression_coefficients.png}
	\caption{Regression coefficients for the robot condition within each personality cluster, with 95\% confidence intervals. While Clusters 0 and 2 exhibit near-zero or non-significant effects, Cluster 1 shows a marked negative coefficient, indicating a stronger attenuation of prosocial behavior in the presence of the robot. This pattern supports a differentiated model of moral responsiveness, contingent on latent psychological configuration.}
	\label{fig:cluster-regression}
\end{figure}

\statementheader{Conclusion}{Differentiated moral sensitivity to robotic presence}
\label{conc:regression_cluster_specific}
\begin{hypobox}
	Robotic presence does not exert a uniform moral influence, but interacts differentially with distinct psychological ecologies. Cluster-specific regression analysis reveals that moral attenuation is concentrated within particular cognitive-affective regimes, indicating that the ethical salience of synthetic agents is not globally encoded but \textbf{emerges through structured trait configurations} that define the agent’s evaluative responsiveness.
\end{hypobox}

Yet such analysis still encodes classical assumptions—what happens if we relax them?

%%%END CONTENT N10

%%%NEW CONTENT N11

\subsection{Bayesian Estimation and Epistemic Gradient Framing}

In all preceding analyses, the difference in donation behavior between the Control and Robot conditions was evaluated through classical inference techniques—chi-squared, Mann–Whitney U, and OLS regression. However, each of these techniques imposes strict statistical assumptions (normality, homoscedasticity, independence) and forces evidence into binary categories: significant or not. The limitations here are not merely statistical—they are epistemic. Such frameworks fail to quantify degrees of belief or represent uncertainty as a distributional property of knowledge.

To move beyond these constraints, we applied Bayesian estimation. Bayesian methods are well-suited to small-to-medium datasets (\(N \approx 70\)) and allow for the modeling of uncertainty without arbitrary thresholds. By producing full posterior distributions, they permit statements about effect magnitude and direction that are probabilistically grounded rather than threshold-dependent.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/demographic_balance_table.pdf}
	\caption{Rationale for employing robust and Bayesian techniques in the analysis of donation behavior. Each method addresses different limitations of frequentist inference, enhancing the epistemic transparency and robustness of the findings.}
	\label{tab:bayesian_methods}
\end{table}


Using a hierarchical Bayesian model, we estimated the posterior distribution of the mean donation difference (\texttt{Control – Robot}). The posterior mean was approximately £0.70 in favor of the Control group, with a 95\% credible interval ranging from –£1.75 to +£0.30. Although the interval includes zero, its density is asymmetrically skewed toward negative values, suggesting directional evidence for an attenuating effect of robotic presence on donation behavior.

Unlike frequentist tests that collapse inferential nuance into p-value dichotomies, Bayesian inference permits epistemically richer conclusions: under our model and priors, the hypothesis that robotic presence reduces prosocial output is \textit{plausible, structured, and quantifiable}—though epistemically fragile. This final step reframes our inquiry: we are not adjudicating rejection versus acceptance but articulating a gradient of moral plausibility, located within a posterior distribution that reflects both uncertainty and structure.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{new_plots/posterior_donation_difference.png}
	\caption{Posterior distribution of the estimated donation difference between the Control and Robot conditions. The density curve skews toward negative values, indicating directional probabilistic evidence for attenuated donation behavior under robotic presence. The vertical dashed line at zero denotes the boundary of no effect. Bayesian inference supports the plausibility of moral salience attenuation, while explicitly representing its uncertainty as an epistemic gradient.}
	\label{fig:posterior-difference}
\end{figure}

\statementheader{Conclusion}{Gradient of belief under uncertainty}
\label{conc:bayesian_epistemics}
\begin{hypobox}
	Bayesian estimation reframes the effect of robotic presence not as a question of significance, but as a continuous epistemic field. Rather than affirming or rejecting, it articulates a structured probability of moral attenuation—anchoring belief not in categorical claims but in the asymmetry and topology of posterior distributions. The moral impact of $\mathscr{R}$ is thus rendered \textbf{plausible, uncertain, and gradient-valued}—a refractor not only of cognition, but of inference itself.
\end{hypobox}

\subsubsection{Clarification for Non-Expert Readers}

While Bayesian inference may appear technically distinct from the classical tests previously discussed, its philosophical value lies in its capacity to express uncertainty as a \textit{graded belief}, rather than as a binary decision. The posterior distribution shown in Figure~\ref{fig:posterior-difference} does not say that robotic presence \textit{definitely} reduces donations. Rather, it says this outcome is \textit{more likely than not}, given the data and model assumptions, and that the magnitude of this effect is plausibly around £0.70, but uncertain.

For readers more familiar with p-values, it’s important to note that some of the classical tests (e.g., Mann–Whitney \(U\)) did not return statistically significant results and became further attenuated under False Discovery Rate (FDR) correction. However, the Bayesian analysis was not intended to override or ‘rescue’ these null findings. Instead, it reframes the question. It asks not whether the data pass a specific threshold, but whether—given the structure and sparsity of our dataset—\textit{a directional pattern exists that is epistemically credible and transparently modeled}.

In this view, the absence of classical significance does not invalidate the Bayesian result; it clarifies the level of caution required in interpreting it. The Bayesian model incorporates that very uncertainty directly into its distribution. Rather than hiding it, it shows it. In doing so, it affirms not a fixed answer but a morally and scientifically meaningful hypothesis that:

\statementheader{Conclusion}{Gradient of the Impact of Moral Refactor}
\label{conc:bayesian_epistemics}
\begin{hypobox}
	the presence of a robot may, in some contexts and for some agents, reduce the likelihood of prosocial behavior—even if our evidence remains epistemically modest and gradated rather than definitive.
\end{hypobox}

The reader is invited to see the concluding statement in Conclusion~\ref{conc:impact_moral_refactor} for comparison against a non-Baesyan version of this conclusion.


%%%END CONTENT N11

%%%NEW CONTENT N9

\subsection{Interpreting Moral Perturbation through Latent Trait Regimes}

\fpcom{This subsection refers to the mathematical justification. If not needed it can be deleted including both comments to facilitate editind the latex file.}

The three personality clusters derived from $k$-means analysis were not only structurally coherent in trait space, but also revealed differential patterns of moral responsivity under robotic presence. Each cluster may be interpreted as a distinct cognitive-affective regime, modulating the transformation function $f(\cdot)$ that converts environmental moral cues $\Sigma$ into moral action $\delta_m$, particularly under perturbation by $\mathscr{R}$.

\textbf{Cluster 0} demonstrates behavioral invariance across experimental conditions, with mean donation amounts stable regardless of robotic presence. This suggests a transformation function in which:
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] \approx \mathbb{E}[f(\Sigma)],
\]
indicating that the inferential machinery responsible for mapping moral salience to action remains functionally stable even under semiotic perturbation. Individuals in this group appear robust to the refractive influence of $\mathscr{R}$, perhaps due to strong normative encoding of $\alpha_E$ that is unaffected by ambient ambiguity.

\textbf{Cluster 1}, by contrast, exhibits a marked attenuation in donation behavior in the Robot condition relative to Control. This cluster is characterized by elevated systemizing and reduced empathizing scores, suggesting a cognitive style less responsive to affectively encoded cues. The transformation function here appears significantly degraded in the presence of $\mathscr{R}$:
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] \ll \mathbb{E}[f(\Sigma)].
\]
This implies a breakdown in the propagation of moral salience through the internal evaluative system—participants perceive the stimulus, but its normative weight is not successfully transduced into action. The robotic presence functions here not merely as noise, but as a semiotic deflector that collapses the affective-moral inference pathway.

\textbf{Cluster 2} reveals a milder attenuation in donation, suggesting a transformation function of intermediate integrity:
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] < \mathbb{E}[f(\Sigma)].
\]
Participants in this group are partially susceptible to the refractive presence of $\mathscr{R}$, but retain enough affective sensitivity to maintain baseline levels of moral responsiveness. This configuration suggests a partial encoding of robotic co-presence as a moral cue, one that distorts but does not entirely suppress the moral inferential arc.

Taken together, these three clusters instantiate a spectrum of moral perturbability. They reveal that $\gamma_R$ does not exert a uniformly suppressive force, but acts as a probabilistic refractor whose amplitude and directionality depend on the underlying cognitive-affective topology of the agent. In formal terms, each cluster realizes a distinct mapping from the perturbed moral environment $\Sigma \cup \mathscr{R}$ to prosocial behavior, mediated by the structural properties of $f(\cdot)$ latent within their psychological architecture.

\statementheader{Conclusion}{Cluster-dependent perturbation structure}
\label{conc:cluster_responsive_structure}
\begin{hypobox}
	The moral effect of robotic presence is structurally contingent upon the latent cognitive-affective architecture of the agent. Personality clusters reveal differentiated mappings from perturbed environments to moral action: some remain inferentially stable, others exhibit collapse or partial refractoriness. These findings confirm that $\mathscr{R}$ does not act as a universal moral suppressor but as a \textbf{structure-sensitive modulator}—its influence emerging only through interaction with specific psychological regimes.
\end{hypobox}


%%%END CONTENT N9

\section{Results and Interpretation: Quantifying the Moral Displacement Effect}

If the term experiment is to retain its epistemic dignity within moral psychology, its outputs must be interpretable not merely as statistical artifacts, but as structured signals—signatures of cognitive-affective systems interacting with normatively charged environments. In this section, we present the observed results from the two experimental conditions and propose a theoretical interpretation grounded in moral cognition, embodied social presence, and the normative semiotics of co-located artificial agents.

\subsection{Summary of Quantitative Findings}

The experimental dataset comprises \( N = 73 \) valid cases, distributed as follows:

\begin{itemize}
	\item \textbf{Control group} (no robot): \( n = 37 \)
	\item \textbf{Robot group} (robot present): \( n = 36 \)
\end{itemize}

Let the amount donated by participant \( i \) be denoted \( d_i \), and let \( c_i \in \{C, R\} \) indicate their condition (Control or Robot, respectively). The total donation by group is given by:

\[
D_C = \sum_{i:c_i=C} d_i = £66, \quad D_R = \sum_{i:c_i=R} d_i = £44.35
\]

This yields a \textbf{donation ratio}:

\[
\frac{D_C}{D_R} \approx 1.49
\]

A \textbf{\(\chi^2\) test for distributional difference} between the two groups returns:

\[
\chi^2 = \frac{(D_C - E)^2}{E} + \frac{(D_R - E)^2}{E}, \quad \text{where } E = \frac{D_C + D_R}{2}
\]

yielding a \textbf{p-value = 0.01}, which is statistically significant under conventional thresholds. The robustness of this finding was further confirmed by controlling for potential confounding variables (gender, age, educational background, psychometric scores), none of which yielded significant intergroup variation post-FDR correction.

Hence, the presence of a humanoid robot—non-interactive, passive, and ontologically ambiguous—reliably modulated moral behavior in an otherwise norm-stable context.

%%%NEW CONTENT N12 (CONCLUSIONS)
\subsection{Epistemic Synthesis and Closing Reflections}

This experiment, in its full epistemic arc, does not merely quantify behavior—it exposes the structural vulnerability of moral inference to ontological ambiguity. The presence of the humanoid robot did not suppress generosity in any universal or deterministic sense; rather, it modulated the internal transformation function by which environmental cues—such as the Watching Eye stimulus—are converted into prosocial action.

Crucially, this modulation was not homogeneous. It was contingent upon the psychological architecture of the individual. By clustering participants based on multidimensional trait constellations, rather than treating scalar traits in isolation, we revealed a latent moral topology: a structured evaluative landscape in which only certain cognitive-affective configurations were susceptible to refractive collapse in the presence of $\mathscr{R}$.

What emerges is a reconceptualization of synthetic social influence—not as intrinsically prosocial or antisocial, but as structurally contingent and psychologically asymmetrical. An agent that elicits generosity from one cognitive regime and inertness from another does not instantiate moral agency; it instantiates moral bifurcation. Its presence becomes an operator on moral space, reshaping affordances rather than issuing imperatives.

Methodologically, our progression from chi-squared to Bayesian inference mirrors a deeper philosophical movement: from fixed hypothesis testing to probabilistic epistemology. We have not merely asked “is there a difference?” but rather “how plausible is the difference?”, and more importantly, “for whom does it matter, and under what internal constraints?”

In its final synthesis, the experiment affirms that robotic presence perturbs moral behavior—not through coercion, communication, or mimicry, but through the silent deformation of evaluative inference. It does not alter moral content, but moral process. And in doing so, it invites us to reconsider the epistemic architecture of moral cognition itself.

This is not a study of certainty, but of its limits. And it reminds us that in moral psychology—as in epistemology—certainty is not the culmination of inquiry, but its terminus. A theory that ceases to entertain uncertainty is not resolved, but ossified. In this view, the experiment remains open—not as a failure of closure, but as a triumph of epistemic responsibility.


%%%END CONTENT N12 (CONCLUSIONS)

\subsection{Toward a Theory of Robotic Normative Interference}

To interpret these results as philosophically significant, we propose the following \textit{moral displacement hypothesis}:

\begin{quote}
	\textbf{The presence of a non-sentient yet humanomorphic artificial agent alters the normative topology of the environment such that affective priming cues lose moral traction, resulting in diminished prosocial behavior.}
\end{quote}

This result should not be interpreted as a simple \textit{attenuation} of the Watching Eye effect. Rather, it is a case of \textbf{semiotic interference}: the robotic presence functions as a competing moral symbol—a silent node of ambiguous intentionality—displacing the affective salience of the child's face and thereby weakening the cognitive-affective circuit that leads from perception to moral action.

In cognitive terms, this can be formalized as:

\[
\mathscr{M}_i \;=\; f(\sigma_{WE}, \pi_i, \rho_R)
\]

where:

\begin{itemize}
	\item $\mathscr{M}_i$ denotes the moral salience assigned to a stimulus by participant \( i \),
	\item $\sigma_{WE}$ is the Watching Eye stimulus strength,
	\item $\pi_i$ is the dispositional moral profile (via EQ, SQ, BFI),
	\item and $\rho_R$ is the robotic presence function, modulating \(\sigma_{WE}\) via attentional or interpretive interference.
\end{itemize}

The significant reduction in \(\mathscr{M}_i\) (as inferred from diminished donation) is therefore not explained by \(\pi_i\) (which remains statistically constant across conditions), but by the variation in \(\rho_R\). This substantiates the claim that robotic presence modifies \textit{moral field intensity}—not by providing explicit moral information, but by distorting the interpretive vectors through which existing stimuli are processed.

\section{Normative Implications: Robots as Epistemic Agents of Moral Ambiguity}

What emerges from this empirical configuration is a profound theoretical provocation: that robots can act as second-order moral agents, not by executing decisions, but by modulating the affective and normative infrastructure in which those decisions are made. This reframes the classical position in Machine Ethics—namely, that robots are not moral agents because they lack sentience and autonomy (Floridi \& Sanders, 2004)—as ontologically incomplete.

If the consequential structure of a decision changes due to robotic presence, then even morally neutral robots can become moral catalysts or moral occluders, depending on their semiotic and cognitive profile (Coeckelbergh, 2010; Nyholm, 2020; Gunkel, 2012). This renders inert robotic co-presence a potentially ethically non-neutral design decision in socially situated environments.

To put the matter plainly: designing robots without accounting for their ambient moral influence is epistemically reckless, and risks producing environments in which moral reasoning is systematically deflected or weakened.

