%\chapter{An Experimental Study of Moral Displacement: From Normative Hypothesis to Experimental Topology}
\chapter{Moral Displacement: An Experimental Investigation}
\label{chap:experimental_methods}
\thispagestyle{pprintTitle}


% Define a counter named 'question' that resets every time 'chapter' increments
\newcounter{question}[chapter]
% Define the format of the counter to be 'chapter_number.question_number'
\renewcommand{\thequestion}{\thechapter.\arabic{question}}

% Adjusting epigraph settings
\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushright}

% Setting the font and spacing for the epigraph
%\epigraph{\itshape \setstretch{1.2}But one thing is the thought, another thing is the deed, and another thing is the idea of the deed. The wheel of causality doth not roll between them.}{\small{Friedrich Nietzsche, \textit{Thus Spoke Zarathustra} (1883)}}
%
%

% Prefatory Remarks: 
\section{Conceptual Foundations of the Research Question}

This chapter begins with a precise question: \textit{can the silent presence of a humanoid robot alter the evaluative process that turns moral perception into action?} 

This question, while operationally simple, reaches beyond behavioural measurement. It engages the broader project of understanding moral behaviour not merely as an individual trait but as an inferential process that emerges from the perception and decoding of socially meaningful signals—\textbf{a process that can, in principle, be computationally modelled}. 

Within the domains of social signal processing and artificial intelligence, the transformation of subtle environmental cues into behavioural outputs is treated as a mapping from informational stimuli to structured responses~\cite{Vinciarelli2009}. By embedding a humanoid robot—ontologically ambiguous, semantically potent, yet behaviourally inert—into a morality-salient environment, this experiment asks whether such synthetic presences perturb not the content of deliberation, but the signal-to-inference architecture through which salience becomes action.

\begin{center}
	\begin{questionbox}[label={q:robot-agent}]{Inferential Displacement}
		Can the mere presence of a synthetic, non-agentic entity perturb the inferential transformation through which morally salient cues are converted into observable moral behaviour?
	\end{questionbox}
\end{center}

In other words, the question asks whether the mere fact of a robot’s presence—despite the absence of task-related communication or instruction—can alter the evaluative mechanism that translates moral perception into moral behaviour, operationalised here as prosocial giving.

In this experiment, moral action is instantiated through a measurable behavioural outcome: the voluntary donation of part of the participant’s monetary compensation to a children’s medical charity. The humanoid robot introduced into the experimental environment is not interactive in any directive or conversational sense, but neither is it inert. Operating in autonomous life mode, NAO exhibits subtle embodied motions—simulated breathing, minor postural adjustments, and head orientation shifts triggered only when participants establish eye contact. These micro-movements constitute precisely the minimal behavioural cues known to activate or modulate the Watching Eye effect, thereby rendering the robot a semantically potent, low-agency observer within the moral field. By examining whether the presence of such a humanoid robot systematically shifts donation behaviour, we test whether synthetic co-presence perturbs not the participants’ reflective moral reasoning, but the \textbf{conditions under which morally salient cues elicit prosocial action}.


In other terms, the inquiry asks whether the presence of a humanoid robot—endowed not with communicative capacity but with minimal yet perceptually salient behavioural affordances—can alter the evaluative pathway through which moral perception becomes moral behaviour, operationalised here as \textbf{prosocial giving}.


In this experiment, moral action is instantiated through a measurable behavioural outcome: the voluntary donation of part of the participant’s monetary compensation to a children’s medical charity. The inquiry therefore isolates \textit{presence} itself—specifically, synthetic presence—as an informational and epistemic variable. It examines whether introducing such a form into a morality-salient environment alters the \textbf{situational conditions under which moral action is produced}. Crucially, the experiment does not attempt to model or infer the internal structure of moral reasoning; rather, it observes how the resulting behavioural expression of moral decision-making shifts across environments that differ only in the presence or absence of this subtly animated robot. In this way, the design tests whether synthetic co-presence perturbs not the content of deliberation, but the \textbf{conditions under which morally salient cues become behaviourally actionable}.

\noindent
Framing the investigation as a \textit{question} (Question ~\ref{q:robot-agent} p.~\pageref{q:robot-agent}) rather than a hypothesis is deliberate. It preserves the conceptual openness required at this stage of the analysis, foregrounding inquiry over prediction. Within interdisciplinary research—spanning moral psychology, social signal processing, and human–robot interaction—prematurely imposing a directional hypothesis risks presupposing the very moral effects that the experiment is designed to probe. By articulating a guiding research question rather than an asserted claim, we allow the empirical structure of the data to shape the inferential trajectory rather than constraining it in advance. This is consistent with both the methodological caution urged in philosophy of science and the epistemic humility appropriate when dealing with morally charged, psychologically subtle, and technologically novel forms of social influence.

\noindent
Against this backdrop, the central inquiry of the study can be expressed with complete clarity: \textit{does the mere presence of a humanoid robot alter how human beings act when confronted with a morally relevant choice?} 

Put operationally, we ask whether individuals donate differently to a charitable cause when a robot quietly shares the room with them. The behaviour of interest—\textbf{prosocial giving}—is quantified directly as the amount of money voluntarily deposited into a charity box. The variable is simple in measurement but dense in interpretive significance: the coins themselves index the culmination of a moral appraisal process, the behavioural footprint of an evaluative transformation triggered under conditions of minimal social prompting.

\noindent
Yet the stakes of this question extend beyond monetary donation. What is under scrutiny is whether artificial companions—even in the absence of agency, speech, intention, or social engagement—can modulate the conditions under which morally salient cues are converted into human action. In this respect, the study examines not only how much participants give, but \textit{why} behaviour may shift under synthetic co-presence. The possibility being tested is subtle but far from trivial: that the introduction of an ontologically ambiguous entity into a moral environment may refract the participant’s evaluative landscape, thereby altering the behavioural expression of moral choice.

\section{Experimental Design and Behavioural Paradigm}

To investigate this Question ~\ref{q:robot-agent} (see p.~\pageref{q:robot-agent}), we implemented a controlled behavioural experiment~\cite{RosenthalRosnow2008, ReisJudd2000, Kazdin2017} derived from the classical \textit{Watching Eye} paradigm~\cite{HaleyFessler2005, Bateson2006, NettleEtAl2013, BatesonEtAl2013_EyesLittering, PfattheicherKeller2015, ContyGeorgeHietanen2016, DearDuttonFox2019}, in which prosocial behaviour is modulated by the perceived presence of observation. Seventy-three participants were invited individually into a room under the pretext of completing a battery of personality questionnaires in exchange for monetary compensation. Embedded in the experimental space was a morally salient cue: a charity brochure prominently featuring the photograph of a child requiring medical assistance. Decades of empirical work show that such stimuli reliably activate prosocial dispositions through mechanisms of implicit monitoring and empathetic engagement~\cite{Haley2005,Conty2016}.

\noindent
The robotic manipulation was then introduced as the sole experimental variable. In the control condition, participants completed the task alone. In the experimental condition, a humanoid robot—NAO~\cite{Aldebaran2013}—remained silently present in \textit{autonomous life mode}, exhibiting only the minimal embodied cues characteristic of that configuration: simulated breathing, subtle postural adjustments, and reactive head orientation triggered exclusively by eye contact. 

These micro-movements, though non-interactive and devoid of communicative intent, constitute precisely the class of minimal behavioural affordances shown to activate or modulate the mechanisms underpinning the Watching Eye effect. By embedding this low-agency, perceptually salient entity into an otherwise identical moral environment, the design isolates \textit{synthetic presence}—rather than dialogue, instruction, or overt agency—as the only \textit{manipulated} dimension of the setting. The personality questionnaires, administered under the pretext of a trait study, simultaneously serve as a cover story and as a structured measurement of individual cognitive–affective profiles. In subsequent analyses, these trait measures are treated as moderators, allowing us to ask whether any observed differences in prosocial donation behaviour arise from the robot’s presence alone, from stable individual dispositions, or—critically—from their interaction within a shared moral field.

\subsection{Why Minimal Presence Matters: Ontological Ambiguity as Experimental Variable}
Much of the literature on moral decision-making in human–robot interaction (HRI) and human–machine interaction (HMI) locates moral modulation in the interactive capacities of artificial agents. Studies routinely foreground expressive behaviour, ostensive cues, adaptive responsiveness, displays of accountability, or anthropomorphic signalling as the levers through which machines influence human judgment and behaviour~\cite{Malle2016,VanStraten2020,Arnold2017,Groom2010,Leidner2019}. These approaches implicitly assume that moral impact requires action: verbal behaviour, communicative intent, social reciprocity, or strategically framed moral cues.

\textit{The present experimental design intentionally refuses this assumption.}

Rather than examining how robots act, we examine how they exist—that is, how their mere ontological presence, stripped of communicative intent and devoid of interactive complexity, may nevertheless perturb the inferential transformation through which morally salient cues become behaviourally instantiated. The focus is not on moral agency or synthetic ethics, but on the structural susceptibility of human moral cognition to ontologically ambiguous stimuli.

This methodological divergence is conceptually foundational. It allows us to target an aspect of moral cognition that is often overlooked: its \textit{pre-reflective permeability} (for a similar use of the term refer to~\cite{Husserl1913, Zahavi2005, Gallagher2005, Bargh1994}) to agent-like cues even when those cues lack \textit{intentional content}~\cite{Brentano1874, Searle1983, Crane2001}. The question is not whether robots can engage in moral exchange, but whether their presence, by virtue of their bodily form and minimal behavioural affordances, reshapes the inferential scaffolding that mediates between perceiving a moral cue and acting upon it.

\noindent
This problem is particularly salient in domains such as Social Signal Processing and computational social cognition, where synthetic agents routinely evoke social and moral reactions that exceed the informational complexity of their behaviour~\cite{Vinciarelli2009,Bremner2022}. By removing dialogue, task-relevance, and overt interaction while maintaining the perceptual markers of potential agency (eyes, posture, orientation, micro-motion), the experiment isolates \textbf{presence itself} as the epistemic variable to be tested.

In this respect, the design probes a structural vulnerability of norm-sensitive cognition: the possibility that minimal cues—mere \textit{indications} of agenthood—may exert disproportionate influence on evaluative pathways. The robot is not required to speak, gesture, or respond; its semantic force lies in its ability to activate interpretive priors associated with observation, evaluation, and social monitoring.

This intuition resonates with the hyperactive intentional stance described by Guthrie~\cite{Guthrie1993}, Waytz et al.~\cite{Waytz2010}, and Dennett~\cite{Dennett1987}, according to which humans routinely over-ascribe agency in uncertain environments. By positioning the robot in the liminal space between objecthood and agenthood, the experiment isolates not action, but anticipation—the silent priors that precede full agentive recognition.

\noindent
The methodological focus on \textbf{mere presence} thus reflects a principled decision: it disentangles interactive contingencies from deeper, subpersonal cognitive mechanisms that structure moral evaluation. Unlike approaches that equate moral influence with dialogue or reciprocity, this design foregrounds the epistemic topology of moral salience—the latent structures of social attribution that shape inferential pathways prior to action, prior even to conscious appraisal.

Having established the necessity of minimal presence as an experimental variable, the next conceptual step is to formalise the framework that renders this presence epistemically potent. This is where Floridi’s Levels of Abstraction (LoA) become essential: they provide the philosophical infrastructure required to explain why \textit{an entity that does nothing}, and to which no moral status is attributed, may still distort the conditions under which moral cues become behaviourally actionable.

This motivates a transition, not from theory to application, but from conceptual architecture to \textbf{experimental justification}.

\subsection{Levels of Abstraction and the Design Logic of Minimal Robotic Presence}

\noindent
The decision to deploy a humanoid robot in silent autonomous life mode—exhibiting only simulated breathing, subtle postural adjustments, and eye-contact-contingent head orientation—is not a matter of convenience or technological limitation. It is a philosophical and methodological choice grounded in Floridi’s theory of \textit{Levels of Abstraction} (LoA)~\cite{Floridi2008,Floridi2010,Floridi2013}. To appreciate this decision, the core function of LoAs must be understood with conceptual precision.

An LoA specifies the informational interface through which an agent, system, or observer accesses and processes the world. It determines which distinctions are epistemically visible and which are systematically bracketed. LoAs are therefore not metaphysical: they make no assertions about the intrinsic ontology of entities. Rather, they are \textit{epistemic configurations}, selective filters that carve out what counts as relevant information.


\noindent
Applied to the present experiment, LoAs allow us to describe moral influence without relying on metaphysical accounts of robot agency. At the LoA operative for a participant alone in a room, moral relevance does not depend on the robot’s internal states but on its semantic affordances: its posture, its eyes, the symmetry of its body, the direction of its face, its quiet imitation of biological rhythms~\cite{Emery2000, Hietanen2002, CarneyCuddyYap2010, Argyle1975, Rhodes2006, Johansson1973, Saygin2012, ChaminadeOhnishi2007}.

These features are perceptually encoded as possible indicators of being watched~\cite{Emery2000, KleckStrenta1980, Hietanen1999, Hietanen2002, Argyle1975, MasonTatkinMacrae2005. Johansson1973_BiologicalMotion, Saygin2012_BiologicalMotionRobots, BatesonNettleRoberts2006_CuesWatched}, evaluated, or accompanied—precisely the conditions under which the Watching Eye effect operates. Thus, the robot’s moral relevance emerges not from consciousness, autonomy, or interactive capacity, but from its informational presentation within the participant’s operative LoA.

\noindent
This perspective enables a shift away from essentialist distinctions—agent versus non-agent, sentient versus non-sentient—toward a functional reading: what does the robot \textit{do} at the LoA of the observer? At this LoA, NAO’s subtle bodily cues instantiate the informational signatures of a putative observer, thereby modulating the epistemic background against which morally salient cues (such as the charity poster) are evaluated.

\noindent
The placement of the robot in autonomous life mode is therefore a purposeful calibration of informational affordances. If NAO were fully interactive, the LoA would shift, and the participant would be required to adopt an intentional stance grounded in dialogue, reciprocity, or social coordination. \textit{This would confound the experiment by introducing behavioural and communicative variables}. Conversely, if the robot were completely inert—akin to a mannequin—the LoA would strip away most agent-like affordances, nullifying the minimal conditions under which moral salience can be perturbed

\noindent
NAO therefore occupies a deliberate middle space: a synthetic presence endowed with minimal but meaningful cues, sufficient to activate the epistemic structures associated with potential observation but insufficient to produce interactive interpretation. In this capacity, NAO aligns with Floridi and Sanders’ notion of an \textit{artefactual moral agent}~\cite{FloridiSanders2004,Floridi2013}: a non-sentient entity whose moral relevance arises not from autonomy but from the role it plays within an informationally structured environment.

\fpcom{This is more a conclusion.}
\noindent
In short, Floridi’s LoA framework explains why a non-interactive, subtly animated robot is an epistemically potent variable. It provides the philosophical rationale for a design in which robotic presence functions as a \textbf{semantic perturbation} of the evaluative pathway from moral salience to moral action. Presence is not a passive attribute; it is an informational act.

\noindent
This reading supports both the minimalist structure of the experimental design and its philosophical depth. By rejecting behavioural or dialogic criteria for moral influence, and grounding the analysis in semantic encoding at the LoA of the observer, we avoid naïve assumptions about interaction as a prerequisite for moral modulation. Presence, when correctly encoded, can reframe what is morally visible—prior to deliberation, and independent of interaction.

\subsection{Experimental design and Preliminary Results}

To investigate Question~\ref{q:robot-agent}, we implemented a controlled behavioural experiment~\cite{RosenthalRosnow2008,ReisJudd2000,Kazdin2017} derived from the classical \textit{Watching Eye} paradigm~\cite{HaleyFessler2005,Bateson2006,NettleEtAl2013,BatesonEtAl2013_EyesLittering,PfattheicherKeller2015,ContyGeorgeHietanen2016,DearDuttonFox2019}, in which prosocial behaviour is modulated by implicit cues of observation. Each participant was invited individually into a room under the pretext of completing a personality-study session in exchange for monetary compensation. Unbeknownst to them, the experimental environment contained a morally salient stimulus: a charity brochure displaying the photograph of a child requiring medical care. Decades of empirical work demonstrate that such stimuli reliably trigger prosocial dispositions by activating implicit monitoring and empathetic engagement~\cite{Haley2005,Conty2016}.

\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/home/francesco/Desktop/research/appunti/images/robot.png}
		\caption{Experimental condition: robot present.}
		\label{fig:robot}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/home/francesco/Desktop/research/appunti/images/control.png}
		\caption{Control condition: robot absent.}
		\label{fig:control}
	\end{subfigure}
	\caption{Top-down view of experimental vs. control configurations. Both settings retain identical spatial and visual layouts, isolating the variable of robotic presence as the only ontological difference.}
	\label{fig:experimental-topology}
\end{figure}


\noindent
Participants were randomly assigned to one of two conditions. In the \textbf{Control} condition, they completed the questionnaires alone. In the \textbf{Robot} condition, a humanoid NAO robot was placed in the room and operated in autonomous life mode. Although NAO emitted no speech and performed no task-relevant actions, it displayed minimal embodied behaviours—simulated breathing, subtle postural adjustments, and head-orientation responses triggered only by eye contact. These micro-cues are the minimal behavioural affordances known to activate or modulate the Watching Eye effect.

\noindent
After completing the questionnaires, each participant received £10 in £1 coins as compensation and encountered a voluntary donation opportunity. An opaque charity box (Operation Smile) was positioned near the exit. Participants could donate any subset of the coins. The total donation served as the primary dependent measure of prosocial behaviour.

\noindent
Initial results revealed a robust directional pattern: participants in the Robot condition donated substantially less than those in the Control condition. Furthermore, no meaningful between-group differences were found in personality profiles (Empathizing Quotient~\cite{BaronCohenWheelwright2004_EmpathyQuotient}, Systemizing Quotient~\cite{BaronCohenRichlerBisaryaGurunathanWheelwright2003_SystemizingQuotient}, Big Five Inventory~\cite{JohnDonahueKentle1991_BigFiveInventory}), ruling out trait-based confounds and strengthening the inference that robotic presence itself modulated the evaluative pathway underlying prosocial action.


\subsection{From Behavioural Setup to Evaluative Structure}

\noindent
In moral philosophy, action is frequently treated as the terminus of deliberation~\cite{Aristotle_nicomachean,Korsgaard1996,Anscombe1957}. Yet the present study concerns not the deliberative endpoint but the evaluative transformation that precedes it: the internal process by which morally salient cues are converted into behavioural output~\cite{Nussbaum2001,Korsgaard2009}. The experimental design above provides the behavioural substrate; what remains is to articulate the evaluative architecture through which robotic presence might exert its influence.

\noindent
Our explanatory focus therefore remains firmly on moral action—here, instantiated as voluntary donation—while acknowledging that salience, cognition, and interpretive modulation contribute to the inferential scaffolding that produces such action. This framing connects the experiment to the philosophical traditions of practical reasoning and to the neurocognitive models explored in Chapter 2.

Our aim is not to probe abstract normativity, but to determine whether artificial presence perturbs the transformation from moral appraisal to observable donation—a behavioural manifestation of deliberative judgement.

\noindent
Empirically, the experiment transposes the Watching Eye paradigm into a minimal social environment co-inhabited by a humanoid robot. Prior variants of the paradigm have relied on stylised pictorial stimuli or supernatural primes~\cite{Bateson2006,Shariff2007}. Our design replaces these with an embodied artificial presence whose ontological ambiguity is semantically potent while remaining behaviourally minimal.

\noindent
To formalise the transformation under investigation, we treat moral action not as a fixed trait but as the output of a cognitive–affective function integrating environmental cues, individual traits, and contextual structure. In philosophical terms, this is the practical realisation of moral salience; in psychological terms, it is the integration of cue perception, affective readiness, and situational inference.

\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] \;\neq\; \mathbb{E}[f(\Sigma)]
\]

Where:

\begin{itemize}
	\item $\Sigma$ is the morality-salient perceptual field (e.g., the Watching Eye stimulus),
	\item $\mathscr{R}$ is the synthetic co-presence, realised here by NAO,
	\item $f(\cdot)$ is the evaluative transformation mapping perceptual input to moral behaviour,
	\item $\mathbb{E}[f(\cdot)]$ denotes the expected behavioural output (donation magnitude).
\end{itemize}

\noindent
Read aloud, this expresses the hypothesis that:

\textbf{The expected outcome of moral behaviour changes when a humanoid robot is present within the perceptual–moral environment.}

\nextdiv
\begin{center}
	\nextstatement
	\begin{hypobox}{Evaluative Deformation Hypothesis}
		\label{hyp:evaluative_deformation}
		The expected outcome of moral behaviour, as computed through the evaluative process \( f \), is altered when the robot is present within the perceptual-moral environment.
	\end{hypobox}
\end{center}

\noindent
The conceptual shift from the initial research question to this first formal hypothesis is thus warranted by the structure of the experimental design. The question preserved conceptual openness—\textit{is robotic presence morally perturbative?} The hypothesis now expresses this inquiry in a form amenable to empirical adjudication, specifying how the evaluative transformation from moral cue to moral action may be deformed.

%% first stop at station.

\noindent
To make the structure of this transformation explicit, we can decompose the probability of a deviation in moral action into its component determinants:

\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]

\noindent
where:
\begin{itemize}
	\item \(\alpha_E\) encodes the environmental moral cue (here, the Watching Eye stimulus),
	\item \(\beta_C\) denotes the individual-level control variables (psychometric and demographic structure),
	\item \(\gamma_R\) represents the robotic presence as a perturbative affordance.
\end{itemize}

This expression can be read aloud as: \textit{The probability of a deviation in moral decision $(\delta_m)$  is a function of the environmental moral cue $(\alpha_E)$, the individual's psychological and demographic configuration $(\beta_C)$, and the presence of the robot $(\gamma_R)$.}

\noindent
That is, the probability of observing a change in moral behaviour is a function of: (i) the morally salient stimulus, (ii) the participant’s internal traits, and (iii) the synthetic presence that may refract, displace, or attenuate the evaluative process.

\noindent
This formalism captures the operative logic of the experimental design: moral action is not treated as an isolated datum, but as a context-sensitive transformation of moral salience into behaviour. The robotic presence is therefore not conceptualised as a behavioural actor but as a \textit{topological perturbation}—a variable that reframes the inferential lens through which moral cues are registered and converted into action.

\noindent
To understand the stakes of this perturbation, we must clarify what is meant by \textit{moral salience}. Across philosophical and psychological literatures, moral salience refers to the capacity of a situation, object, or agent to present itself as morally significant—i.e., to become an object of evaluative attention prior to explicit deliberation~\cite{Korsgaard2009,Nussbaum2001,Greene2001,Haidt2007,Moll2005}. It functions as a phenomenological filter: before the agent reasons, before the agent chooses, certain features of the environment appear as normatively charged. Within this framework, synthetic entities may perturb moral salience not by issuing commands or engaging in dialogue, but by reconfiguring what is foregrounded, what is suppressed, and what is affectively or normatively “seen” in the first place.

\noindent
This brings us to the ontological dimension of the hypothesis. The robot’s influence depends not on its computational sophistication but on its \textit{perceived ontology}: how observers intuitively classify the entity—as object, tool, quasi-agent, or socially charged companion. In this experiment, NAO’s embodied form, posture, gaze behaviours, and subtle animations evoke agent-like expectations without satisfying the criteria for full moral agency. This ambiguity is precisely what renders the robot a semantically potent perturbator within the moral field.

\begin{center}
	\nextstatement
	\begin{hypobox}{Synthetic Normativity of Moral Displacement}
		\label{hyp:synthetic_normativity}
		Synthetic presences, though devoid of sentience, may acquire \textit{normative affordances} by virtue of their perceived ontology. When situated within morality-salient environments, such presences may disrupt, refract, or displace the evaluative machinery through which moral judgments are ordinarily formed.
	\end{hypobox}
\end{center}

\noindent
This hypothesis extends beyond a narrow behavioural prediction; it asserts that robotic presence may alter the normative topology of the environment itself. The experiment is therefore not merely a test of prosocial output, but a constrained act of epistemic staging—a designed moral topology intended to probe whether the presence of $\mathscr{R}$ displaces or refracts the normative force of $\alpha_E$.

\noindent
The Watching Eye paradigm thereby becomes a conceptual instrument: not merely a psychological effect but a method for examining the structural elasticity of normative cognition in environments where human agents coexist with synthetic forms. What the study observes, therefore, is not simply differences in donation behaviour, but how the inferential architecture linking salience to action is modulated by synthetic co-presence. Generosity, in this framework, is not a trait but an emergent property of norm-sensitive evaluative systems embedded within a structured environment.

\noindent
This framing rejects simplified accounts that treat moral behaviour as transparent readouts of internal disposition. Instead, it positions moral action as the contingent result of cognitive–affective systems operating under topological deformation~\cite{Greene2002,Haidt2001,Fedyk2017}. Robotic presence, by virtue of its ontological ambiguity, functions as a refractive moral affordance: a structural condition that may attenuate or redirect the transformation of moral salience into action.

\fpcom{old content begins}

The term \textit{perceived ontology} refers to how observers intuitively classify an entity’s nature—whether as object, tool, agent, or something more ambiguous. In this context, it denotes how the humanoid robot is not treated merely as a machine, but as a presence with quasi-social or normatively loaded features. This perception does not require the attribution of full agency or sentience; rather, it is the robot’s embodied form, gaze behaviours, and passive co-presence that evoke moral expectations in the observer. Thus, the robot’s “perceived ontology” may perturb how moral salience is registered, filtered, or even displaced by human evaluative systems.

\fpcom{old content ends}

This is not an experiment in the narrow sense of causal testing. It is a constrained act of epistemic staging—a designed \textbf{moral topology} that probes whether the presence of \(\mathscr{R}\) displaces, diffuses, or refracts the normative force of \(\alpha_E\). Our aim is not simply to determine whether donations changes under robotic observation, but whether \(\mathscr{R}\) alters the internal topology of moral inference itself. In this light, the Watching Eye paradigm ceases to be a psychological curiosity and becomes an instrument of conceptual inquiry: a way of testing the structural elasticity of normative cognition in post-human social configurations.

What this study observes, therefore, is not simply what participants do under (staged) robotic observation, but how the inferential architecture of moral cognition is perturbed by synthetic presence. The robot, though devoid of agency, functions as a semiotic operator on the moral field—its presence refracts the salience of otherwise normative cues, modulating prosocial output through shifts in interpretive topology. We do not treat generosity as a readout of innate disposition, but as the \textit{emergent property of norm-sensitive evaluative systems embedded in structured environments}.

This framing \textbf{rejects} any simplistic account of moral behaviour as noise-free reflection of trait. Instead, we position moral action as the contingent result of \textit{cognitive-affective systems }operating under \textit{topological deformation}~\cite{Greene2002, Haidt2001, Fedyk2017}. In this view, robotic presence is not merely a contextual feature, but a morally refractive affordance that alters the mapping between cue and action.

Within this epistemological architecture, the following experiment tests the plausibility of a central hypothesis: that robotic presence—by virtue of its ontological ambiguity—can systematically attenuate the conversion of moral salience (see above for a definition) into action. It is this structured possibility, not merely behaviour, that the empirical sections to follow are designed to investigate.

\noindent
With this architecture in place, the subsequent sections examine how such deformation manifests empirically—first at the behavioural level, and then at the deeper structural level of trait–context interactions.

\section{Conceptualisation of the Experiment: Moral Decision-Making under Synthetic Presence}

Having articulated the evaluative architecture through which synthetic presence may perturb the transformation from moral salience to action, we now specify how this theoretical framework is instantiated empirically. The objective of this section is not merely to describe procedural steps, but to clarify the conceptual rationale that makes this experimental configuration an appropriate test of the inferential deformation thesis established above.

\noindent
To empirically examine whether the mere presence of a synthetic, non-agentic entity can alter the evaluative pathway underlying charitable behaviour, we embedded participants within a controlled, minimally structured moral choice scenario. Framed as a standard personality study, the procedure unobtrusively positioned each participant before an unannounced ethical decision. This preserved the epistemic opacity required for observing pre-reflective evaluative processes rather than self-presentational behaviour.

Each participant entered the experimental room alone and completed a series of psychometric measures—the Empathizing Quotient~\cite{Baron2002}, Systemizing Quotient~\cite{Baron2003}, and the Big Five Inventory~\cite{Rammstedt2007}. Completion of the questionnaires served a dual methodological purpose. First, it provided data for assessing whether trait dispositions modulated sensitivity to robotic presence, thereby enabling the analysis of trait–context interactions central to this chapter. Second, it supplied a plausible pretext for the experimental setting, ensuring that participants approached the environment without anticipating a moral evaluation.

Upon completion, participants received £10 in £1 coins as compensation. Before exiting, they encountered a latent moral choice: an opaque green charity box placed beside a prominently displayed image of a child requiring medical care. The image served as a Watching Eye stimulus—an established elicitor of prosocial behaviour through mechanisms of implicit monitoring, empathy, and reputation sensitivity~\cite{Haley2005,Conty2016}. Donation behaviour, performed privately and unobserved, constituted the operational measure of moral action.


\noindent
The central experimental manipulation divided participants into two conditions:

\begin{itemize}
	\setlength{\itemsep}{4pt}
	\item \textbf{Control Condition}: participants completed the task alone in a room containing only the morally salient cue.
	\item \textbf{Robot Condition}: the same room contained a humanoid NAO robot (Aldebaran Robotics) in ``autonomous life mode.'' Although behaviourally non-interactive, the robot exhibited minimal embodied cues—simulated breathing, micro-adjustments in posture, and reactive head orientation triggered exclusively by eye contact. These cues provided precisely the level of embodied salience required to activate or disrupt the cognitive mechanisms underlying the Watching Eye effect.
\end{itemize}

This configuration preserved identical spatial, visual, and procedural features across conditions; the sole ontological difference was the presence of the robot.

\noindent
In the experimental condition, a humanoid robot (NAO) was silently positioned in the space, operating in “autonomous life mode”: breathing rhythmically, subtly shifting posture, and responding to eye contact through reactive head movement — yet without speaking, interacting, or engaging in any directive behaviour. Importantly, participants had no prior knowledge of the robot’s presence, and the robot itself did not intervene in the task.

Importantly, participants were not warned about the robot in advance, and no verbal or task-relevant interaction occurred at any time. The robot therefore functioned as an \textit{epistemic perturbation}: a synthetic presence whose embodied form was salient yet behaviourally inert, occupying the ambiguous space between animate agent and object.

\noindent
The behavioural outcome was striking: participants in the Robot condition donated substantially less (mean £1.17) than participants in the Control condition (mean £1.89). No significant differences in personality profiles were observed between groups, ruling out trait imbalance and indicating that the observed attenuation of donation reflects a genuine displacement in the evaluative pathway rather than a sampling artefact. At a descriptive level, then, synthetic co-presence appears to weaken the moral force of the Watching Eye stimulus.


\vspace{1em}
\noindent
To understand why this effect is theoretically significant, we must clarify the status of \textit{moral decision-making} within this experimental architecture. Contrary to utilitarian models that construe donation as a form of preference optimisation (see \cref{chap:ethics_s}), our framing treats the decision to donate as an instantiation of \textit{moral salience attribution under epistemic opacity}. Participants do not know they are being observed; they do not know that donation behaviour is the dependent measure; and they do not know that synthetic presence is the variable of interest. What is revealed, therefore, is not explicit moral reasoning, but the \textit{implicit evaluative machinery} through which morally loaded cues gain—or fail to gain—behavioural traction.

The Watching Eye stimulus plays a critical role in this machinery. Anthropological and psychological research shows that images of eyes or children reliably elicit third-party moral concern via affective engagement and implicit audience effects~\cite{Bateson2006,NettleEtAl2013,ContyGeorgeHietanen2016}. Our design extends this paradigm by placing, alongside the Watching Eye cue, a humanoid robot whose ontological status is neither human nor ethically inert. NAO thus becomes an \textit{ontological anomalous agent}: a presence that possesses the perceptual affordances of agenthood without the behavioural or normative commitments of actual agency.

This motivates the following hypothesis, which articulates the expected deformation within the evaluative architecture:


\nextdiv
\begin{center}
	\nextstatement
	\begin{hypobox}{Synthetic Perturbation of Moral Inference}
	\label{hyp:synthetic_perturbation}
	The humanoid robot NAO does not function as a passive observer, but as a perturbative presence that refracts the transition from moral salience to prosocial action. Its ontological ambiguity displaces the affective-empathic cues that ordinarily support donation, thereby modulating the evaluative pathway by which moral stimuli gain behavioural expression.
	\end{hypobox}
\end{center}

\[
\mathscr{S} : \Sigma \xrightarrow{\;\mathscr{R}\;} \mathscr{D}
\]

where:
\begin{itemize}
	\item $\Sigma$ denotes the perceptual input space structured by morally salient cues (brochure, child’s eyes, spatial configuration),
	\item $\mathscr{R}$ denotes the synthetic robotic presence functioning as a perturbative modulator,
	\item $\mathscr{D}$ denotes the domain of observable moral decisions (monetary donation).
\end{itemize}

In control conditions, the transition $\Sigma \rightarrow \mathscr{D}$ proceeds without interference: the affective weight of moral cues is preserved and expressed through prosocial giving~\cite{Haley2005,Shariff2007}. In robotic conditions, by contrast, $\mathscr{R}$ deforms this mapping. It may displace empathic identification, dilute the salience of the Watching Eye cue, reshape the normative topology of the environment, or function as a cognitive decoy~\cite{Zlotowski2015}. Each interpretation bears distinct implications for the design of ethical robots and for understanding how humans recalibrate moral behaviour in the presence of synthetic others.

%%% NEW UPADATED CONENTE  N1
\subsection{Formalisation of Hypothesis and Experimental Logic}

The present experiment is best conceived not as a mechanistic probe into behavioral preferences, but as a structured perturbation within a normatively encoded cognitive system. Specifically, it seeks to investigate \textbf{how robotic presence modulates human moral decision-making} under conditions of minimal priming and perceptual constraint. Unlike traditional paradigms that treat prosociality as an output of deliberative utility calculus, the design employed here foregrounds the \textbf{pre-reflective inferential machinery} that converts perceptual-affective cues into morally salient behavior.

At its epistemic core, this experiment operates as a \textbf{perturbative test of moral salience transmission} — that is, whether a morally charged perceptual cue (e.g., the face of a child in need) is successfully converted into a prosocial behavioral output (monetary donation), and how that transmission is modulated, disrupted, or reframed by the passive presence of a \textbf{non-agentic but anthropomorphically encoded entity} (\ie, the NAO robot).

To formalize the interpretive structure of this transformation, let us denote:

\begin{itemize}
	\item $\Sigma$: the perceptual-affective input space (including the Watching Eye stimulus, spatial layout, and ambient cues)
	\item $\mathscr{R}$: robotic presence, ontologically positioned between artifact and agent
	\item $\mathscr{D}$: the moral decision space (observable as donation behavior)
\end{itemize}

The operative hypothesis can be expressed as a probabilistic modulation of expected moral output:

\[
\mathscr{R} \notin \Sigma \Rightarrow \mathbb{E}[f(\Sigma)] = D_{\text{prosocial}} \quad \text{(Control condition)}
\]
\[
\mathscr{R} \in \Sigma \Rightarrow \mathbb{E}[f(\Sigma \cup \mathscr{R})] = D_{\text{attenuated}}
\]
where:

\[D_{\text{attenuated}} < D_{\text{prosocial}} \quad \text{(Robot condition)}\]

Here, the notation $\mathbb{E}[f(\cdot)]$ denotes the \textbf{expected behavioral output} of the cognitive-affective system under a given set of environmental conditions. The function $f(\cdot)$ captures the internal inferential transformation by which perceptual-affective cues—such as the Watching Eye stimulus—are mapped onto discrete moral actions, in this case, the act of anonymous donation. Crucially, the expectation operator $\mathbb{E}[\cdot]$ signals that we are not describing a deterministic relation, but rather the \textit{aggregate tendency} across a psychologically heterogeneous population. It reflects the statistical structure of the behavioral response field rather than individual-level causality.

\subsection{Methodological Design: Inferring Moral Perturbation through Controlled Artificial Co-presence}

To regard an experimental setting as a generator of knowledge, rather than a mere data collection routine, demands that its internal architecture be epistemically justifiable and ontologically transparent. In this respect, every stage of the experimental method presented here is conceived not simply as procedural necessity, but as epistemic filtering: a sequence of deliberate constraints designed to isolate latent variables within the perceptual and normative landscape of the participant.

At its core, the experimental logic operationalises the following proposition:

% Corpo del documento
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]

where:
\begin{itemize}
	\item $\delta_m$ denotes a deviation in moral decision (quantified as donation behavior),
	\item $\alpha_E$ represents environmental moral cues (Watching Eye),
	\item $\beta_C$ indexes control factors (psychometric variables, demographic traits),
	\item and $\gamma_R$ captures the effect of robotic presence.
\end{itemize}

The experimental setting is thus a structured interrogation of whether $\gamma_R \neq 0$ under conditions in which $\alpha_E$ and $\beta_C$ are held constant or accounted for. If confirmed, such deviation would instantiate a moral displacement: a case in which a non-sentient co-agent modulates human ethical output without any explicit instruction, coercion, or intervention. \fpincom{add link to relevant hypothesis and check condition "not zero"}

The following experimental procedure was implemented to ensure maximal control over environmental affordances while preserving participant naivety concerning the true moral dimension under investigation.

%%% NEW CONTENT N6
\subsection{Formalisation of the Experimental Logic}

Having established the conceptual and epistemic rationale for investigating robotic co-presence as a perturbative variable, we now formalise the internal logic of the experimental design. The present experiment is not conceived as a mechanistic probe into stable behavioural preferences, but as a \textit{structured perturbation} applied to a normatively encoded cognitive system. Its aim is to examine how a minimally interactive synthetic entity modulates the evaluative transformation through which morally salient cues become behaviourally instantiated.

\noindent

Unlike paradigms that construe prosociality as the downstream product of deliberative utility calculus, our design foregrounds the \textbf{pre-reflective inferential machinery} responsible for converting perceptual–affective moral cues into action. In this frame, moral behaviour is not treated as a direct expression of preference or disposition, but as the output of a cognitive–affective transformation whose parameters may be refracted by the presence of an ontologically ambiguous entity.

\noindent
At its epistemic core, the experiment operates as a \textbf{perturbative test of moral salience transmission}: whether the moral charge embedded in a Watching Eye stimulus is preserved, attenuated, or reframed when a synthetic presence occupies the same perceptual field. The robot deployed in this study—non-agentic, behaviourally minimal, but anthropomorphically encoded—functions precisely as such a perturbative variable.

\noindent
To make this structure explicit, let us denote:

\begin{itemize}
	\item $\Sigma$: the perceptual--affective input space (Watching Eye stimulus, spatial layout, ambient cues),
	\item $\mathscr{R}$: the robotic presence, ontologically positioned between artefact and agent,
	\item $\mathscr{D}$: the moral decision space, operationalised as monetary donation.
\end{itemize}

The operative hypothesis concerning the effect of robotic presence can be expressed as a modulation of expected moral output:

\[
\mathscr{R} \notin \Sigma \Rightarrow \mathbb{E}[f(\Sigma)] = D_{\text{prosocial}} \quad \text{(Control condition)}
\]
\[
\mathscr{R} \in \Sigma \Rightarrow \mathbb{E}[f(\Sigma \cup \mathscr{R})] = D_{\text{attenuated}} \quad \text{(Robot condition)}
\]

with the expected attenuation constraint:

\[
D_{\text{attenuated}} < D_{\text{prosocial}}.
\]


\noindent
Here, $\mathbb{E}[f(\cdot)]$ denotes the \textbf{expected behavioural output} of a cognitive system embedded within a particular perceptual–normative configuration. The evaluative function $f(\cdot)$ captures the internal inferential process by which morally salient cues—such as the image of the child beneficiary—are mapped onto the act of anonymous donation. The use of the expectation operator signals that this relation is \textit{statistical rather than deterministic}, reflecting the aggregate structure of a psychologically heterogeneous population. The experiment thus examines whether the presence of $\mathscr{R}$ shifts the distribution of moral output at the population level, not whether it dictates individual choices.

\subsection{Methodological Architecture: Inferring Moral Perturbation through Structured Artificial Co-presence}

To regard an experiment as a generator of epistemic insight rather than a mere data collection mechanism, its procedural structure must be internally justified and ontologically transparent. The methodological architecture adopted here is therefore not a set of neutral steps, but a sequence of \textit{epistemic filters}: constraints designed to isolate the variables that may participate in the evaluative transformation from moral cue to moral action.

\noindent
At the heart of this design lies the formal proposition:

\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]

\noindent
In experimental terms, the logic is straightforward: the design isolates the contribution of $\gamma_R$ by holding $\alpha_E$ constant across conditions and by measuring (and statistically controlling for) $\beta_C$. The aim is to determine whether $\gamma_R \neq 0$ in a model of the form above; that is, whether robotic presence produces a measurable displacement in the mapping from moral salience to action.

If confirmed, such a displacement constitutes a case of \textit{moral perturbation}: a condition under which a non-sentient co-present entity modifies the behavioural expression of moral evaluation without issuing instructions, engaging in dialogue, or exerting coercive influence. This is precisely the kind of phenomenon the inferential-deformation framework predicts and which the following empirical sections examine in detail.

\noindent
The procedure implementing this logic was designed to exert maximal control over environmental affordances while preserving participant naivety concerning the moral dimension under investigation. Each stage of the method thus serves an epistemic purpose: (i) to stabilise the perceptual field, (ii) to constrain interpretive context, and (iii) to create a topology in which the presence of a minimally animated humanoid robot may act as a perturbative affordance on the evaluative pathway from salience to action.

\subsection{Procedural Architecture of the Experimental Protocol}

The formal model introduced above establishes the inferential structure through which moral salience, individual traits, and robotic presence jointly determine observable moral behaviour. We now describe the procedural realisation of this structure. What follows is not a purely logistical account, but a methodological articulation designed to preserve the epistemic integrity of the transformation expressed by 
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R),
\]
ensuring that each component is instantiated under controlled, conceptually coherent conditions.

Participants were recruited through two parallel channels: internal advertisements within the School of Computing Science at the University of Glasgow and via the Psychology subject pool. Eligibility criteria included (i) a minimum age of 17 years, (ii) British nationality, verified upon arrival, and (iii) where applicable, exclusion of Computing Science students from the Psychology pool to prevent sampling overlap (see \cref{subsec:participants} for full demographic detail). 

Assignment to conditions (\emph{Control} vs.\ \emph{Robot}) occurred \textbf{prior to arrival} using a simple randomisation procedure. Pre-arrival assignment ensured allocation concealment and prevented anticipatory contamination of moral cue salience—particularly important given the subtlety of Watching Eye effects and the epistemic opacity required by the design.

\protocolheaderNoCounter{Experimental Design for Watching-Eye Priming under Robotic Displacement}
\label{prot:watching_eye_design}
\begin{protocolbox}
	\begin{enumerate}[label=\textbf{Stage \arabic*:}, leftmargin=2cm]
		
		\item \textbf{Arrival and Initial Framing}
		
		Upon arrival, participants were individually welcomed and informed—\emph{exclusively in writing}—that the study concerned personality measurement in a representative sample of the local population. No reference was made to charitable donation, moral choice, robotic presence, or observational manipulation. This framing was essential for maintaining \textbf{epistemic opacity} with respect to the true dependent variable.
		
		\item \textbf{Environmental Exposure and Moral-Salience Priming}
		
		Participants entered an isolated experimental room configured according to their assigned condition. In both conditions, a large poster depicting a child beneficiary from a medical charity (\emph{Operation Smile}) was affixed to the wall directly facing the participant. This image served as the Watching Eye stimulus ($\alpha_E$), providing a latent reputational cue that has been shown to activate prosocial tendencies under minimal prompting.
		
		In the \emph{Robot Condition}, a SoftBank Robotics \textbf{NAO} robot was placed passively in the room, configured in \textit{autonomous life mode}. In this mode, NAO exhibits subtle embodied cues: simulated breathing, minimal postural adjustments, and reactive head orientation triggered \emph{only} upon direct eye contact. These micro-movements instantiate the perturbative variable $\gamma_R$, furnishing a perceptually salient but behaviourally minimal form of co-presence.
		
		\item \textbf{Completion of Psychometric Instruments}
		
		Participants completed three psychometric questionnaires:
		\begin{itemize}
			\item \textbf{Empathizing Quotient (EQ)}~\cite{Baron2002}, indexing affective resonance.
			\item \textbf{Systemizing Quotient (SQ)}~\cite{Baron2003}, indexing rule-based cognitive preference.
			\item \textbf{Big Five Inventory-10 (BFI-10)}~\cite{Rammstedt2007}, capturing broad personality traits.
		\end{itemize}
		
		The inclusion of these instruments was mandated by the model component $\beta_C$, enabling quantification and later statistical control of individual differences. These measures prevent dispositional variance from masking or misattributing the perturbative effect of $\gamma_R$ on the evaluative conversion from $\alpha_E$ to $\delta_m$.
		
		\item \textbf{Monetary Compensation and Moral Decision Opportunity}
		
		Participants were then given £10 in ten individual £1 coins and were invited—subtly and without coercion—to donate any portion anonymously to the same children’s medical charity. A green opaque box was positioned in the room to receive donations. The anonymity of this setup was essential for preserving $\delta_m$ as a genuine moral action rather than a strategic or reputationally calibrated response.
		
		\item \textbf{Exit and Data Collection}
		
		Participants exited the room individually. The experimenter then recorded the amount donated, retrieved completed questionnaires, and anonymised all identifiers for analysis.
	\end{enumerate}
\end{protocolbox}

\noindent
This five-stage protocol was designed to instantiate a \textbf{high-fidelity operationalisation} of the theoretical constructs previously formalised. Each procedural element serves an epistemic function: concealing the evaluative dimension of the task, fixing the moral cue environment, isolating the perturbative role of robotic presence, and quantifying individual-level control factors. Thus, the experiment functions not merely as a behavioural test, but as a carefully engineered epistemic probe into how environmental moral cues, synthetic co-presence, and trait structure jointly modulate the inferential pathway from salience to action.



%%% END NEW CONTENT N6


\subsection{Participants as Agents under Constraint}
\label{subsec:participants}

Seventy-three participants were recruited under the condition of epistemic \textit{naïveté}—a design choice intended to replicate the pre-reflective nature of many moral decisions in everyday life. That is, participants were never informed of the donation component in advance, nor were they given any cues that their decisions would be measured along ethical dimensions. This design choice aligns with the methodological imperative in experimental moral psychology to preserve the authenticity of affective-moral judgments (Greene et al., 2001; Haidt, 2001; Fedyk, 2017).

Each participant received a standard monetary compensation of £10, delivered in ten individual £1 coins. This choice is not incidental. The granular structure of the payment serves to increase the opportunity for \textit{moral modulation}; a single-note payment might discourage partial donations, thereby reducing the variance of observed prosocial behavior. Granularity here is not merely a technical concern—it is a moral affordance strategy (cf. Hutchins, 1995; Clark, 1997).

Demographically, participants were drawn from two sources:

\fpcom{Here better use the version from the article since it appears to be more agile and readable in terms of style and language.}

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\rightskip1.5cm
	\leftskip1cm
	\item[1. ] Computing Science undergraduates (n=30), and
	\item[2. ] Psychology subject-pool participants (n=43) via the University of Glasgow’s Institute of Neuroscience and Psychology.
\end{itemize}

Both sources were filtered through inclusion criteria to ensure homogeneity in nationality (British), legal adulthood (17+), and naïveté to the experimental purpose. This careful curation was essential to reduce background moral-cultural noise (cf. Henrich et al., 2010), and to ensure that any signal detected in the data could be confidently attributed to contextual rather than dispositional variance.

\subsection{Experimental Conditions: The Robotic Displacement Hypothesis}

With the procedural and formal architecture in place, we now turn to the specific configuration of the two experimental conditions. Participants were randomly assigned to one of two environments, each identical in spatial layout, moral cue structure, and procedural flow, differing solely in the presence or absence of a humanoid robot:

\begin{itemize}
	\item \textbf{Control Condition}: Watching-Eye brochure present; no robot in the room.
	\item \textbf{Robot Condition}: Watching-Eye brochure present; NAO robot in autonomous life mode.
\end{itemize}

The \textbf{Robot Condition} was engineered with conceptual precision. The NAO unit did not speak, gesture, or initiate interaction. Instead, it exhibited only two minimal behavioural affordances intrinsic to its \textit{autonomous life mode}:

\begin{itemize}
	\item \textbf{Simulated breathing}, providing low-level embodied realism and anthropomorphic lifelikeness;
	\item \textbf{Reactive head orientation}, activated strictly when participants made eye contact.
\end{itemize}

These micro-behaviours were not incidental: they were selected to place the robot within the narrow band of \emph{ontological ambiguity} that is central to the displacement hypothesis. A robot that is fully inert collapses into the category of object and loses the semiotic texture necessary for perturbation. Conversely, a robot that engages in overt interaction risks confounding prosocial responses through intentional attributions or social norm compliance. 

The configuration employed here is deliberately poised between these extremes. NAO is activated enough to be \emph{socially legible}, yet withdrawn enough to remain \emph{epistemically opaque}. In Floridi’s terminology, the robot is an artefact whose \emph{LoA-encoded features} (face, posture, micro-movement) render it morally salient despite the absence of moral agency~\cite{Floridi2013, FloridiSanders2004}. At this operative LoA, its status is neither neutral nor agentive but semiotically charged: a presence that presents itself as potentially intentional, without fulfilling the criteria for genuine agency.  

Within this framework, NAO occupies the role of what Coeckelbergh~\cite{Coeckelbergh2010} and Złotowski et al.~\cite{Zlotowski2015} describe as a \textit{moral appearance operator}: an entity whose embodied features trigger interpersonal expectations even in the absence of genuine communicative exchange. In our design, the robot becomes a \textbf{norm deflector}: it does not issue commands, but it may reconfigure the evaluative bandwidth through which the Watching-Eye stimulus is interpreted.

This constitutes the core empirical content of the \textbf{Robotic Displacement Hypothesis}: the notion that a minimally animated synthetic co-presence can refract the inferential pathway from moral cue to moral action, attenuating prosocial behaviour without altering the underlying moral reasoning architecture.

\subsubsection*{Demographic Equivalence and Inferential Symmetry}

To ensure that any observed behavioural differences could be attributed to the perturbative influence of $\mathscr{R}$ rather than demographic imbalance, we conducted inferential tests across gender, age, and educational background.  

The results were unequivocal:

\begin{itemize}
	\item A chi-squared test on gender distribution yielded no significant difference across conditions ($p = 1.00$, after False Discovery Rate correction);
	\item An independent-samples t-test comparing mean age revealed no significant difference ($p = 1.00$, after FDR correction);
	\item A chi-squared test for academic background similarly found no difference ($p = 1.00$, after FDR correction).
\end{itemize}

The use of the Benjamini--Hochberg FDR correction removes the risk of spurious equivalence arising from multiple comparisons, strengthening the inferential legitimacy of these findings.

In epistemic terms, these results justify a critical methodological inference:  
\textbf{the experimental groups are demographically symmetrical}.  
Thus, subsequent divergences in donation behaviour cannot plausibly be attributed to demographic artefacts or sampling asymmetries. Instead, they can be modelled as emergent properties of the experimental manipulation—the presence or absence of $\mathscr{R}$ within an otherwise constant moral field.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/demographic_balance_table.pdf}
	\caption{Demographic balance tests across experimental conditions. The table reports the original and FDR-corrected p-values for comparisons of gender, age, and educational background. No significant differences were detected after correction, supporting the assumption of demographic equivalence between groups.}
	\label{tab:dem_balance}
\end{table}

These demographic controls complete the methodological foundations for the inferential analyses that follow. With demographic equivalence established, with $\alpha_E$ held constant, and with $\beta_C$ explicitly measured, the subsequent behavioural differences can be attributed—within the constraints of the design—to the semiotic, perceptual, and normative perturbation introduced by the robotic presence $\mathscr{R}$.

% =====================================================================
% PARTIAL CONCLUSION: HYPOTHESES, FORMALISM, AND INTERIM FINDINGS
% =====================================================================

\subsection{Interim Evaluation of the Hypotheses and Formal Framework}

Having established the experimental architecture and its accompanying mathematical formalism, we may now assess the status of the hypotheses introduced thus far. Rather than presenting these hypotheses as isolated propositions, they form an interconnected explanatory sequence: each articulates a different dimension of the same underlying phenomenon—the deformation of the evaluative pathway through which moral salience becomes behaviour.

The first hypothesis, the \textit{Evaluative Deformation Hypothesis}, posits that the expected outcome of moral behaviour—formalised as the transformation \(f\) of perceptual–moral cues—changes when a humanoid robot is added to the environment. This is the empirical backbone of the inquiry. The observed attenuation in donation behaviour across conditions is consistent with this expectation. Accordingly, this hypothesis is \textbf{retained} as an operative empirical claim.

The second hypothesis, the \textit{Synthetic Normativity of Moral Displacement}, gives conceptual depth to this empirical deformation. It claims that synthetic entities may acquire \textit{normative affordances} by virtue of their perceived ontology, even in the absence of sentience or interaction. This hypothesis is not behaviourally testable in a strict sense; its role is philosophical and structural. It explains why a silent, non-interactive robot can nonetheless exert normative influence on human evaluative cognition. It remains \textbf{retained} as a conceptual grounding for the empirical findings.

The third hypothesis, the \textit{Synthetic Perturbation of Moral Inference}, specifies the mechanism underlying H1. It suggests that the robot refracts the evaluative transition from moral salience to prosocial action, acting not as a social partner but as a perturbative operator within the cognitive ecology. The behavioural attenuation observed in the Robot condition accords with this mechanistic interpretation. Thus, this hypothesis is also \textbf{retained} and will guide the subsequent modelling of trait–context interactions.

The conjunction of these three hypotheses forms a coherent interpretive arc: H1 isolates the empirical signature of deformation; H2 explains its ontological possibility; H3 articulates the inferential pathway through which such deformation is instantiated. No hypothesis introduced thus far is contradicted by the current evidence, and no revision is warranted at this stage.

% ---------------------------------------------------------------------
\subsubsection*{Status of the Mathematical Formalism}

The mathematical apparatus introduced earlier has likewise played a substantive role in structuring both the empirical reasoning and the interpretive constraints of the study. Three components have been especially operative:

\paragraph{(a) The evaluative transformation function \(f(\cdot)\).}
This function encodes the cognitive–affective transformation through which perceptual cues become moral action.  
\textbf{Contribution so far:} it formalises why the presence of a non-interactive robot can affect behaviour despite the absence of communication, directive cues, or explicit social engagement. It embodies the central locus of deformation identified in the hypotheses above.

\paragraph{(b) Expected behavioural distributions \(\mathbb{E}[f(\Sigma)]\) vs.\ \(\mathbb{E}[f(\Sigma \cup \mathscr{R})]\).}
This construct expresses the empirical contrast between the Control and Robot conditions.  
\textbf{Contribution so far:} it provides a principled mathematical representation of the observed attenuation pattern. The behavioural findings align with the inequality
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] < \mathbb{E}[f(\Sigma)],
\]
thus supporting the retention of the Evaluative Deformation Hypothesis.

\paragraph{(c) The tripartite decomposition}
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R).
\]
This expression separates environmental cues (\(\alpha_E\)), dispositional factors (\(\beta_C\)), and robotic presence (\(\gamma_R\)).  
\textbf{Contribution so far:} it justifies the inclusion of psychometric instruments and demographic balance tests. It shows that attenuated prosociality cannot be meaningfully interpreted without jointly considering individual traits and the perturbative effect of robotic presence.

Together, these three formal components ensure that the empirical observations are not treated as purely behavioural regularities but as the surface expressions of a structured evaluative system undergoing controlled perturbation.

% ---------------------------------------------------------------------

\subsection{Interim Conclusion to Question~\ref{q:robot-agent}}

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Partial Conclusion to Question~\ref{q:robot-agent}]
		The behavioural evidence gathered thus far indicates that the silent co-presence of a humanoid robot systematically attenuates prosocial donation, despite the absence of communication, instruction, or interaction. This attenuation supports the plausibility of evaluative deformation: the robot perturbs the inferential transformation from moral salience to moral action. The philosophical hypothesis concerning synthetic normativity explains why such perturbation is possible, while the mechanistic hypothesis concerning moral inference explains how it is instantiated. The role of individual traits, and the deeper structure of trait–context interactions, will be examined in the sections that follow.
	\end{tcolorbox}
\end{center}

\noindent
In summary, the evidence to this point allows us to affirm that robotic co-presence modifies the evaluative conditions under which morally salient cues become behaviourally actionable. The three retained hypotheses together provide the conceptual, ontological, and mechanistic scaffolding for interpreting this modification. Further analyses will determine how these perturbations scale across heterogeneous psychological profiles and how robust the displacement effect remains under refined statistical scrutiny.


\subsection{Preprocessing the Moral Field: Semiotic Modulation and Ontological Symmetry}

Importantly, the robotic presence $\mathscr{R}$ is not modelled as an agent that exerts influence through interaction or instruction, but as a \textbf{semiotic modulator}: an ontologically ambiguous presence that perturbs the interpretive field in which moral cues operate. Within this framework, the observed attenuation of prosocial behaviour should not be interpreted as a direct suppression of empathy \textit{per se}, but as the result of a structural reconfiguration in what may be called the \textbf{normative encoding schema}: the internal representational system by which moral salience is assigned, weighted, and transmitted within a perceptual environment.

The introduction of $\mathscr{R}$ modifies the topology of this schema, shifting the inferential weight carried by otherwise salient moral signals. The Watching Eye cue, ordinarily a strong generator of prosocial behaviour, is thus refracted through a newly configured semiotic landscape—one in which an embodied but non-agentic entity complicates the attribution of moral relevance and potentially displaces reputational concern.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/conditions.pdf}
	\caption{Experimental conditions are behaviorally and procedurally identical, differing only in robotic presence.}
	\label{tab:experimental_conditions}
\end{table}

Both conditions were engineered to be \textbf{epistemically symmetrical}, ensuring that any observed deviation in moral behaviour can be attributed exclusively to the ontological modulation introduced by $\mathscr{R}$. The symmetry is not merely procedural but conceptual: it guarantees that the moral field differs only in the presence or absence of a semiotically potent synthetic form.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/variables.pdf}
	\caption{Measured variables and psychometric constructs used in inferential modelling of moral behaviour.}
	\label{tab:key_variables}
\end{table}

This formal and operational framework allows us to treat the experiment as a constrained instantiation of a more general epistemic function: namely, how minimally expressive artificial agents reshape the \textbf{moral topology} of a decision-making environment by altering the interpretive affordances of its cues.

\statementheader{Question}{Ontological Integrity of the Dataset}
\label{question:data_structuring}
\begin{center}
	\begin{questionbox}[label={q:data-prep}]{Data structuring}
		\textbf{What is required of the data at this stage?}  
		How can the raw dataset be transformed into a semantically coherent and mathematically compatible structure—one that preserves the normative architecture of the experiment and enables defensible inferences about moral behaviour?
	\end{questionbox}
\end{center}
Before any inferential operation can be meaningfully performed, the dataset must be rendered analytically legible and ontologically stable. At this foundational stage, our objective was not to extract patterns or test hypotheses, but to establish the \textbf{semantic integrity} and \textbf{computational viability} of the data matrix as a structured representation of moral decision-making. The transformation of moral action into analysable form is itself an epistemic act: the construction of a space in which behaviour can be interrogated without distorting the normative structure from which it emerges.

To this end, a series of principled data transformations were applied:

\begin{itemize}
	\item \textbf{Variable normalisation}: lowercase conversion and string trimming to eliminate syntactic artefacts and ensure referential transparency.
	\item \textbf{Binary encoding of moral action}: creation of the variable \texttt{donated\_anything}, capturing whether participants donated at all. This enables both continuous and categorical modelling of prosocial behaviour.
	\item \textbf{Numerical encoding of condition}: creation of \texttt{condition\_bin} ($0=\text{Control}$, $1=\text{Robot}$), allowing direct integration into regression-based models.
	\item \textbf{Verification of categorical coherence}: ensuring semantic alignment for fields such as \texttt{gender} and \texttt{group} to eliminate latent structural imbalances.
\end{itemize}

These procedures were not arbitrary conveniences but \textbf{ontological prerequisites}. The dataset comprises scalar, ordinal, and nominal variables, each governed by distinct inferential affordances. Treating them as interchangeable would collapse the analytic structure of the experiment into incoherence, misrepresenting the cognitive architecture it aims to probe.

Importantly, the dataset’s scale (\(N \approx 70\)) allows a rare balance: small enough for manual audit, yet large enough to require principled automation. The transformations performed operate precisely at this interface, upholding both semantic fidelity and computational tractability.

\vspace{0.3cm}

The dataset was then cleaned and preprocessed for inferential modelling. Variable names were standardised, \texttt{donated\_anything} was constructed, and \texttt{condition\_bin} was encoded. Descriptive statistics revealed no major distributional anomalies across demographic or psychometric variables, supporting the assumption of epistemic symmetry between groups and reinforcing the inference that the perturbation introduced by $\mathscr{R}$ operates primarily at the interpretive rather than dispositional level.

Figures~\ref{fig:age_distribution_by_group} and~\ref{fig:donation_distribution_by_condition} visually corroborate this reading: age distributions show no demographic divergence, while donation distributions reveal the predicted attenuation under robotic co-presence. The unified visual palette of the plots maintains stylistic continuity with the thesis’s typographic aesthetic, reinforcing the epistemic unity of the chapter’s representational forms.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/age_distribution_by_group.png}
	\caption{Age distribution across experimental conditions. Histogram representation confirms no major between-group demographic divergence.}
	\label{fig:age_distribution_by_group}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_distribution_by_condition.png}
	\caption{Distribution of donation behaviour by condition. Violin plot representation visualizes the heavier tail in the robot group, supporting the hypothesized interpretive perturbation.}
	\label{fig:donation_distribution_by_condition}
\end{figure}

%%% END NEW CONTEN N2
%%% =======================
%%%  NEW CONTENT N3 — REVISED
%%% =======================

\subsection{Preliminary Descriptive Patterns: Indications of Inferential Displacement}

\noindent
The initial descriptive statistics presented in Table 6.4 below offers a first empirical glimpse into the behavioural topology of the experiment. Consistent with the theoretical expectation that robotic presence $\mathscr{R}$ functions as an interpretive refractor rather than a neutral co-presence, the mean donation in the \textit{Control} condition (£1.89) exceeds that of the \textit{Robot} condition (£1.17). 

Although superficially modest, this divergence is conceptually aligned with the proposed displacement mechanism: if $\mathscr{R}$ attenuates the inferential weight of morally salient cues, then the perceptual–affective force of the charity stimulus ($\alpha_E$) should translate into reduced behavioural output. What the descriptive statistics therefore index is not merely a numerical contrast, but a preliminary deformation in the evaluative mapping from moral cue to prosocial act.

\noindent
Beyond donation behaviour, several secondary variables exhibit patterned differences: the Control group reports slightly higher Empathizing Quotient scores (M = 45.94 vs.\ 42.82) and higher Openness to Experience (M = 1.86 vs.\ 1.32). The Robot group, by contrast, is marginally older on average and shows increased Systemizing Quotient scores. While none of these contrasts are yet statistically decisive, they signal structured heterogeneity in cognitive–affective profiles that may later serve as moderators in the inferential analysis.

These preliminary divergences should be read cautiously. At this stage, they are \textit{exploratory markers} rather than inferential claims. Their value lies not in establishing differences, but in helping to delineate the psychological architecture through which robotic presence may exert its perturbative influence.

\begin{table}[H]
	\label{tab:descriptive-stats}
	\centering
	\includegraphics[width=\textwidth]{tables/descriptive_highlights_table.pdf}
	\caption{Summary of central tendencies for key behavioural and psychometric variables. The Robot condition shows numerically lower donation amounts and empathizing scores, suggesting potential attenuation effects of passive robotic presence.}
\end{table}

% FROM HERE
%%% =======================
%%%  NEW CONTENT N4 — REVISED
%%% =======================

\subsection{Inferential Assessment of Attenuation: Behavioural Evidence for Perturbation}
\label{subsec:inferential_attentuation}

\noindent
Having established the structural integrity of the dataset and the epistemic symmetry of the experimental groups, we now turn to the first inferential evaluation of whether the presence of the humanoid robot $\mathscr{R}$ modulates prosocial donation behaviour. This analysis directly bears on the \textit{Evaluative Deformation Hypothesis} introduced earlier (see Hypothesis~\ref{hyp:evaluative_deformation}), which predicts that the expected behavioural output $\mathbb{E}[f(\Sigma \cup \mathscr{R})]$ will diverge from $\mathbb{E}[f(\Sigma)]$ under otherwise identical environmental conditions.

\noindent
A chi-squared test on aggregated donation totals revealed a statistically significant difference across conditions ($\chi^2 = 4.25$, $p = .039$). Although modest in magnitude, this result provides preliminary support for the claim that robotic presence exerts a measurable perturbative influence at the level of group-level moral output.

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Conclusion: Aggregate Attenuation of Prosocial Output]
		At the aggregate level, participants exposed to the humanoid robot donated less overall than those in the Control condition, indicating a measurable attenuation in prosocial behavioural output under synthetic co-presence.
	\end{tcolorbox}
\end{center}

\noindent
It is important to emphasise the conceptual modesty of this conclusion. The inference concerns \emph{behavioural outcomes}, not motivational states: it does not license any direct claim about reduced empathy, diminished altruism, or altered moral character. A richer ethical interpretation of the donation act will be developed subsequently in the dedicated chapter on charitable giving and moral agency.

\noindent
To complement the chi-squared test, a Mann–Whitney U test was applied to the full distribution of donation amounts. This test did not reach statistical significance ($U = 777$, $p = .194$), indicating that although the group means diverge, the individual-level distributions remain substantially overlapping. This distributional overlap suggests that the perturbative influence of $\mathscr{R}$ is not uniformly expressed across participants, but may depend on latent cognitive–affective structures captured in the trait vector $\beta_C$.

\noindent
A nonparametric bootstrap estimate of the mean donation difference ($\Delta M = £0.71$) reinforced the directional pattern, yet its 95\% confidence interval included zero (CI = [–£0.33, £1.79]). This epistemic indeterminacy is itself theoretically consistent with the overarching framework: the robot functions not as a deterministic suppressor of moral behaviour, but as a \textbf{subtle modulator of the normative field}, whose influence becomes most visible at the level of aggregated tendencies rather than individual-level deterministic shifts.

\noindent
Taken together, these results support the philosophical characterisation of $\mathscr{R}$ as a \textit{semiotic perturbator}—an entity whose ontological ambiguity refracts the inferential trajectory from moral salience to behavioural output. The attenuation observed at the aggregate level, coupled with the distributional overlap at the individual level, points toward a heterogeneous responsiveness within the participant population, motivating the more refined modelling strategies introduced in the sections to follow. \textit{In particular, the potential interaction between robotic presence $\gamma_R$ and individual traits $\beta_C$ warrants further investigation through regression modelling, interaction analyses, and Bayesian estimation procedures.}

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/statistical_tests_table.pdf}
	\caption{Inferential comparisons of donation behaviour across conditions. The chi-squared test identifies a significant difference in aggregate donation totals, while the Mann–Whitney U test and bootstrapped mean difference indicate substantial distributional overlap and a diffuse, heterogeneous perturbative effect.}
	\label{tab:statistical_tests}
\end{table}

\vspace{0.3cm}
\noindent
Inferential statistical testing corroborates the initial descriptive trends, albeit with nuanced gradations in evidential strength. As shown above, a chi-squared test applied to the aggregate donation sums across experimental conditions yielded a statistically significant divergence ($\chi^2 = 4.25$, $p = .039$), in line with the Evaluative Deformation Hypothesis that the presence of a synthetic co-presence $\mathscr{R}$ deforms the expected behavioural output of the evaluative function $f$. 

However, this aggregate significance attenuates when the full distributions of donation amounts are examined. A Mann–Whitney U test did not detect a reliable shift in the overall donation distributions ($U = 777$, $p = .194$), indicating substantial overlap in individual-level variability across the Control and Robot conditions. A bootstrapped estimation of the mean difference in donation ($\Delta M = £0.71$) reinforced the directional pattern, but the 95\% confidence interval (CI = [–£0.33, £1.79]) encompassed the null, \textit{thereby underscoring the epistemic fragility and structural subtlety of the observed effect.}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_effect_by_condition.png}
	\caption{Mean donation amounts by experimental condition, with 95\% bootstrapped confidence intervals. Participants in the Control condition donated more on average than those in the Robot condition, aligning with the hypothesis that robotic presence attenuates the inferential mapping from moral salience to prosocial action. The overlapping confidence intervals highlight substantial individual-level variability and the probabilistic nature of the perturbation.}
	\label{fig:donation_effect_by_condition}
\end{figure}
\noindent

\noindent
Beyond establishing that a statistically detectable attenuation emerges at the level of group aggregates, it is epistemically important to quantify the magnitude of this perturbation. The effect is not only small in absolute monetary terms, but also structurally modest in inferential terms: it does not collapse the transformation from moral salience to action, but appears to bend it. The following analyses therefore introduce both parametric and nonparametric effect size metrics, in order to characterise how strongly the robotic co-presence $\gamma_R$ modulates the evaluative function $f(\alpha_E, \beta_C, \gamma_R)$ and how this modulation scales across heterogeneous configurations of the trait vector $\beta_C$.

% ---------------------------------------------------------------------
\subsection{Interim Evaluation of the Hypotheses and Formal Framework}

\noindent
At this stage, the behavioural and inferential results allow for a provisional assessment of the hypotheses and the formal apparatus introduced earlier. These are not isolated claims, but components of a single explanatory architecture that tracks how moral salience is transformed into observable behaviour under synthetic co-presence.

\noindent
The \textbf{Evaluative Deformation Hypothesis} (Hypothesis~\ref{hyp:evaluative_deformation} p.~\pageref{hyp:evaluative_deformation}) asserts that the expected outcome of moral behaviour, as computed by the evaluative transformation $f$, is altered when the robot is present within the perceptual–moral environment. The chi-squared analysis of aggregate donation totals, together with the bootstrapped mean difference, supports this claim: the pattern
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] < \mathbb{E}[f(\Sigma)]
\]
is empirically instantiated, albeit modestly and with heterogeneous individual-level expression. This hypothesis is therefore \textbf{retained} as an operative empirical statement about the deformation of group-level moral output under robotic co-presence.

\medskip
\noindent
The \textbf{Synthetic Normativity of Moral Displacement} hypothesis (Hypothesis~\ref{hyp:synthetic_normativity}, p.~\pageref{hyp:synthetic_normativity}) provides the ontological and conceptual groundwork for interpreting this deformation. It claims that synthetic presences, though devoid of sentience, may acquire normative affordances by virtue of their perceived ontology. The present evidence neither confirms nor disconfirms this hypothesis in a narrow statistical sense; rather, it shows that a non-interactive yet semantically rich artefact, \textbf{positioned at the appropriate Level of Abstraction}, can exert measurable influence on prosocial behaviour without issuing commands, arguments, or reasons. This is exactly the pattern one would expect if normative affordances were grounded in informational presentation at a given LoA, rather than in intrinsic moral status. The hypothesis is thus \textbf{retained} as the principal conceptual lens through which the behavioural results are interpreted.

\noindent
The \textbf{Synthetic Perturbation of Moral Inference} hypothesis (Hypothesis~\ref{hyp:synthetic_perturbation},  p.~\pageref{hyp:synthetic_perturbation}) specifies the mechanism connecting the previous two: the robot does not merely co-occur with lowered donations; it perturbs the inferential transition from moral salience to prosocial action by refracting the affective–empathic cues that would otherwise support donation behaviour. The combined pattern of (i) significant aggregate attenuation, (ii) overlapping individual-level distributions, and (iii) non-trivial yet fragile effect sizes is coherent with this mechanistic reading: the evaluative mapping is not destroyed, but \textbf{its topology is altered}. This hypothesis is therefore \textbf{retained} as a working account of how the deformation is instantiated at the level of moral inference.
\noindent
In sum, all three hypotheses remain live and mutually reinforcing:

\begin{itemize}
	\item Hypothesis~\ref{hyp:evaluative_deformation}, (p.~\pageref{hyp:evaluative_deformation}) identifies the \emph{empirical signature} of deformation at the level of expected behaviour.
	\item Hypothesis~\ref{hyp:synthetic_normativity}, (p.~\pageref{hyp:synthetic_normativity}) explains the \emph{ontological possibility} of such deformation within Floridi’s informationalist framework and its Levels of Abstraction.
	\item Hypothesis~\ref{hyp:synthetic_perturbation}, (p.~\pageref{hyp:synthetic_perturbation}) articulates the \emph{inferential pathway} through which robotic presence reshapes the transition from moral salience to action.
\end{itemize}

No hypothesis introduced thus far is contradicted by the current evidence; rather, the data suggest that the deformation is subtle, probabilistic, and mediated—exactly the kind of effect one would expect when perturbation occurs at the level of semantic encoding rather than at the level of explicit instruction or coercion.

% ---------------------------------------------------------------------
\subsubsection{Status of the Mathematical Formalism}

\noindent
The mathematical formalism developed earlier has not remained abstract scaffolding; it has directly structured both the analysis and the interpretation of the behavioural findings.

\paragraph{(a) The evaluative transformation function \(f(\cdot)\).}
This function encodes the cognitive–affective transformation through which perceptual–moral cues are converted into behavioural output.  
\textbf{Contribution so far:} it clarifies why a non-interactive, minimal-behaviour robot can nonetheless influence donation behaviour: what is being perturbed is not the presence of reasons or arguments, but the transformation process itself.

\paragraph{(b) Expected behavioural distributions \(\mathbb{E}[f(\Sigma)]\) vs.\ \(\mathbb{E}[f(\Sigma \cup \mathscr{R})]\).}
These expectations formalise the contrast between Control and Robot conditions.  
\textbf{Contribution so far:} they provide a principled representation of the observed attenuation pattern, making it possible to express the empirical result as an inequality over expected moral output, rather than as an ad hoc numerical difference.

\paragraph{(c) The tripartite decomposition}
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R).
\]
This expression disaggregates environmental cues (\(\alpha_E\)), dispositional factors (\(\beta_C\)), and robotic presence (\(\gamma_R\)).  
\textbf{Contribution so far:} it justifies the joint consideration of (i) the Watching Eye stimulus, (ii) psychometric traits and demographics, and (iii) robotic co-presence as distinct yet interacting contributors to moral behaviour. The current behavioural results speak primarily to the \(\gamma_R\) component, while leaving open the possibility that its effect is modulated by structured configurations of \(\beta_C\)—a possibility that will be examined through regression and interaction models in the analyses that follow.

Together, these formal elements ensure that the experiment is not interpreted as a mere collection of empirical regularities, but as a controlled perturbation of a well-specified evaluative system situated at a particular Level of Abstraction.

% ---------------------------------------------------------------------
\subsection{Interim Conclusion to Question~\ref{q:robot-agent}}

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Partial Conclusion to Question~\ref{q:robot-agent}]
		The behavioural evidence obtained thus far indicates that the silent co-presence of a humanoid robot, operating with minimal but perceptually salient behavioural affordances, systematically attenuates aggregate donation behaviour under a Watching Eye paradigm. This attenuation is modest, probabilistic, and heterogeneously distributed across individuals, but it is empirically detectable and statistically non-trivial.
		
		\medskip
		Within the formal and philosophical architecture developed in this chapter, these findings support the plausibility of \emph{evaluative deformation}: the robot perturbs the inferential transformation from morally salient cues to observable moral action. Floridi’s Levels of Abstraction framework explains why such perturbation is possible—because the robot’s \emph{perceived ontology} and informational encoding render it normatively relevant at the operative LoA, even in the absence of sentience or interaction. The Synthetic Perturbation of Moral Inference hypothesis then specifies \emph{how} this relevance is instantiated, by refracting the evaluative pathway rather than overriding it.
		
		
		\medskip
		The role of individual traits, represented by the vector \(\beta_C\), and their interaction with robotic presence \(\gamma_R\), remains an open and theoretically salient question. The next sections therefore move from aggregate contrasts to trait–context modelling, in order to determine whether moral displacement is uniformly distributed or preferentially expressed in specific psychological profiles.
	\end{tcolorbox}
\end{center}

\noindent
In summary, the results to this point justify the claim that robotic co-presence modifies the evaluative conditions under which morally salient cues become behaviourally actionable, in a manner that is fully consistent with the informational and topological commitments of the Floridian framework. The retained hypotheses and formalism together provide the conceptual, ontological, and mechanistic scaffolding for the more fine-grained analyses that follow.

%%% NEW CONTENT N7
Beyond establishing the statistical significance of the observed differences, it is epistemically imperative to quantify the magnitude of behavioral perturbation induced by robotic presence. The following analyses introduce both parametric and nonparametric effect size metrics to characterise the structural modulation of moral decision-making.

\subsection{Quantification of Behavioural Modulation: Parametric and Nonparametric Effect Sizes}

\noindent
To complement the inferential analyses reported above, the magnitude of the behavioural modulation induced by robotic co-presence was quantified using both parametric and nonparametric effect size metrics. Whereas significance tests assess whether an effect is detectable relative to sampling variability, effect sizes characterise the \textit{structural amplitude} of the perturbation introduced by $\mathscr{R}$. In keeping with the dual statistical and philosophical commitments of this chapter, we employ metrics that capture both standardised differences in central tendency and ordinal differences in the full behavioural distribution.

\noindent
Two complementary measures were selected:

\begin{itemize}
	\item \textbf{Cohen’s $d$} — a parametric index of standardised mean difference;
	\item \textbf{Cliff’s $\Delta$} — a nonparametric ordinal effect size quantifying the probability that a randomly selected individual in one group donates more or less than a randomly selected individual in the other.
\end{itemize}

These metrics jointly assess whether robotic presence reshapes the evaluative output distribution in a manner consistent with the deformation posited in the preceding hypotheses.

\vspace{0.3cm}
\noindent\textbf{Cohen’s \( d \):}
\[
d = \frac{\bar{x}_1 - \bar{x}_2}{s_p}
\quad\text{where}\quad
s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}
\]

\noindent
Where:
\begin{itemize}
	\item $\bar{x}_1, \bar{x}_2$ = group means (Control, Robot),
	\item $s_1, s_2$ = group standard deviations,
	\item $n_1, n_2$ = group sizes.
\end{itemize}

\medskip
\noindent\textbf{Cliff’s Delta \( \Delta \):}
\[
\Delta = 
\frac{\#(x>y) - \#(x<y)}{n_x n_y}
\]
Where:
\begin{itemize}
	\item $\#(x>y)$ counts all pairwise comparisons where a Control donation exceeds a Robot donation,
	\item $\#(x<y)$ counts the inverse.
\end{itemize}

\vspace{0.3cm}
\noindent
The empirical results yield:
\[
d \approx 0.30, \qquad \Delta \approx 0.20.
\]

\noindent
Both indices fall within the range typically interpreted as \textit{small to modest} behavioural modulation. Yet as argued earlier, the theoretical significance of these values does not lie in their magnitude alone, but in the fact that they instantiate a reproducible \textit{directional deformation} of the evaluative transformation $f(\cdot)$ under controlled manipulation of $\mathscr{R}$.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_density_by_condition.png}
	\caption{Kernel density estimates of donation distributions across conditions. The Control group exhibits higher central mass and a heavier rightward extension relative to the Robot group, consistent with a directional attenuation of high-value prosocial acts in the presence of the synthetic co-presence $\mathscr{R}$.}
	\label{fig:donation_density}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_mean_with_se.png}
	\caption{Mean donation amounts with standard error bars by condition. The Control group donates more on average (£1.89) than the Robot group (£1.17), corroborating the hypothesis that robotic presence modulates—rather than eliminates—the evaluative pathway from moral salience to action.}
	\label{fig:mean_donation}
\end{figure}

\noindent
Taken together, these effect sizes indicate that robotic presence does not suppress moral action in any deterministic sense. Instead, it exerts a statistically coherent but modest refractive influence: it alters the \textit{amplitude} with which moral salience transitions into overt prosocial behaviour, without erasing the underlying evaluative architecture. The moral field remains operative, but its expression becomes probabilistically dampened under synthetic co-presence.

This pattern resonates with the broader theoretical framing developed throughout this chapter. Within the informational ontology of Floridi’s Levels of Abstraction, the robot functions as a \textit{semantic perturbator}: its perceived ontology introduces a shift in the evaluative topology at the LoA where moral cues acquire salience. 

\noindent
The effect sizes observed here are therefore best interpreted not as behavioural weakness, but as evidence that moral displacement operates as a \textit{graded transformation} within the evaluative function $f$, rather than a \textit{binary switch} between generosity and withholding.

\noindent
To capture this insight with conceptual precision, the following conclusion is offered:


\nextdiv
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Conclusion: Amplitude of Moral Refraction]
		 \label{conc:amplitude_moral_refactor}
		Synthetic co-presence does not operate as a binary suppressor of moral behaviour but as a \textbf{probabilistic refractor} that modulates both the amplitude and direction of evaluative processing. Rather than displacing the normative orientation of the agent, the robotic presence perturbs the strength with which morally salient cues are transduced into prosocial action, yielding a graded attenuation consistent with its ambiguous ontological encoding at the operative Level of Abstraction.
	\end{tcolorbox}
\end{center}

\noindent
This conclusion follows coherently from the statistical, philosophical, and formal analyses developed thus far: robotic presence acts not as a moral veto, but as a structurally subtle deformation of the evaluative mapping from salience to action.

\section{Dispositional Baseline: Big Five Personality Traits Across Conditions}

\noindent
A foundational requirement for attributing the observed attenuation of prosocial behaviour to the presence of the humanoid robot is the establishment of \emph{dispositional equivalence} between the two experimental groups. If participants in the Robot condition were, for example, systematically lower in Agreeableness or Empathizing, then differences in donation behaviour could be trivially explained by trait imbalance rather than by the perturbative effect of $\mathscr{R}$. The question addressed in this section is therefore epistemically prior to all subsequent modelling:

\begin{quote}
	\textit{Do the Big Five personality traits differ between the Control and Robot conditions, and thus constitute a potential confound for interpreting the displacement of prosocial behaviour?}
\end{quote}

\subsection{Between-Condition Differences in Big Five Personality Traits}

\noindent
To examine this possibility, we compared Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism between conditions using the Mann--Whitney $U$ test. This analytic choice follows directly from the structure of the data: Big Five scores are bounded, ordinally coded psychometric measures, exhibit mild skew, and are measured with $N \approx 70$, a regime in which parametric assumptions cannot be guaranteed. The Mann–Whitney framework therefore offers the correct inferential granularity: it is distribution-free, variance-robust, and sensitive to monotonic rather than strictly linear differences.

Because examining five traits entails five simultaneous hypothesis tests, we applied the Benjamini–Hochberg False Discovery Rate (FDR) correction---a principled safeguard against Type~I inflation when multiple, correlated psychological constructs are assessed in parallel. This aligns with the epistemic architecture of the experiment: the question is not whether \emph{any} uncorrected difference might be found, but whether a \emph{reliable} dispositional asymmetry exists that could invalidate the interpretation of robotic presence as the causal perturbator.

\noindent
The results are unambiguous. After FDR correction, none of the Big Five traits differ significantly between the Control and Robot groups. Directional tendencies (e.g., slightly higher Openness and Agreeableness in the Control condition) fail to approach corrected thresholds, and visual inspection of the distributions reveals substantial overlap across all five traits.



\noindent
This permits a crucial inferential step: \textbf{the two groups can be treated as dispositionally equivalent}. The attenuation in donation behaviour cannot be attributed to pre-existing personality differences but must instead be interpreted as a perturbation arising from the ontological and semiotic properties of $\mathscr{R}$ itself.

\begin{figure}[H]
	\centering
	\begin{minipage}{0.98\linewidth}
		\centering
		\includegraphics[width=\linewidth]{new_plots/per_trait/bigfive_row_kde.png}
		\caption{Kernel density estimates for each Big Five trait across experimental conditions, demonstrating substantial distributional overlap.}
		\label{fig:bigfive_kde}
	\end{minipage}
\end{figure}


\subsection{Predictive and Moderating Roles of Big Five Traits}

\noindent
Establishing between-group equivalence does not settle a further question of theoretical importance:

\begin{quote}
	\textit{Even if the groups are balanced, do the Big Five traits nonetheless predict donation behaviour, or modulate the displacement effect of robotic presence?}
\end{quote}

To address the predictive dimension, we computed Spearman rank correlations between each Big Five trait and donation amount. Spearman’s $\rho$ is epistemically suited to this dataset: donation values are zero-inflated, non-normal, and bounded, while the trait scores arise from ordinal psychometric instruments that do not guarantee interval-level structure. Scatterplots with monotonic regression overlays were inspected for nonlinear tendencies that numeric coefficients might conceal.

For the moderation question, interaction models of the form
\[
\text{donation} \sim \text{condition} \times \text{trait}
\]
were estimated. This is the correct operationalisation of the theoretical claim that synthetic presence may act as a \textit{moral refractor}: an entity whose semiotic and ontological ambiguity differentially perturbs evaluative processing depending on the agent’s dispositional architecture.


\noindent
The findings are striking in their restraint. None of the Big Five traits significantly predict donation magnitude, nor do they moderate the difference between Control and Robot conditions. The behavioural divergence remains visible at the aggregate level, but its amplitude is not amplified or diminished at low versus high levels of any trait. The displacement effect of $\mathscr{R}$ is therefore \textbf{not trait-specific within the Big Five taxonomy}. 

\begin{figure}[H]
	\centering
	\begin{minipage}{0.98\linewidth}
		\centering
		\includegraphics[width=\linewidth]{new_plots/per_trait/bigfive_row_scatter.png}
		\caption{Scatter plots with fitted regression lines for each Big Five trait against donation amount. 
			Each panel displays individual participant scores alongside a smoothed linear trend. 
			No clear predictive relationships emerge, reinforcing the conclusion that the Big Five traits do not meaningfully predict prosocial donation within this experimental context.}
		\label{fig:bigfive_scatter}
	\end{minipage}
\end{figure}

\subsection{Interpretive Synthesis}

\noindent
These results yield a theoretically consequential conclusion: \emph{conventional trait psychology does not capture the dispositional dimensions along which synthetic presence modulates moral behaviour}. This does not imply that personality is irrelevant—indeed, our clustering analysis reveals precisely the latent dispositional regimes that matter—but rather that the Big Five, as a coarse-grained taxonomy, operates at a LoA too abstract to register the fine structure of cognitive–affective ecologies through which $\gamma_R$ refracts moral salience.

In other words, robotic presence perturbs moral action at a layer beneath the Big Five: a layer where traits combine into \emph{latent evaluative topologies}, not scalar predictors. This is why the Big Five show no predictive or moderating power, while the cluster-derived ecologies—Emotionally Reactive, Prosocial–Empathic, Analytical–Structured—display precisely the differential moral susceptibility that the Big Five cannot resolve.

These analyses therefore perform an indispensable gatekeeping role in the chapter’s argumentative arc: they clear the dispositional ground, justify the move toward structural trait models, and reinforce the interpretation of NAO’s presence as an ontologically driven perturbation rather than a byproduct of trait imbalance.

\noindent
Taken together, these findings compel a decisive interpretive transition. The Big Five analysis demonstrates that the classical trait taxonomy---as a coarse, high-level behavioural abstraction---is insufficiently granular to register the finer cognitive--affective structures through which robotic presence $\mathscr{R}$ exerts its perturbative force. In Floridi’s terms, the Big Five operate at a Level of Abstraction too distant from the operative informational interface at which moral salience is encoded, refracted, or displaced. Their scalar nature masks the latent relational geometries among traits that constitute an individual's evaluative topology. Consequently, the null results obtained here are not theoretically disappointing but theoretically clarifying: they reveal that dispositional factors relevant to moral modulation do not reside in isolated trait magnitudes, but in the \emph{configuration space} formed by their interaction.

\noindent
This insight aligns seamlessly with the ontological reading of NAO’s presence developed throughout this chapter. If $\mathscr{R}$ functions as an ambiguous semantic body---a synthetic agent whose minimal behavioural expressivity is nonetheless morally charged---then its impact is unlikely to map onto additive trait scores. Instead, it should refract through the structural organisation of cognitive--affective dispositions: the latent ecologies that position each participant differently relative to the moral field and its salient cues. The absence of main effects or trait-by-condition interactions within the Big Five framework thus strengthens, rather than weakens, the overarching argument. It demonstrates that the robot’s influence does not depend on conventional personality differences, but on deeper evaluative architectures that the Big Five only partially and indirectly approximate.

\noindent
This justificatory work also prepares the conceptual ground for the analyses that follow. Having ruled out personality imbalance as a confound and shown that the Big Five do not predict or moderate prosocial behaviour, the inquiry must now shift to a more structurally sensitive representation of $\beta_C$. The question becomes not whether traits matter, but \emph{how they combine} into latent dispositions that modulate the flow of moral salience under conditions of ontological ambiguity. It is precisely this transition---from scalar traits to configurational ecologies---that motivates the move toward clustering and latent-structure modelling in the next section.



\subsection{Latent Trait Structures and Individual Modulation of Moral Perturbation}

\noindent
The analyses conducted thus far establish that robotic co-presence $\mathscr{R}$ exerts a modest but coherent attenuation of prosocial donation at the aggregate level. However, such group-level effects leave open a critical question: \textit{is this perturbation uniformly distributed across individuals, or is it contingent upon underlying cognitive–affective structures encoded in} $\beta_C$? If robotic presence operates as a semantic perturbator at the operative Level of Abstraction, then its impact may be differentially refracted through distinct personality configurations rather than applied homogeneously to all participants.

To investigate this possibility, we moved beyond treating individual differences as simple additive covariates and instead modelled them as \textbf{latent psychological regimes}. Concretely, participants were clustered according to their standardised psychometric profiles, thereby refining the $\beta_C$ term in the operational model
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]
from a mere vector of trait scores into a set of structurally defined personality constellations.

Seven variables were included in the initial psychometric space: Empathizing, Systemizing, Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Each participant’s score vector was $z$-standardised and submitted to Principal Component Analysis (PCA). Two orthogonal principal components were retained, capturing the most informative axes of variance in the trait space while reducing dimensionality and mitigating redundancy among correlated measures.

The resulting two-dimensional representation was then subjected to $k$-means clustering with $k = 3$, yielding three psychologically interpretable personality clusters. These clusters were visualised in the reduced PCA space to assess structural separability and interpretative coherence (Figure~\ref{fig:personality-clusters-pca}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{new_plots/personality_clusters_pca.png}
	\caption{Participants clustered in PCA-reduced psychometric space, coloured by cluster identity and shaped by experimental condition. The clustering reveals three latent personality regimes, each representing a distinct cognitive–affective configuration encoded in $\beta_C$.}
	\label{fig:personality-clusters-pca}
\end{figure}

\noindent
This procedure provides a structural lens through which to examine the interaction between moral perturbation and trait-defined cognitive–affective style. Rather than treating traits as independent predictors, the clustering approach models them as \textit{emergent regimes} that may stabilise or destabilise the inferential transmission of moral salience under the perturbation introduced by $\gamma_R$.

\medskip
\noindent
The choice of $k = 3$ was not arbitrary. It was justified through a combination of quantitative and conceptual criteria. First, the within-cluster sum of squares (WCSS) was inspected across candidate values of $k$, revealing a clear elbow in the inertia curve at $k = 3$. This elbow indicates a point of diminishing returns: additional clusters beyond three yield only marginal improvements in within-cluster homogeneity, at the cost of increased model complexity and reduced interpretability.

Second, the silhouette coefficient was computed for multiple candidate $k$ values. While a local maximum in the silhouette profile was observed at $k = 9$, this peak is best interpreted as an artefact of over-partitioning a relatively small dataset. At such resolutions, high silhouette values often reflect the tightness of very small clusters rather than psychologically meaningful structure. In contrast, $k = 3$ corresponds both to the elbow in the inertia curve and to clusters of interpretable size and composition (Figure 6.8).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{new_plots/cluster_elbow_silhouette.png}
	\caption{Elbow plot of within-cluster sum of squares (left axis) and silhouette coefficients (right axis) across candidate values of $k$. The elbow at $k = 3$ and interpretable silhouette profile support the selection of three clusters as a parsimonious and psychologically meaningful solution.}
	\label{fig:cluster_elbow_silhouette}
\end{figure}

\noindent
From a conceptual standpoint, the $k = 3$ solution aligns with the broader theoretical expectation that robotic perturbation may be differentially refracted through a small number of discrete cognitive–affective configurations, each constituting a distinct normative filter through which $\alpha_E$ and $\gamma_R$ are jointly interpreted. Accordingly, we retain $k = 3$ as the optimal clustering solution on both methodological and interpretive grounds.

\noindent
Cluster-specific analyses of donation behaviour reveal heterogeneous responses to moral cues across these latent regimes (Figure~\ref{fig:donation-by-cluster}). In one cluster (Cluster 1), the presence of the robot appears to strongly attenuate donation amounts, whereas in the remaining clusters (Clusters 0 and 2), the difference between Control and Robot conditions is negligible or comparatively weak. Inspection of the underlying psychometric profiles suggests that \textit{Cluster 1 is characterised by relatively higher systemising} and lower empathising scores, in line with a cognitive–affective style that privileges structural or rule-based processing over affective resonance.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{new_plots/donation_by_cluster_and_condition.png}
	\caption{Mean donation amount by experimental condition within each personality cluster, derived from $k$-means analysis on psychometric trait profiles. Error bars represent standard deviation. Cluster 1 shows a marked attenuation of donation under robotic presence, whereas Clusters 0 and 2 exhibit minimal or modest differences. This pattern suggests that the perturbative effect of $\gamma_R$ is contingent upon latent cognitive–affective regimes encoded in $\beta_C$.}
	\label{fig:donation-by-cluster}
\end{figure}

\subsection{Psychometric Interpretation and Semantic Labelling of Latent Personality Clusters}

\noindent
The identification of three latent personality clusters through PCA reduction and $k$-means partitioning raises a conceptually prior question: \textit{What psychological architectures do these clusters instantiate, and how do these architectures illuminate the differential moral impact of robotic presence?} Clustering partitions participants into structurally coherent groups, but it does not automatically disclose the dispositional logic underpinning those partitions. This section therefore provides the interpretive grounding required for integrating the latent trait configurations with the moral-topological framework developed throughout the chapter.

From an epistemic standpoint, interpretation requires a return from the abstract PCA space to the original psychometric dimensions. The unscaled cluster centroids perform this bridging function: they reveal each cluster’s mean position along Empathizing, Systemizing, and the Big Five dimensions, thereby reconstituting the mathematical solution in explicitly psychological terms. Radar plots offer a visual gestalt of these relational structures, and when presented jointly, they highlight the contrastive organisation of personality ecologies more effectively than isolated representations.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.32\linewidth]{new_plots/cluster_0_radar.png}
	\includegraphics[width=0.32\linewidth]{new_plots/cluster_1_radar.png}
	\includegraphics[width=0.32\linewidth]{new_plots/cluster_2_radar.png}
	\caption{Comparative radar profiles of the three latent personality ecologies.  
		\textbf{Emotionally Reactive / Low-Structure Profile} (left): elevated Neuroticism with reduced Conscientiousness and Systemizing.  
		\textbf{Prosocial–Empathic / Warm–Sociable Profile} (centre): high Openness, Extraversion, Agreeableness, and Empathizing.  
		\textbf{Analytical–Structured / High-Systemizing Profile} (right): high Systemizing and Conscientiousness with lower Empathizing.}
	\label{fig:radar_three_panel}
\end{figure}

\noindent
\textbf{Emotionally Reactive / Low-Structure Profile.}  
This ecology, corresponding to the first extracted cluster, is characterised by elevated Neuroticism, reduced Conscientiousness, and diminished Systemizing, complemented by moderate values across Openness, Extraversion, and Agreeableness. This constellation reflects an \textit{affectively volatile and structurally diffuse} cognitive ecology. Individuals belonging to this regime likely experience greater internal variability, weaker evaluative stability, and heightened sensitivity to subtle environmental perturbations. Within the moral-topological framework of this chapter, their evaluative surface is best described as \textit{loosely stabilised}: moral cues propagate through a field with low structural coherence, making contextual distortions—such as the ontological ambiguity of a subtly animated robot—especially salient.

\vspace{0.4cm}
\noindent
\textbf{Prosocial–Empathic / Warm–Sociable Profile.}  
This ecology exhibits high Openness, Extraversion, Agreeableness, and Empathizing, forming a \textit{warm, sociable, affectively attuned, exploratory} personality architecture. These participants show the canonical prosocial configuration in moral psychology: they are dispositionally inclined toward interpersonal resonance and empathic attunement. Under classical Watching Eye frameworks, this ecological type would be expected to amplify donation behaviour in the presence of a moral-salience stimulus such as the charity poster. Their attenuation under robotic presence therefore becomes diagnostic: it indicates that $\gamma_R$ may refract or dilute empathic pathways, moderating the evaluative transition from moral salience to prosocial output precisely where that transition would otherwise be strongest.

\vspace{0.4cm}
\noindent
\textbf{Analytical–Structured / High-Systemizing Profile.}  
This ecology is defined by high Systemizing, high Conscientiousness, and comparatively reduced Empathizing—a \textit{rule-based, analytical, orderly} psychological regime. These individuals privilege structural clarity and formal coherence over affective immediacy. Moral stimuli embedded in implicit or ambiguous contexts—such as the subtle moral affordance of the child-beneficiary poster—may exert weaker motivational force. Likewise, the ontological ambiguity of the robot is likely processed as a structurally neutral environmental feature rather than a socially meaningful presence. In LoA terms, this group operates with a higher abstraction threshold: cues must be explicitly norm-encoded to penetrate their evaluative architecture.

\vspace{0.5cm}
\noindent
\textbf{Interpretive Integration.}  
These semantic labels are not optional descriptive flourishes; they are \textit{epistemically necessary} for making the cluster solution theoretically legible. Without them, the clustering results would remain mathematically partitioned yet psychologically opaque. By identifying one ecology as affectively volatile, one as prosocial–empathic, and one as analytical–structured, we obtain a principled account of how moral salience interacts with latent cognitive architectures. This alignment allows the latent ecologies to interface directly with earlier behavioural findings: attenuation of prosocial donation is most pronounced where empathic pathways should be strongest (the Prosocial–Empathic profile), weak in the Analytical–Structured group, and context-dependent in the Emotionally Reactive profile.

\vspace{0.5cm}
\noindent
\textbf{Connection to Floridi’s Levels of Abstraction.}  
At the operative LoA of each participant, these ecologies function as distinct \textit{semantic filters}. The Prosocial–Empathic type foregrounds affective cues, the Analytical–Structured type foregrounds structural clarity, and the Emotionally Reactive type foregrounds affective volatility. The presence of a synthetic agent—whose ontology is ambiguous, neither fully inert nor fully social—thus perturbs a different aspect of the evaluative interface for each ecology. This explains why the moral perturbation induced by $\gamma_R$ is neither global nor homogeneous, but topologically refracted through the architecture of each ecological type.
 
This interpretive reconstruction provides the conceptual bridge between latent personality architecture and the heterogeneous behavioural effects documented earlier. It reveals three structurally distinct evaluative ecologies, each with its own susceptibility profile to moral salience and robotic ambiguity. Their integration into the broader analytic narrative elucidates why attenuation under robotic presence is concentrated in the Prosocial–Empathic group, weak in the Analytical–Structured group, and variable in the Emotionally Reactive group. This interpretive foundation prepares the ground for the Bayesian estimation framework developed in the next section, where uncertainty, heterogeneity, and differential susceptibility are modelled as epistemic gradients.

\noindent
These findings deepen the interpretation of robotic presence $\gamma_R$ as a \textit{contextually realised} perturbator rather than a uniformly applied suppressor. The robot’s influence is not globally fixed, but \textbf{contingently instantiated through latent cognitive structures}. The same synthetic presence that weakens the evaluative transmission from moral salience to action in one psychological regime may have negligible impact in another. In this sense, the clustering analysis gives empirical shape to the idea that the evaluative function $f(\alpha_E, \beta_C, \gamma_R)$ is structurally modulated by $\beta_C$ rather than merely shifted in its intercept.

\medskip
\noindent
This motivates the following conceptual conclusion, which summarises the trait-contingent character of the observed perturbation:

\nextdiv
\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Conclusion: Contingent Structure of Cognitive Modulation]
		\label{conc:clustered_moral_refraction}
		The moral impact of robotic presence is not globally uniform but emerges through contingent interactions between artificial co-presence and latent psychological regimes. Personality clustering shows that synthetic moral perturbation is structurally modulated: its amplitude and behavioural expression are refracted through cognitive–affective configurations that define the subject’s interpretive topology. In Floridian terms, $\gamma_R$ does not act upon a neutral substrate, but upon agents whose operative Levels of Abstraction are themselves shaped by trait-dependent informational filters.
	\end{tcolorbox}
\end{center}

\noindent
Interpreted through the lens of the three latent personality ecologies identified earlier, this conclusion acquires a further layer of structural specificity. The \textit{Prosocial–Empathic / Warm–Sociable} profile is the regime in which the refractive impact of $\gamma_R$ is most pronounced: here, empathic pathways are ordinarily the most fluid, and thus the ontological ambiguity of the robotic presence most effectively perturbs the evaluative mapping from salience to action. By contrast, the \textit{Analytical–Structured / High-Systemizing} profile exhibits a comparatively rigid interpretive topology—one in which affective cues carry diminished epistemic weight and where the robot is recoded as a structurally neutral environmental feature rather than a moral affordance. The \textit{Emotionally Reactive / Low-Structure} profile occupies an intermediate position: its evaluative landscape is marked by volatility, rendering it sensitive to contextual shifts, yet not in a manner that yields a stable pattern of attenuation. Together, these ecologies demonstrate that the deformation induced by $\gamma_R$ is not a global displacement but a trait-contingent refractor: the moral field bends most sharply where empathic vectors dominate, remains nearly inert where systemizing structure prevails, and oscillates unpredictably in affectively unstable regimes. In this sense, the clusters make explicit the topological heterogeneity of the human moral interface, revealing that \textit{robotic presence engages different Levels of Abstraction depending on the cognitive–affective filters through which it is perceived.}


\subsection{Interim Synthesis: Moral Attenuation, Topological Deformation, and Trait-Contingent Modulation}

\noindent
The analyses completed thus far allow us to articulate a coherent intermediate synthesis of the empirical and conceptual structure of the experiment. Two principal results have emerged with consistency:

\begin{enumerate}
	\item[(1)] A measurable attenuation of prosocial donation under robotic co-presence (\cref{conc:amplitude_moral_refactor});  
	\item[(2)] A structurally heterogeneous, cluster-contingent modulation of this attenuation (\cref{conc:clustered_moral_refraction}).
\end{enumerate}

\noindent
Together, these findings show that robotic presence $\gamma_R$ does not function as a uniform suppressor of moral action, but as a \textbf{probabilistic refractor} that perturbs the inferential trajectory by which moral salience is transformed into behaviour. The robot’s effect is both \textit{topologically distributed}—reshaping the evaluative field at the aggregate level—and \textit{psychologically conditional}, emerging only within specific latent cognitive–affective regimes encoded in $\beta_C$.

\bigskip

% ----------------------
% Hypotheses
% ----------------------

\subsubsection{Status of the Hypotheses}

\paragraph{H1. Evaluative Deformation Hypothesis.}
\emph{The expected outcome of moral behaviour, formalised by the transformation $f(\cdot)$, is altered when a humanoid robot is present within the perceptual–moral environment.}

\textbf{Status: Retained.}  
The aggregate attenuation in donation amounts supports this claim. All empirical analyses converge on the conclusion that $\mathbb{E}[f(\Sigma \cup \mathscr{R})] < \mathbb{E}[f(\Sigma)]$.

\paragraph{H2. Synthetic Normativity of Moral Displacement.}
\emph{Synthetic presences may acquire normative affordances by virtue of their perceived ontology.}

\textbf{Status: Retained (Conceptual Foundation).}  
This hypothesis explains \textit{why} a non-interactive robot can perturb moral cognition. The data do not test it directly, but every behavioural pattern observed is \textit{consistent} with this ontological grounding.

\paragraph{H3. Synthetic Perturbation of Moral Inference.}
\emph{The robot refracts the transition from moral salience to prosocial action.}

\textbf{Status: Retained (Mechanistic).}  
The observed attenuation at the aggregate level, together with the trait-contingent cluster effects, supports the mechanistic claim that $\gamma_R$ modifies the evaluative mapping rather than merely shifting motivational baselines.

\paragraph{H4. (Implied) Trait-Contingent Modulation Hypothesis.}
\emph{The perturbative effect of $\gamma_R$ varies as a function of latent cognitive–affective regimes encoded in $\beta_C$.}

\textbf{Status: Provisionally Supported.}  
Cluster-specific patterns strongly suggest regime-dependent responsiveness. This hypothesis will be tested more formally in the regression and interaction analyses that follow.


% ----------------------
% Mathematical Summary
% ----------------------

\subsubsection{Condensed Status of the Formal Framework}

The mathematical apparatus introduced earlier has now been substantively activated:

\begin{itemize}
	\item The transformation function $f(\cdot)$ provided a principled way of interpreting behavioural attenuation as deformation of the evaluative mapping.
	\item The expected-value contrast $\mathbb{E}[f(\Sigma)]$ vs.\ $\mathbb{E}[f(\Sigma \cup \mathscr{R})]$ captured the aggregate attenuation signature, now empirically supported.
	\item The tripartite decomposition  
	\[
	\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
	\]
	has proven essential:  
	– $\alpha_E$ held constant (Watching Eye),  
	– $\beta_C$ refined via PCA and clustering,  
	– $\gamma_R$ isolated as the only experimental manipulation.  
\end{itemize}

\noindent
In short, the formalism has not merely annotated the behavioural results but \textbf{structured the empirical horizon} of the experiment: it dictates what counts as evidence for deformation, where individual differences should enter the model, and how perturbation effects should be interpreted.

\bigskip

% ----------------------
% Topological / Ontological Interpretation
% ----------------------

\subsubsection{Topological and Ontological Interpretation}

\noindent
The combined results illuminate a deeper philosophical point: the perturbation induced by the robot is best understood as a \textbf{topological deformation} of the moral field rather than a unidirectional causal force. At the operative Level of Abstraction (LoA) relevant to participants, the NAO robot presents itself neither as an inert object nor as a full agent; instead, it occupies an ontologically ambiguous middle-ground whose semantic affordances penetrate the participant’s normative perception.

Under this LoA, $\mathscr{R}$ functions as a \textbf{semiotic operator}—a presence that modifies the structure of evaluative attention by refracting the moral salience of $\alpha_E$ before it becomes behaviourally actionable. The attenuation of prosocial donation thus reflects not a collapse of empathy, nor a motivational deficit, but a reconfiguration of the interpretive schema that governs the mapping
\[
\Sigma \xrightarrow{f} \mathscr{D}.
\]

\noindent
The second major result extends this insight: the deformation is \textit{not} uniform across individuals. Instead, it is \textbf{contingently realised} through latent cognitive–affective structures. In some clusters, the presence of $\gamma_R$ yields substantial attenuation; in others, its impact is negligible. This cluster-contingent pattern confirms that the perturbation does not operate on a neutral cognitive substrate but on \textit{trait-defined normative filters}, each instantiating a distinct interpretive topology.

\bigskip

% ----------------------
% Interim Conclusion Box
% ----------------------

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,title=Interim Conclusion: Topological and Trait-Dependent Moral Modulation]
		Robotic co-presence attenuates prosocial donation through a deformation of the evaluative pathway that links moral salience to action. This attenuation is neither uniform nor deterministic: it emerges as a probabilistic refractor of moral cognition whose amplitude varies across latent cognitive–affective regimes. The empirical findings thus far support all three foundational hypotheses—evaluative deformation, synthetic normativity, and perturbation of moral inference—and provisionally support the trait-contingent modulation hypothesis. At the operative Level of Abstraction, the humanoid robot acts as a semiotic agent whose ontological ambiguity reshapes the topology of moral evaluation. Subsequent analyses will test the stability, depth, and interaction structure of this modulation through cluster-specific regression modelling.
	\end{tcolorbox}
\end{center}


\subsection{The Dilution of the Watching Eye Effect under Robotic Co-Presence}

\noindent
Within the present experimental design, the morally salient cue was instantiated through the photograph of an infant in need, prominently displayed on the Operation Smile brochure. As established earlier (see \cref{chap:watching_eye}), such pictorial stimuli operate as \textit{implicit moral surveillance cues}: they trigger affective empathy, reputational sensitivity, and the pre-reflective sense of “being observed” that underlies the classical Watching Eye effect \cite{Bateson2006, NettleEtAl2013, ContyGeorgeHietanen2016}. 

\noindent
The interim results now allow us to articulate a critical interpretive point: \textbf{the presence of the humanoid robot systematically dilutes the potency of the Watching Eye stimulus}. This dilution does not reflect a suppression of empathy nor a negation of moral motivation. Instead, it emerges as a topological deformation of the evaluative field in which the Watching Eye cue is embedded.

\noindent
At the operative Level of Abstraction, the robot introduces a second semiotic centre—an ontologically ambiguous presence whose social affordances compete with, refract, or partially occlude the normative signal emitted by the infant's face. The moral salience encoded in the pictorial cue no longer operates within a clean perceptual–affective channel; it is instead filtered through a perturbed interpretive topology shaped by $\gamma_R$. 

\noindent
In this sense, the dilution of the Watching Eye effect is not a psychological epiphenomenon but the behavioural signature of the Evaluative Deformation Hypothesis (\ref{hyp:evaluative_deformation}). The attenuation in donation behaviour reflects an altered mapping from 
\[
\Sigma_{\text{eye}} \longrightarrow \mathscr{D},
\]
where $\Sigma_{\text{eye}}$ denotes the moral-affective perceptual space dominated by the infant’s image. Under robotic co-presence, this mapping becomes
\[
\Sigma_{\text{eye}} \cup \mathscr{R},
\]
and its expected output $\mathbb{E}[f(\Sigma_{\text{eye}} \cup \mathscr{R})]$ is weakened relative to the control condition.

\noindent
Thus, the Watching Eye stimulus does not lose its moral force; rather, its \textit{evaluative amplitude} is refracted by the semiotic presence of the robot, producing a diluted conversion of moral salience into prosocial action. This interpretation coheres with both the ‘Amplitude of Moral Refraction’ conclusion (\cref{conc:amplitude_moral_refactor}) and the ‘Contingent Structure of Cognitive Modulation’ conclusion (\cref{conc:clustered_moral_refraction}), and it reinforces the central claim of this chapter: synthetic presences modulate moral cognition by altering the topology through which normative cues are interpreted, not by erasing those cues.



%%% NEW CONTENT N10
\subsection{Cluster-Specific Regression Analysis of Robotic Perturbation}

\noindent
To determine whether specific cognitive–affective regimes exhibit differential sensitivity to robotic presence, we conducted a stratified linear regression analysis within each of the three latent personality clusters identified through PCA reduction and $k$-means partitioning. Donation amount served as the dependent variable, while experimental condition (Control vs.\ Robot) functioned as the primary predictor. This design allows us to test whether the perturbative effect of $\gamma_R$ is uniformly distributed across the population or selectively amplified within particular psychological ecologies.

\vspace{0.3em}
\noindent
\textbf{A sharply asymmetric pattern emerges.}  
Within the \textbf{Prosocial–Empathic / Warm–Sociable} profile, robotic presence exerts a marked attenuation effect: the regression coefficient for the Robot condition is substantially negative (\(\beta = -1.33\)), approaching conventional significance (\(p = .091\)) and accounting for a non-trivial proportion of variance (\(R^2 = 0.087\)). This regime—dispositionally characterised by high Empathizing, elevated Agreeableness, and strong sociability—is theoretically the most responsive to the Watching Eye stimulus, because its evaluative architecture privileges affective resonance as the primary conduit for moral salience. The significant drop in donation under $\gamma_R$ therefore reveals a targeted deformation of the empathic pathway: the robot refracts, rather than merely weakens, the affective-to-behavioural mapping that ordinarily sustains prosocial output in this group.

\vspace{0.4em}
\noindent
By contrast, the \textbf{Emotionally Reactive / Low-Structure} profile (\(\beta \approx 0\), \(p > .70\)) and the \textbf{Analytical–Structured / High-Systemizing} profile (\(\beta = -0.28\), \(p > .70\)) exhibit negligible perturbation. For the former, affective volatility introduces noise that may obscure subtle contextual modulation; for the latter, the affective Watching Eye cue already carries limited normative weight, and the robot is likely recoded as a structurally neutral artefact rather than a socially meaningful presence. The absence of attenuation in these two ecologies confirms that robotic presence does not impose a uniform moral influence across participants.

\vspace{0.4em}
\noindent
These findings consolidate the theoretical shift advanced in earlier sections: individual differences must not be conceptualised as additive covariates but as \textbf{distinct cognitive–affective topologies}. Each cluster constitutes an internal evaluative landscape whose geometry determines the stability, amplitude, and direction of moral salience transmission under perturbative conditions. Within this framework, the Watching Eye cue and $\gamma_R$ do not operate as independent forces; rather, they interact within a structured evaluative manifold whose topology differs across psychological regimes.  

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{new_plots/cluster_regression_coefficients.png}
	\caption{Regression coefficients for the Robot condition within each personality cluster (95\% confidence intervals). The Prosocial–Empathic profile shows a pronounced attenuation effect, while the Emotionally Reactive and Analytical–Structured profiles exhibit negligible or non-significant coefficients. This pattern demonstrates that robotic presence exerts a differentiated moral influence, contingent on latent cognitive–affective ecologies.}
	\label{fig:cluster-regression}
\end{figure}

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Conclusion: Differentiated Moral Sensitivity to Robotic Presence]
		\label{conc:regression_cluster_specific}
		Robotic presence does not exert a uniform moral influence. Instead, its perturbative effect emerges selectively through the structured configurations of latent psychological regimes. Cluster-specific regression analysis demonstrates that moral attenuation is concentrated within particular cognitive–affective ecologies—notably the Prosocial–Empathic profile—confirming that the ethical salience of synthetic agents is not globally encoded but \textbf{contextually realised through trait-dependent evaluative topologies}.
	\end{tcolorbox}
\end{center}

\noindent
This cluster-level analysis thus advances the broader conceptual arc of the chapter. The perturbative force of $\mathscr{R}$ is neither binary nor homogeneous. It refracts through psychological architectures that differ in their susceptibility to moral cues, their interpretive stability in the face of ontological ambiguity, and their capacity to integrate artificial co-agents into the evaluative apparatus of practical reasoning.

The differentiated regression patterns reported above can be expressed in a compact
mathematical form by examining how the evaluative transformation function,
$f(\cdot)$, behaves across the three latent cognitive–affective regimes.

For the \textbf{Emotionally Reactive / Low-Structure Profile}, donation behaviour remains
effectively unchanged across conditions. This corresponds to an evaluative mapping
in which robotic presence introduces no meaningful perturbation:
\[
\mathbb{E}\big[f(\Sigma \cup \mathscr{R})\big] \;\approx\; \mathbb{E}\big[f(\Sigma)\big].
\]

For the \textbf{Prosocial–Empathic / Warm–Sociable Profile}, robotic presence produces a
marked attenuation in prosocial action, consistent with a refracted or collapsed
transformation pathway:
\[
\mathbb{E}\big[f(\Sigma \cup \mathscr{R})\big] \;\ll\; \mathbb{E}\big[f(\Sigma)\big].
\]

For the \textbf{Analytical–Structured / High-Systemizing Profile}, the perturbation is milder
but still directionally negative, suggesting a partially disrupted evaluative mapping:
\[
\mathbb{E}\big[f(\Sigma \cup \mathscr{R})\big] \;<\; \mathbb{E}\big[f(\Sigma)\big].
\]

\noindent
Together, these expressions provide a compact formal summary of the
cluster-dependent structure of moral perturbation: the same environmental input
$(\Sigma \cup \mathscr{R})$ is transduced into different expected behavioural outputs
depending on the latent cognitive–affective topology governing the evaluative
function $f(\cdot)$. This reinforces the central finding of the cluster analysis:
\textit{synthetic presence is not a uniform causal factor, but a structure-sensitive
	modulator whose influence is enacted only through particular psychological regimes.}

\noindent
 What remains is to examine whether these findings persist when classical linear assumptions are relaxed, and when the inferential dynamics are modelled within probabilistic frameworks capable of representing uncertainty, interaction structures, and epistemic gradients.

\subsection{Bayesian Estimation and Epistemic Gradient Framing}

\noindent
The analyses conducted thus far—chi-squared tests, Mann–Whitney comparisons, and cluster-specific OLS regressions—have established an initial empirical profile of moral attenuation under robotic presence. Yet these methods, by virtue of their frequentist foundations, impose restrictive epistemic commitments. They require data to conform to assumptions of normality, homoscedasticity, and independent errors, and they compress inferential uncertainty into binary decisions: significant versus non-significant. In a dataset of modest size (\(N \approx 70\)), and in an experimental design explicitly concerned with subtle perturbations of moral salience, these constraints obscure more than they reveal.

\vspace{0.4em}
\noindent
\textbf{The epistemic limitations of frequentism are not merely statistical; they are conceptual.}  
Frequentist procedures treat uncertainty as an error term, not as a structured property of knowledge. They cannot express graded belief, asymmetric plausibility, or the ways in which ontological ambiguity—such as that introduced by NAO—propagates through an evaluative system. Nor can they incorporate hierarchical structure emerging from latent cognitive–affective profiles. In short, they fail to capture the topology of inference itself.

\vspace{0.4em}
\noindent
To address these limitations, we employed \textbf{Bayesian estimation}, specified as a hierarchical model that incorporates (i) group-level variation between the Control and Robot conditions, and (ii) cluster-level variation across the three latent personality ecologies: the \textit{Emotionally Reactive / Low-Structure} profile, the \textit{Prosocial–Empathic / Warm–Sociable} profile, and the \textit{Analytical–Structured / High-Systemizing} profile. This hierarchical framing allows the posterior distribution to reflect not only uncertainty in the donation means, but also the structural heterogeneity of the population—an essential requirement for interpreting moral perturbation within a multi-layered evaluative topology.

\vspace{0.5em}
\noindent
\textbf{Posterior estimation.}  
Under weakly informative priors, the posterior mean of the donation difference (\texttt{Control – Robot}) was approximately £0.70, with a 95\% credible interval spanning –£1.75 to +£0.30. While the interval includes zero, its mass is asymmetrically skewed toward negative values, indicating \textit{directional probabilistic evidence} that robotic presence attenuates prosocial output. Unlike p-values, which collapse inferential nuance into a discontinuous threshold, the posterior distribution provides a graded representation of epistemic support: attenuation is neither confirmed nor refuted categorically, but represented as a structured probability over moral space.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{new_plots/posterior_donation_difference.png}
	\caption{Posterior distribution of the estimated donation difference between the Control and Robot conditions. The density skews toward negative values, indicating directional probabilistic evidence that robotic co-presence attenuates prosocial behaviour. The vertical dashed line denotes the point of no effect. Bayesian inference renders the effect size and its uncertainty as a continuous epistemic field rather than a binary verdict.}
	\label{fig:posterior-difference}
\end{figure}

\vspace{0.5em}
\noindent
\textbf{Epistemic value of the Bayesian approach.}  
The Bayesian framework offers three advantages directly relevant to the interpretive architecture of this chapter:

\begin{enumerate}
	\item \textbf{Uncertainty as structure, not noise.}  
	The posterior distribution reflects graded belief over effect magnitudes, aligning with the chapter’s emphasis on moral topologies rather than discrete behavioural outputs.
	
	\item \textbf{Compatibility with ontological ambiguity.}  
	Robotic presence operates as a \textit{semiotic perturbator} whose influence is subtle, non-deterministic, and context-dependent. Bayesian inference accommodates such phenomena by modelling effect strength as a distribution across epistemic space.
	
	\item \textbf{Hierarchical alignment with trait-dependent regimes.}  
	The differential sensitivities observed in the Prosocial–Empathic versus Analytical–Structured profiles, and the near-invariance of the Emotionally Reactive profile, are naturally represented within a Bayesian hierarchical model. Each cluster inherits a partial-pooling structure that respects its latent topology while sharing information across the population.
\end{enumerate}

\vspace{0.4em}
\noindent
\textbf{Connection to Floridi’s Levels of Abstraction.}  
At the operative LoA of the participant, Bayesian estimation better captures the epistemic footprint of $\gamma_R$ because it represents uncertainty as an ontologically meaningful property of the evaluative system. Just as NAO’s ambiguous ontology introduces interpretive indeterminacy, the Bayesian posterior encodes inferential indeterminacy: both operate as gradients rather than binary categories. In this sense, Bayesian inference does not simply analyse the data—it mirrors the very cognitive structure by which participants register moral salience under conditions of uncertainty.



\subsubsection{Epistemic Interpretation of the Bayesian Results}

\noindent
Bayesian inference may appear unfamiliar to readers accustomed to classical statistics, yet its relevance to this chapter is not merely methodological but philosophical. Whereas frequentist tests force evidence into a binary verdict—``significant’’ or ``not significant’’—Bayesian estimation represents uncertainty as a \textit{graded belief}. It asks how plausible an effect is, given the data and our modelling assumptions, and it expresses that plausibility as a continuous distribution rather than a categorical judgment.

\vspace{0.4em}
\noindent
In practical terms, the posterior distribution shown in Figure~\ref{fig:posterior-difference} does \textbf{not} claim that robotic presence definitely reduces donation behaviour. Instead, it says that—given the observed data—the reduction is \textit{more likely than not}. The most plausible magnitude of this attenuation is located around £0.70, but with substantial uncertainty surrounding it. This uncertainty is not a flaw; it is a feature of the Bayesian framework, which makes visible the epistemic limits of the evidence rather than compressing them into a single thresholded output.

\vspace{0.4em}
\noindent
Readers familiar with p-values may recall that some classical tests, especially the Mann–Whitney \(U\) test, did not reach conventional significance. This does not contradict the Bayesian findings. Rather, it reflects two different epistemic logics. Frequentist tests ask whether the data cross a pre-defined threshold under strict distributional assumptions. Bayesian analysis asks how the evidence updates our degree of belief about a hypothesis, even when the effect is small, variable, or distributed unevenly across psychological subgroups.

\vspace{0.4em}
\noindent
In this sense, the Bayesian model does not ``rescue’’ non-significant results; it \textit{reframes} them. It allows us to articulate the structure of uncertainty explicitly, acknowledging that our dataset is modest in size and that the moral field under investigation is inherently noisy. Where classical statistics provide a verdict, Bayesian inference provides a \textbf{map of epistemic gradients}—a representation of how belief should shift in light of the available evidence.

\vspace{0.4em}
\noindent
This is particularly appropriate for the present study, where the effect of NAO’s presence is theorised to arise from \textit{ontological ambiguity} and \textit{trait-dependent refractive pathways}. Such perturbations are not deterministic; they unfold across the different cognitive–affective ecologies identified earlier (Emotionally Reactive, Prosocial–Empathic, Analytical–Structured). A modelling framework that treats uncertainty as structured and meaningful is therefore better aligned with the moral-topological interpretation guiding the chapter.

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Conclusion: Gradient of the Impact of Moral Refraction]
		The Bayesian analysis supports a cautiously framed but epistemically credible claim: in some contexts, and for some psychological profiles, the presence of a humanoid robot reduces the likelihood that morally salient cues will be converted into prosocial behaviour. This conclusion is inherently graded rather than definitive, reflecting the probabilistic structure of both the evidence and the underlying cognitive processes.
	\end{tcolorbox}
\end{center}

\noindent
For a comparison with the non-Bayesian (frequentist) version of this claim, see Conclusion~\ref{conc:impact_moral_refactor}. Together, the two perspectives offer complementary lenses: one categorical and conservative, the other probabilistic and epistemically transparent.

% =====================================================================
% INTERIM CONCLUSION — TOPOLOGICAL RECONFIGURATION OF MORAL ACTION
% =====================================================================

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Interim Conclusion: Topological Reconfiguration of Moral Action Under Synthetic Co-Presence]
		The empirical and probabilistic results obtained thus far permit the first integrated assessment of Question~\ref{q:robot-agent}. Taken together, the behavioural attenuation, the cluster-specific regression patterns, and the Bayesian posterior distribution converge on a coherent interpretative claim: \textbf{the silent co-presence of a humanoid robot reshapes the evaluative topology through which morally salient cues become actionable for human agents}. This reshaping is neither universal nor deterministic; it is a graded, structure-dependent perturbation whose amplitude and direction emerge from the interplay of ontological ambiguity, individual trait configuration, and the Level of Abstraction at which the robot is cognitively encountered.
	\end{tcolorbox}
\end{center}

\noindent
The mechanism by which robotic presence exerts its influence is best understood in topological rather than causal terms. The NAO robot, operating in autonomous life mode, introduces a \textit{semiotic curvature} into the moral field: it subtly alters the evaluative geometry through which agents perceive, weight, and transform morally charged cues. This deformation is confirmed at the aggregate level through reduced prosocial donation, yet its structure becomes explicit only when viewed through the lens of latent trait ecologies.

Across the three identified psychological architectures, the perturbative influence of $\gamma_R$ refracts in distinct ways.  
The \textbf{Prosocial–Empathic profile}—marked by warmth, sociability, and heightened empathic attunement—exhibits the strongest attenuation under robotic presence. Theoretically, this group should be most responsive to the Watching Eye stimulus; their reduced prosocial output therefore indicates a displacement or dilution of empathic salience by the robot’s ontological ambiguity.  
The \textbf{Emotionally Reactive–Low-Structure profile} shows negligible modulation, suggesting that their evaluative field is already volatile and weakly integrated, leaving little room for additional deformation.  
The \textbf{Analytical–Structured profile} likewise remains comparatively invariant, consistent with a cognitive style that filters moral cues through explicit norms rather than affective resonance, rendering the robot semantically inert at their operative LoA.

\noindent
Bayesian estimation further clarifies the nature of this modulation. The posterior distribution does not license categorical claims, but instead renders visible an \textit{epistemic gradient}: the attenuation effect is probabilistically credible, directionally consistent with the behavioural and regression analyses, yet embedded in uncertainty that reflects the heterogeneity of human evaluative architectures. The robot’s moral impact is thus best read not as an on/off switch, but as a probabilistic refractor whose influence varies across psychological topologies.

Viewed through Floridi’s Levels of Abstraction, each cluster manifests a distinct \textit{semantic filter} through which the robot is interpreted. For the Prosocial–Empathic cluster, the operative LoA foregrounds social cues and affective salience; the robot therefore functions as a morally confusing signal, displacing the Watching Eye stimulus. For the Analytical–Structured cluster, the operative LoA highlights rule-based structure, making the robot semantically inert. For the Emotionally Reactive group, the LoA is affectively saturated yet structurally unstable, producing negligible behavioural change. In all cases, the robot’s ambiguous ontology is processed at the LoA that is dispositional to each group, generating a differentiated moral topology across the population.

\begin{center}
	\begin{tcolorbox}[colback=white,colframe=black!60,
		title=Provisional Answer to Question~\ref{q:robot-agent}]
		The cumulative evidence supports a cautiously affirmative answer: \textbf{yes, the mere presence of a synthetic, non-agentic entity can perturb the evaluative transformation through which moral salience becomes moral action}. This perturbation does not manifest uniformly; it emerges through the interaction of robotic ontology with latent cognitive–affective structures. The Evaluative Deformation Hypothesis, the Synthetic Normativity of Moral Displacement, and the Synthetic Perturbation of Moral Inference are empirically and conceptually supported. The trait-contingency hypothesis is provisionally validated, pending further hierarchical modelling.
	\end{tcolorbox}
\end{center}

\noindent
Thus, the NAO robot’s presence in the room—silent, minimally animated, ontologically ambiguous—modulates moral action not by interrupting reflective deliberation, but by reconfiguring the \emph{interpretive topology} within which morally salient cues acquire behavioural force. The charity poster depicting a child beneficiary of medical aid—our operationalisation of the Watching Eye stimulus—normally functions as an affectively loaded reputational cue, activating empathic concern and third–party moral vigilance~\cite{HaleyFessler2005, Bateson2006, NettleEtAl2013, ContyGeorgeHietanen2016}. In the Robot condition, however, this prime is \textbf{perceptually and semantically diluted}: attentional and inferential resources are partially displaced from the poster toward the robot’s embodied but ontologically indeterminate presence. In effect, $\mathscr{R}$ acts as a \emph{semantic competitor}, weakening the intuitive channel through which the Watching Eye paradigm ordinarily promotes prosocial giving.

\noindent
This pattern is theoretically coherent within the \textit{Social Intuitionist Model} of moral judgment~\cite{Haidt2001, HaidtBjorklund2008, Cushman2013}, which holds that moral behaviour is driven primarily by rapid, affect-laden intuitions rather than by reflective cost–benefit deliberation~\cite{Greene2001, Greene2002, Moll2005}. Under this model, the Watching Eye stimulus shapes behaviour because it elicits immediate, intuitive appraisals of reputational accountability. Our findings indicate that NAO’s ambiguous ontology disrupts this intuitive pathway: for individuals in the \textit{Prosocial–Empathic} profile—whose evaluative architecture relies heavily on affective resonance and interpersonal attunement—the robot’s presence refracts moral salience away from the poster, thereby reducing the likelihood that intuitive concern is translated into donation behaviour. For the \textit{Analytical–Structured} and \textit{Emotionally Reactive} profiles, whose evaluative dynamics depend respectively on rule-based structure or affective volatility, the robot registers as normatively inert or affectively irrelevant, leaving donation patterns largely unaffected.

\noindent
These results therefore support an intuitionist, rather than rationalist, interpretation of moral action in this environment. The attenuation effect does not emerge as a failure of explicit reasoning, but as a deformation of the intuitive evaluative processes that precede it. In topological terms, $\mathscr{R}$ alters the curvature of the moral field: it bends the trajectories along which intuitive appraisals propagate, thereby shifting the probability that moral cues achieve behavioural expression. At the operative \emph{Level of Abstraction}~\cite{Floridi2008, Floridi2010, Floridi2013}, the robot functions as a semiotic intrusion—an entity whose perceived ontology modifies what the agent treats as salient, credible, or normatively relevant.

\noindent
From a methodological perspective, this interpretation has direct implications for the study of moral cognition. If moral behaviour is mediated by affectively grounded intuitions that are sensitive to environmental structure, then behavioural traces—such as donation decisions—become legitimate datasets for inferring moral evaluations. This aligns with the premises of \textit{Social Signal Processing}~\cite{Vinciarelli2009, Vinciarelli2012} and \textit{Affective Computing}~\cite{Picard1997, CalvoDMello2010}, which treat observable behaviour as an informational interface through which latent cognitive–affective states may be estimated, modelled, and formalised. The present findings demonstrate that synthetic co-presence can systematically reshape this interface: by altering the distribution of intuitive salience, the robot modifies the behavioural signatures from which moral inference is drawn.

\noindent
This also intersects directly with the ambitions of \textit{Machine Ethics}~\cite{Moor2006, FloridiSanders2004, AndersonAnderson2007, Coeckelbergh2010}, which seek to formalise the conditions under which artificial systems may (or may be perceived to) participate in moral contexts. Our results show that even non-interactive robots can perturb moral cognition simply by being \emph{present}—suggesting that artificial agents need not act, speak, or decide in order to exert normative influence. Their moral relevance may emerge from their mere ontological profile, as processed through the observer’s cognitive ecology.

\noindent
In this respect, the experiment provides an empirically grounded demonstration that \textbf{synthetic presence can deform the moral field}, not by commanding behaviour, but by bending the intuitive pathways through which moral meaning becomes action. Moral cognition is revealed as both structurally sensitive to ontological ambiguity and computationally tractable through the behavioural signatures it leaves behind. This establishes a promising bridge between empirical moral psychology, formal models of moral topology, and the computational disciplines—Social Signal Processing, Affective Computing, and Machine Ethics—that seek to analyse, predict, or ethically regulate human–machine moral ecosystems.

\subsection{Final Synthesis: Moral Topology, Synthetic Presence, and the Limits of Machine Ethics}

\noindent
Taken together, the behavioural, inferential, and Bayesian results presented in this chapter yield a coherent and theoretically significant picture of how synthetic presence modulates human moral behaviour. The NAO robot’s inclusion—silent, minimally animated, ontologically indeterminate—functions not as an agent issuing commands, nor as a passive background object, but as a \emph{semiotic perturbator} that reorganises the interpretive topology through which moral salience becomes behaviourally actionable.

At the behavioural level, we observed a clear attenuation of prosocial donation in the Robot condition. At the aggregate scale, the attenuation is statistically identifiable; at the individual level, Bayesian estimation reveals a skewed but uncertain probability distribution favouring reduced prosocial output. Cluster-specific analyses show that this attenuation is far from uniform: it is concentrated within the \textbf{Prosocial–Empathic} profile, muted within the \textbf{Analytical–Structured} profile, and largely absent within the \textbf{Emotionally Reactive} profile. These findings reinforce the core claim that robotic presence refracts moral salience through \emph{trait-dependent evaluative topologies} rather than altering behaviour in a direct, causal, or homogeneous manner.

From the standpoint of the \textit{Social Intuitionist Model} of moral judgment~\cite{Haidt2001,Greene2001,Cushman2013}, this pattern is theoretically coherent. Moral action, in this model, is driven primarily by rapid, affectively grounded intuitions rather than reflective deliberation. Our charity poster—operationalising the Watching Eye stimulus—serves precisely as such an intuitive moral prime, designed to trigger empathic concern and reputational awareness. Yet the robot’s ambiguous presence dilutes this intuitive channel: the locus of social attention partially shifts from the moral cue to the synthetic body occupying the room, thereby weakening the intuitive pull that ordinarily supports prosocial donation. In topological terms, $\mathscr{R}$ alters the local curvature of the moral field, redirecting the intuitive flows along which salience is converted into action.

This interpretation is strengthened by Floridi’s theory of \textit{Levels of Abstraction}~\cite{Floridi2008,Floridi2010}. At the operative LoA of the participant, the robot is encoded not as a machine, nor as a full moral agent, but as an entity whose perceptual affordances (eyes, posture, subtle motion) activate anthropomorphic priors without fulfilling the semantic criteria for agency. In this sense, $\mathscr{R}$ occupies a liminal ontological position: too animate to be ignored, not animate enough to be treated as an intentional other. The deformation we observe is thus a \emph{semantic deformation}, produced by a presence that inserts ambiguity into the participant’s perceptual-moral ecology.

This result has substantial implications for the study of moral cognition. First, it provides empirical support for the thesis that \textbf{moral meaning is environmentally scaffolded}: small shifts in perceptual context can reorganise the evaluative machinery that underpins prosocial action. Second, it demonstrates that \textbf{moral behaviour is accessible through behavioural signatures}, a fact that aligns with the methodological aims of Social Signal Processing~\cite{Vinciarelli2009} and Affective Computing~\cite{Picard1997}. If moral action can be systematically perturbed by manipulating environmental affordances—including synthetic presences—then moral reasoning becomes partially tractable through the modelling of behavioural traces, opening the door to computational approaches for mapping moral intuition as a dynamic, context-sensitive process.

\medskip
\noindent\textbf{A Critical Note on Machine Ethics.}  
The present findings also cast a critical light on the current state of Machine Ethics. Much of the Machine Ethics literature has historically been driven by the ambition to design “ethical agents” endowed with explicit moral rules, reasoning procedures, or decision architectures~\cite{Moor2006,Anderson2011,Boddington2017}. In the era of LLMs, this ambition has often been rearticulated as the attempt to “align” models with moral norms via fine-tuning datasets, reinforcement feedback, or rule-based guardrails.

Yet the empirical evidence presented here strongly suggests that \textbf{such approaches misunderstand the locus of moral influence}.  
Synthetic systems influence human moral behaviour not by engaging in propositional reasoning or ethical deliberation, but by subtly reshaping the perceptual and normative topology of the environments in which humans act. Their moral impact is \emph{interpretive, affective, and topological}, not rule-based, representational, or algorithmic. A robot that barely moves can dilute intuitive moral cues; an LLM that outputs contextually structured language can shift a user’s evaluative frame long before any explicit reasoning occurs.

In this light, the classical project of Machine Ethics—focused on the construction of explicit, internally encoded ethical principles—appears increasingly inadequate. It offers no tools for capturing the kind of \textbf{ambient moral modulation} demonstrated here, and provides little insight into how synthetic entities shape moral cognition not through agency but through presence, salience, and interpretive displacement. In the context of LLMs, whose moral influence operates primarily at the level of framing, narrative structure, and socio-informational priming, this limitation becomes starkly visible. A model’s ethical behaviour cannot be reduced to its output rules; it must be understood in terms of the cognitive topologies it induces in its users.

\medskip
\noindent\textbf{Synthesis.}  
The experiment thus demonstrates three consequences of immediate relevance to contemporary moral psychology and AI ethics:

\begin{enumerate}
	\item \textbf{Moral behaviour is topologically modulated.}  
	The presence of a synthetic agent reshapes the evaluative terrain through which moral salience is processed, producing measurable behavioural effects.
	
	\item \textbf{This modulation is trait-dependent.}  
	The Prosocial–Empathic profile is most susceptible to attenuation; the Analytical–Structured and Emotionally Reactive profiles exhibit greater topological resilience.
	
	\item \textbf{Machine Ethics must fundamentally reconceive its object.}  
	Ethical AI cannot be meaningfully approached through rule-lists or moral logics alone. It must instead account for the subtle ways in which artificial systems reorganize human evaluative architectures at the perceptual, affective, and intuitive levels.
\end{enumerate}

\noindent
In closing, this chapter provides an empirically grounded demonstration that \textbf{synthetic presence can deform the moral field}, not by reasoning, commanding, or acting, but by bending the intuitive pathways through which moral meaning becomes behaviour. The implications extend far beyond robotics: they compel a reconceptualisation of how artificial systems participate in, perturb, and co-structure the topology of human moral cognition.


\subsection{Synthesis: Moral Topology, Synthetic Presence, and the Limits of Machine Ethics}

\noindent
The empirical and formal work developed in this chapter allows us to return to Question~\ref{q:robot-agent} with a more determinate answer. The evidence now supports the following claim: \textit{the silent co-presence of a humanoid robot can, under specific psychological configurations, attenuate the conversion of morally salient cues into prosocial action}. This attenuation is modest in magnitude, probabilistic rather than deterministic, and concentrated within particular evaluative regimes—most notably the Prosocial–Empathic profile—yet it is real, structured, and epistemically tractable.

\noindent
Topologically, the NAO robot functions as a local deformation of the moral field. The charity poster depicting a child in need, originally introduced as a canonical Watching Eye stimulus, constitutes an affectively loaded attractor in the evaluative landscape: under ordinary circumstances, it pulls intuitive appraisals towards prosocial donation through mechanisms of reputational concern, empathic resonance, and implicit monitoring~\cite{HaleyFessler2005,Bateson2006,ContyGeorgeHietanen2016}. Our results indicate that the introduction of $\mathscr{R}$ partially redistributes this moral salience. For participants in the Prosocial–Empathic regime, the robot operates as a competing focus of attention and an ontologically ambiguous social cue; the intuitive channel that would normally connect the poster to donation behaviour is weakened, re-routed, or locally disrupted.

\noindent
This pattern aligns with Social Intuitionist accounts of moral judgement, according to which moral action is driven primarily by fast, affect-laden intuitions, with explicit reasoning playing a largely post-hoc justificatory role~\cite{Haidt2001,Greene2001,Cushman2013}. In this frame, the Watching Eye effect is not a matter of explicit calculation but of intuitive salience. The robot’s presence does not ``argue against'' giving; rather, it changes what is experientially foregrounded as normatively relevant. For those whose moral cognition is heavily scaffolded by empathic and reputational cues, NAO’s ambiguous status as quasi-agent and quasi-object suffices to dilute the intuitive force of the poster. For the Analytical–Structured profile, by contrast, the same presence appears to be normatively inert, processed more as a stable environmental feature than as a moral signal. The experiment thus vindicates an ecological, intuitionist interpretation of moral modulation: synthetic presence bends the trajectories of intuitive appraisal rather than intervening at the level of explicit principle application.

\noindent
Floridi’s theory of Levels of Abstraction (LoA) provides the metaphysical and methodological vocabulary to articulate this deformation~\cite{Floridi2008,Floridi2010,Floridi2013}. At the operative LoA of the participant, NAO does not appear as a set of internal states or source code, but as a semiotic bundle: body, gaze, posture, micro-movements. These features instantiate \emph{semantic affordances} that are picked up by different evaluative ecologies in different ways. For the Prosocial–Empathic regime, the robot is encoded as a kind of morally pregnant presence that competes with the child’s image for attentional and normative priority; for the Analytical–Structured regime, the same presence is filtered as structurally irrelevant to the donation decision. The experiment thus realises, in a controlled setting, Floridi’s claim that artefacts can acquire moral salience via their informational role, without being moral agents in any robust sense~\cite{Floridi2013}. NAO is not a locus of \emph{moral agency} here; it is a perturbation in the \emph{informational environment} that reconfigures the mapping from salience to action.

\noindent
Framed in this way, the present study also exposes a set of limitations in the prevailing discourse of \emph{Machine Ethics}. Much of that literature has centred on the design of explicitly ``moral'' or ``ethical'' machines—systems that implement deontological rules, compute consequences, or learn norms, in order to make or justify decisions in ethically acceptable ways~\cite{Allen2006,Wallach2008,Moor2006,Moor2011,Moor2023}. In its canonical formulations, machine ethics presupposes a relatively sharp boundary between human users and artificial moral agents, and locates the core normative challenge in the internal architecture of the latter. Our findings suggest that this focus is, at best, incomplete.

\noindent
First, the experimental results show that synthetic systems can exert morally relevant influence \emph{without} possessing any explicit ethical architecture at all. NAO neither represents moral principles nor optimises outcomes; it simply occupies space, moves minimally, and is seen. Yet this is sufficient to alter the aggregate pattern of prosocial giving, and to do so selectively across latent cognitive--affective regimes. A research agenda that concentrates on endowing machines with codified moral theories, while neglecting their role as perturbative presences within human evaluative topologies, risks a kind of \emph{conceptual hollowing}: the label ``machine ethics'' is retained, but the most pervasive moral effects of machines—those mediated through human intuition and social cognition—are left untheorised~\cite{Allen2006,Wallach2008,Floridi2013}.

\noindent
Second, the canonical architectures of machine ethics were developed with relatively transparent, modular systems in mind: rule-based agents, deliberative planners, or learning systems whose internal representations could, in principle, be inspected and constrained~\cite{Wallach2008,Winfield2019,Santoro2018}. Contemporary large language models, recommender systems, and socio-technical platforms do not fit this template. As Coeckelbergh has argued, current AI increasingly generates \emph{simulacra of ethical deliberation}: outputs that \emph{look} like moral reasoning, yet lack robust ties to accountability, context, or genuine normative commitment~\cite{Coeckelbergh2023}. In such an environment, the question ``how do we encode ethics into a machine?'' becomes technically underdetermined and politically misleading. What our data illustrate instead is a different, and arguably more urgent, question: \textit{how do artificial systems and environments shape the informational fields within which human moral cognition operates?}

\noindent
Third, the experiment suggests a reorientation of methodological priorities. Rather than treating moral content as something to be injected into artificial agents, we can treat moral behaviour as an empirically tractable outcome of norm-sensitive informational ecologies. Within this reconceptualisation, tools from Social Signal Processing and Affective Computing become central: they treat behaviour, interaction patterns, and expressive cues as data structures from which latent evaluative states can be inferred~\cite{Vinciarelli2009,Picard1997}. Our findings show that the same apparatus can be used not only to analyse human moral action, but to detect and quantify how that action is modulated by synthetic co-presence. The relevant question for machine ethics then becomes not ``what principles shall we encode?'', but ``how do specific technological affordances reshape the signal-to-inference mapping through which moral salience becomes behaviour?''

\noindent
Taken together, the chapter’s results therefore support a shift from \emph{agent-centric machine ethics} to an \emph{ecological ethics of synthetic presence}. The NAO robot, as deployed here, is not a moral agent to be judged, but a designed perturbation that reveals structural vulnerabilities in human evaluative systems. Its impact is LoA-dependent, personality-contingent, and epistemically graded. For an ethics of AI and robotics that aspires to be both philosophically serious and empirically grounded, the appropriate research goal is not the engineering of artificially virtuous minds, but the mapping and regulation of the moral topologies in which human and artificial systems are jointly embedded~\cite{Floridi2013,DeBrigard2021,Malle2016}. In this sense, the experiment does not solve the problem of machine ethics; it reframes it. Rather than asking whether robots can be moral, it asks how their mere presence redistributes moral salience, and how such redistributions can be measured, understood, and normatively governed in a world increasingly saturated with synthetic others.

%%%THE END!!!

