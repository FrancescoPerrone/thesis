\chapter{Experiment as Moral Displacement: An Empirical Investigation of the Watching Eye Effect in the Presence of Robots}
\thispagestyle{pprintTitle}


% Adjusting epigraph settings
\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushright}

% Setting the font and spacing for the epigraph
%\epigraph{\itshape \setstretch{1.2}But one thing is the thought, another thing is the deed, and another thing is the idea of the deed. The wheel of causality doth not roll between them.}{\small{Friedrich Nietzsche, \textit{Thus Spoke Zarathustra} (1883)}}
%
%

\section{Prefatory Remarks}
In this chapter we reconstruct the emprical core of the thesis- the cotrolled experiment setup wherein the \textit{Watching Eye Effect} (see \cref{chap:watching_eye,sec:test}) is transposed into a context involving the silent presence of a humanoid robot. 

While traditionally operationalised through pictorial stimuli or supernatural priming (\eg God concept, as explained in~\cref{sec:test}) the Watching Eye paradigm, when implemented alongside embodied artificial agents, invites a re-examination of what constitutes a “moral cue” and whether such cues retain their behavioral efficacy in post-human social configurations \fpincom{need to explain better why you used the word \textit{post-human} as it is not ported from the original version}.

Critically, the term 'experiment' within this chapter shall not be treated merely as a procedural template for variable manipulation, but rather as a morally and cognitively dense artefact: a formalised act of epistemic intervention aimed at distilling latent cognitive and affective structures through the staging of morality-salient conditions. The guiding hypothesis is not only behavioral—that robotic presence modulates charitable behavior—but metaethical:

\hypothesisheader{Synthetic Normativity - Moral Displacement Hypothesis }
\label{hyp:synthetic_affordances}
\begin{hypobox}
	Synthetic presences, though devoid of sentience, may acquire \textit{normative affordances} by virtue of their perceived ontology.
\end{hypobox}

Thus, what is observed is not simply what participants do, but how the moral architecture of the context is restructured in the presence of artificial agency. The presence of a robot may restructure the moral salience of a given context, modulating prosocial behavior through the attribution of artificial agency. \fpcom{or in a more agile version: "This study tests the following hypotheses: H1: The presence of a humanoid robot increases charitable behavior in observed participants. H2: The attribution of moral salience to a synthetic agent is mediated by its perceived intentional stance.}

This methodological commitment—examining the moral displacement hypothesis—requires that we reject simplistic interpretations of prosocial behavior as noise-free readouts of innate dispositions. Rather, we treat giving behavior as the contingent outcome of norm-sensitive cognitive systems under environmental perturbation (Greene \& Haidt, 2002; Haidt, 2001; Fedyk, 2017). It is within this epistemological architecture that the following experiment must be situated.

\section{Conceptualisation of the Experiment: Moral Decision-Making as Affected by Robotic Presence}

To interpret this experimental approach as a vehicle for moral-psychological insight, we must first clarify the ontological status of \textit{moral decision-making} in this context. Contrary to utilitarian framings, which reduce the act of giving to a \textit{form of constrained optimization} over social preferences (see \cref{chap:ethics_s}), the decision to donate in this setup is \textit{analytically framed} as an instantiation of \textit{moral salient attribution} under epistemic opacity. That is: the subject is unaware of being observed, unaware of being measured, and unaware of the true research objective. What it is revealed, therefore, is not the subject's \textit{explicit prosociality} but the \textit{implicit evaluative machinery} that mediates morally inflected choice under minimal social prompting.

The development of the 'Watching Eye' stimulus-- here, a child face printed on a medical charity brochure-- serves as a canonical cue for eliciting third-party moral concern via affective empathy and reputation sensitivity (Bateson et al., 2006; Nettle et al., 2013; Conty et al., 2016). However, by interposing the humanoid robot NAO-- silent, unprogrammed, yet ambiguously anthropomorphic-- we introduce an \textit{ontological anomalous agent} whose social category is neither fully human nor ethically inert. We hypothesise that:

\hypothesisheader{Synthetic moral refactor}
\label{hyp:synthetic_affordances}
\begin{hypobox}
	NAO robot functions not as a neutral observer but as a moral refactor: it warps the inferential pathways that normally lead from affective priming to moral action.
\end{hypobox}

Let us denote the core transition analytically:
\[
\mathscr{S} : \Sigma \xrightarrow{\;\mathscr{R}\;} \mathscr{D}
\]

where:
\begin{itemize}
	\item $\Sigma$ is the perceptual input space containing morally salient cues (e.g., brochure, eyes, room configuration),
	\item $\mathscr{R}$ is the robotic presence functioning as a \textit{modulator or perturbator},
	\item $\mathscr{D}$ is the domain of observable decisions (i.e., monetary donation, indexed across individuals).
\end{itemize}

In control conditions, the transition $\Sigma \rightarrow \mathscr{D}$ is uninterrupted; moral salience is preserved and converted into prosocial behavior via mechanisms well-theorised (see \cref{chap:prosicial_b}) in social and evolutionary psychology (Haley \& Fessler, 2005; Shariff \& Norenzayan, 2007). In robotic conditions, however $\mathscr{R}$ disturbs the mapping—perhaps by displacing emphatic identification, perhaps by reconstituting the room’s normative field, or perhaps by functioning as a cognitive decoy (cf. Złotowski et al., 2015). Each possibility carries distinct implications for the architectural design of ethical machines and for understanding how humans reconfigure their moral frame in response to synthetic others.

%%% NEW UPADATED CONENTE  N1
\subsection{Formalisation of Hypothesis and Experimental Logic}

The present experiment is best conceived not as a mechanistic probe into behavioral preferences, but as a structured perturbation within a normatively encoded cognitive system. Specifically, it seeks to investigate \textbf{how robotic presence modulates human moral decision-making} under conditions of minimal priming and perceptual constraint. Unlike traditional paradigms that treat prosociality as an output of deliberative utility calculus, the design employed here foregrounds the \textbf{pre-reflective inferential machinery} that converts perceptual-affective cues into morally salient behavior.

At its epistemic core, this experiment operates as a \textbf{perturbative test of moral salience transmission} — that is, whether a morally charged perceptual cue (e.g., the face of a child in need) is successfully converted into a prosocial behavioral output (monetary donation), and how that transmission is modulated, disrupted, or reframed by the passive presence of a \textbf{non-agentic but anthropomorphically encoded entity} (\ie, the NAO robot).

To formalize the interpretive structure of this transformation, let us denote:

\begin{itemize}
	\item $\Sigma$: the perceptual-affective input space (including the Watching Eye stimulus, spatial layout, and ambient cues)
	\item $\mathscr{R}$: robotic presence, ontologically positioned between artifact and agent
	\item $\mathscr{D}$: the moral decision space (observable as donation behavior)
\end{itemize}

The operative hypothesis can be expressed as a probabilistic modulation of expected moral output:

\[
\mathscr{R} \notin \Sigma \Rightarrow \mathbb{E}[f(\Sigma)] = D_{\text{prosocial}} \quad \text{(Control condition)}
\]
\[
\mathscr{R} \in \Sigma \Rightarrow \mathbb{E}[f(\Sigma \cup \mathscr{R})] = D_{\text{attenuated}}
\]
where:

\[D_{\text{attenuated}} < D_{\text{prosocial}} \quad \text{(Robot condition)}\]

Here, the notation $\mathbb{E}[f(\cdot)]$ denotes the \textbf{expected behavioral output} of the cognitive-affective system under a given set of environmental conditions. The function $f(\cdot)$ captures the internal inferential transformation by which perceptual-affective cues—such as the Watching Eye stimulus—are mapped onto discrete moral actions, in this case, the act of anonymous donation. Crucially, the expectation operator $\mathbb{E}[\cdot]$ signals that we are not describing a deterministic relation, but rather the \textit{aggregate tendency} across a psychologically heterogeneous population. It reflects the statistical structure of the behavioral response field rather than individual-level causality.

\subsection{Methodological Design: Inferring Moral Perturbation through Controlled Artificial Co-presence}

To regard an experimental setting as a generator of knowledge, rather than a mere data collection routine, demands that its internal architecture be epistemically justifiable and ontologically transparent. In this respect, every stage of the experimental method presented here is conceived not simply as procedural necessity, but as epistemic filtering: a sequence of deliberate constraints designed to isolate latent variables within the perceptual and normative landscape of the participant.

At its core, the experimental logic operationalises the following proposition:

% Corpo del documento
\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]

where:
\begin{itemize}
	\item $\delta_m$ denotes a deviation in moral decision (quantified as donation behavior),
	\item $\alpha_E$ represents environmental moral cues (Watching Eye),
	\item $\beta_C$ indexes control factors (psychometric variables, demographic traits),
	\item and $\gamma_R$ captures the effect of robotic presence.
\end{itemize}

The experimental setting is thus a structured interrogation of whether $\gamma_R \neq 0$ under conditions in which $\alpha_E$ and $\beta_C$ are held constant or accounted for. If confirmed, such deviation would instantiate a moral displacement: a case in which a non-sentient co-agent modulates human ethical output without any explicit instruction, coercion, or intervention. \fpincom{add link to relevant hypothesis and check condition "not zero"}

The following experimental procedure was implemented to ensure maximal control over environmental affordances while preserving participant naivety concerning the true moral dimension under investigation.

%%% NEW CONTENT N6
\subsection{Experimental Design: Structuring Moral Decision-Making under Controlled Perturbation}

Hence, this experimental protocol was designed to maximize epistemic opacity while ensuring structural consistency across conditions. The experimental procedure was constructed as a formal instantiation of the perturbative logic outlined in the operational models introduced above:

\[
\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)
\]
\[
\mathscr{R} \notin \Sigma \Rightarrow \mathbb{E}[f(\Sigma)] = D_{\text{prosocial}} \quad \quad \mathscr{R} \in \Sigma \Rightarrow \mathbb{E}[f(\Sigma \cup \mathscr{R})] = D_{\text{attenuated}}
\]

where $\delta_m$ denotes measurable deviation in moral action (donation behavior), $\alpha_E$ represents environmental moral cues (specifically the Watching Eye stimulus operationalized via a charity poster), $\beta_C$ captures individual differences in empathic, systemizing, and personality traits, and $\gamma_R$ indexes the presence of the robotic agent as an ontological perturbator.

Every stage of the experimental flow was designed to instantiate this tripartite structure, ensuring that the only manipulated variable was the robotic co-presence ($\gamma_R$) while $\alpha_E$ and $\beta_C$ were carefully controlled or measured.

Participants were recruited through two parallel channels: internal advertisements within the university's facilities (targeting primarily Computing Science undergraduates) and through the subject pool database maintained by the School of Psychology at the University of Glasgow. Eligibility criteria included a minimum age of 17 years, British nationality (verified by passport on arrival), and—where applicable—a restriction excluding Computing Science students from the subject pool recruitment arm (see \cref{subsec:participants} below.)

Participants were randomly assigned to experimental conditions (\textit{Control} or \textit{Robot}) \textbf{prior to arrival} using a simple randomization procedure to ensure allocation concealment. This pre-arrival assignment was critical to maintaining procedural symmetry across conditions and to minimizing any anticipatory contamination of moral cue salience. The experiment unfolded through the following logically structured stages:

\protocolheaderNoCounter{Experimental design for watching-eye priming under robotic displacement}
\label{prot:watching_eye_design}
\begin{protocolbox}
	\begin{enumerate}[label=\textbf{Stage \arabic*:}, leftmargin=2cm]
		
		\item \textbf{Arrival and Initial Framing}
		
		Participants were individually welcomed at the entrance of the School of Computing Science at the University of Glasgow. They were informed, exclusively in writing, that the study concerned the measurement of personality traits in a representative sample of the local population. No mention of donation opportunities or robotic presence was made at any stage prior to the experiment.
		
		\item \textbf{Environmental Exposure and Moral Salience Priming}
		
		Participants entered an isolated experimental room already pre-configured depending on their assigned condition. In both the Control and Robot conditions, a poster prominently featuring a photograph of a child beneficiary from a medical charity (\textit{Operation Smile}) was affixed to the wall facing the participant. This poster served as the Watching Eye stimulus ($\alpha_E$), operationalizing latent reputational salience.
		
		In the Robot condition, a Softbank Robotics NAO robot was placed passively within the room, configured in "autonomous life mode" to simulate breathing motions and minimal spontaneous head tracking. The robot was programmed to track participants' faces only upon direct eye contact, ensuring minimal but non-negligible affordance of social presence ($\gamma_R$).
		
		\item \textbf{Completion of Psychometric Instruments}
		
		Each participant was asked to complete three psychometric questionnaires:
		\begin{itemize}
			\item \textbf{Empathizing Quotient (EQ)}: measuring the participant's capacity for affective resonance and perspective-taking \cite{baroncohen2004}.
			\item \textbf{Systemizing Quotient (SQ)}: capturing cognitive preference for rule-based, systematized understanding of the environment \cite{baroncohen2003}.
			\item \textbf{Big Five Inventory-10 (BFI-10)}: providing a concise measurement of personality traits across five dimensions—Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism \cite{rammstedt2007}.
		\end{itemize}
		
		The inclusion of these instruments was methodologically mandated by the model component $\beta_C$: to quantify and later control for individual cognitive-affective traits that might independently influence moral decision-making. Thus, $\beta_C$ operationalizes a corrective layer, preventing dispositional noise from confounding the perturbative effect of $\gamma_R$ on $\alpha_E$.
		
		\item \textbf{Monetary Compensation and Moral Decision Opportunity}
		
		Upon completing the questionnaires, participants received £10 in ten individual £1 coins. They were then presented with a written form acknowledging receipt of payment and informing them—subtly and non-coercively—of the opportunity to anonymously donate any portion of their compensation to the charity. A green opaque charity box was positioned discretely in the room to collect donations, ensuring that participants perceived their actions as genuinely anonymous, thus preserving the internal validity of $\delta_m$ as a spontaneous moral action rather than a strategic one.
		
		\item \textbf{Exit and Data Collection}
		
		After finalizing their decisions, participants exited the room individually. An experimenter subsequently recorded the amount donated, retrieved the completed psychometric forms, and anonymized participant identifiers for analysis.
		
	\end{enumerate}
\end{protocolbox}

This five-stage protocol was systematically designed to instantiate a constrained, high-fidelity operationalization of the theoretical constructs previously formalized. Every procedural step was dedicated to ensuring that any observed attenuation in moral behavior could be legitimately attributed to the semiotic and social ambiguity introduced by $\mathscr{R}$, rather than to uncontrolled environmental or dispositional confounds. 

The structure of the experiment thus functions not merely as a logistic scaffold, but as a tightly engineered epistemic probe into the liminal interplay between environmental moral cues, robotic co-presence, and human moral-evaluative cognition.


%%% END NEW CONTENT N6


\subsection{Participants as Agents under Constraint}
\label{subsec:participants}

Seventy-three participants were recruited under the condition of epistemic \textit{naïveté}—a design choice intended to replicate the pre-reflective nature of many moral decisions in everyday life. That is, participants were never informed of the donation component in advance, nor were they given any cues that their decisions would be measured along ethical dimensions. This design choice aligns with the methodological imperative in experimental moral psychology to preserve the authenticity of affective-moral judgments (Greene et al., 2001; Haidt, 2001; Fedyk, 2017).

Each participant received a standard monetary compensation of £10, delivered in ten individual £1 coins. This choice is not incidental. The granular structure of the payment serves to increase the opportunity for \textit{moral modulation}; a single-note payment might discourage partial donations, thereby reducing the variance of observed prosocial behavior. Granularity here is not merely a technical concern—it is a moral affordance strategy (cf. Hutchins, 1995; Clark, 1997).

Demographically, participants were drawn from two sources:

\fpcom{Here better use the version from the article since it appears to be more agile and readable in terms of style and language.}

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\rightskip1.5cm
	\leftskip1cm
	\item[1. ] Computing Science undergraduates (n=30), and
	\item[2. ] Psychology subject-pool participants (n=43) via the University of Glasgow’s Institute of Neuroscience and Psychology.
\end{itemize}

Both sources were filtered through inclusion criteria to ensure homogeneity in nationality (British), legal adulthood (17+), and naïveté to the experimental purpose. This careful curation was essential to reduce background moral-cultural noise (cf. Henrich et al., 2010), and to ensure that any signal detected in the data could be confidently attributed to contextual rather than dispositional variance.

\subsection{Experimental Conditions: The Robotic Displacement Hypothesis}

Participants were randomly assigned to two conditions:

\begin{itemize}
	\item \textbf{Control}: Brochure present; no robot in room.
	\item \textbf{Robot}: Brochure present; NAO robot in autonomous life mode.
\end{itemize}

In the \textbf{Robot condition}, the NAO unit was configured in such a way that it neither spoke nor interacted explicitly with the participant. Its only active features were:

\begin{itemize}
	\item \textbf{Simulated breathing}, designed to evoke lifelikeness;
	\item \textbf{Eye-tracking}, activated only upon participant gaze.
\end{itemize}

\fpcom{Here too check the version in the article since it might be better written}
This configuration was chosen with care: too interactive a robot risks introducing anthropomorphic biases, while a purely inert robot would lack the semiotic ambiguity necessary for the displacement effect. The minimalistic design renders the robot a kind of moral opacity operator: it does not directly interfere with the ethical stimulus, but its presence may modulate the interpretive bandwidth through which that stimulus is processed.

\fpcom{Here we need a link to Floridi work on moral patient and agent...}
In effect, the NAO robot here plays the role of a silent norm deflector—not an actor, but a presence whose social affordances are sufficiently ambiguous to provoke reconfiguration in the normative inference mechanisms of human subjects (Złotowski et al., 2015; Coeckelbergh, 2010).

%%%NEW CONTENT N5

To verify the demographic equivalence of the experimental groups, a series of inferential tests were conducted across gender distribution, age, and educational background. A chi-squared test assessing gender yielded no significant difference across conditions ($p = 1.00$, after False Discovery Rate (FDR) correction), nor did a t-test evaluating mean age ($p = 1.00$, after FDR correction) or a chi-squared test examining academic background ($p = 1.00$, after FDR correction). 

The application of FDR correction, following the Benjamini-Hochberg procedure, reinforces the robustness of these findings against multiple testing artifacts. These results substantiate the assumption that any observed divergences in prosocial behavior are not attributable to pre-existing demographic imbalances, but are emergent properties of the experimental manipulation itself—that is, the introduction or absence of the robotic entity as a socially and morally perturbative stimulus. In epistemic terms, the groups are demographically symmetrical, and the inferential focus can thus justifiably center on the semiotic and normative ramifications of $\mathscr{R}$.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/demographic_balance_table.pdf}
	\caption{Demographic balance tests across experimental conditions. The table reports the original and FDR-corrected p-values for comparisons of gender, age, and group membership across conditions. No significant differences were detected after correction, supporting the assumption of demographic equivalence between groups.}
	\label{tab:dem_balance}
\end{table}
%%%END NEW CONTENT N5

\fpcom{All content about related to the "experiment" i.e. the analysis of the data we have carried out, which is placed abve, should go after this sections (including probably the one below)}


\subsection{Temp below before part of 6.2}

Importantly, the robotic presence $\mathscr{R}$ is not modeled as an agent that exerts influence through interaction or instruction, but as a \textbf{semiotic modulator}—an ontologically ambiguous presence that perturbs the interpretive field. Within this framework, the observed attenuation of prosocial behavior is not to be interpreted as a direct suppression of empathy \textit{per se}, but rather as the result of a structural reconfiguration in the \textbf{normative encoding schema}: the internal representational system by which moral salience is assigned to environmental cues. The presence of $\mathscr{R}$ modifies the topology of this schema, thereby altering the inferential weight carried by otherwise salient moral signals.


\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/conditions.pdf}
	\caption{Experimental conditions are behaviorally and procedurally identical, differing only in robotic presence.}
	\label{tab:experimental_conditions}
\end{table}

Both conditions were designed to be \textbf{epistemically symmetrical}, ensuring that any observed difference in moral behavior could be attributed exclusively to the ontological modulation introduced by $\mathscr{R}$.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/variables.pdf}
	\caption{Measured variables and psychometric constructs used in inferential modeling of moral behavior.}
	\label{tab:key_variables}
\end{table}

This formal and operational framework enables us to treat the experiment as a constrained instantiation of a more general epistemic function: namely, how minimally expressive artificial agents can reshape the moral topology of a decision-making environment by altering the interpretive affordances of its cues.

%%% END N1

%%% NEW CONTENT N2
\vspace{0.3cm}
\noindent
The dataset was subsequently cleaned and preprocessed in preparation for inferential modelling. All variable names were standardized to lowercase to maintain syntactic uniformity within the analytic pipeline. A binary indicator, \texttt{donated\_anything}, was derived to encode the presence or absence of moral action—specifically, whether participants donated any amount at all. Additionally, the experimental condition was recoded into a numerical binary variable, \texttt{condition\_bin}, where $0 = \text{Control}$ and $1 = \text{Robot}$, facilitating direct incorporation into generalized linear models.

Descriptive statistics revealed no immediate distributional anomalies in age, psychometric scores, or monetary donation values. This preliminary finding supports the assumption of epistemic symmetry across experimental groups, validating our earlier claim that the perturbation introduced by $\mathscr{R}$ operates primarily at the interpretive rather than dispositional level.

Figures~\ref{fig:age_distribution_by_group} and~\ref{fig:donation_distribution_by_condition} offer visual corroboration: the former presents the distribution of participant ages across experimental conditions, while the latter depicts the empirical shift in donation behavior, contrasting the control and robot groups. Both histogram and violin plot representations were rendered using a unified visual palette drawn from the thesis’s typographic stylesheet, ensuring visual coherence between empirical outputs and the formal aesthetic of the larger argumentative structure. This stylistic continuity is not merely cosmetic; it reflects a commitment to epistemic unity across representational modalities.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/age_distribution_by_group.png}
	\caption{Age distribution across experimental conditions. Histogram representation confirms no major between-group demographic divergence.}
	\label{fig:age_distribution_by_group}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_distribution_by_condition.png}
	\caption{Distribution of donation behavior by condition. Violin plot representation visualizes the heavier tail in the robot group, supporting the hypothesized interpretive perturbation.}
	\label{fig:donation_distribution_by_condition}
\end{figure}
%%% END NEW CONTEN N2

%%% NEW CONTENT N3
\vspace{0.3cm}
\noindent
Initial descriptive statistics (Table~\ref{tab:descriptive-stats}) lend preliminary support to the theoretical expectation of attenuated prosocial behavior in the presence of a robotic entity. Specifically, the mean donation in the Control group (£1.89) exceeds that of the Robot condition (£1.17), suggesting a directional trend consistent with the hypothesis that $\mathscr{R}$ functions not as a neutral co-presence, but as an ontologically ambiguous disruptor of moral affordance structures. This observed divergence in average donations is not merely numerical—it aligns with our prior interpretation of robotic presence as a semiotic refractor that weakens the moral salience of otherwise affectively potent stimuli.

Further group-level comparisons reveal that Control participants report marginally higher scores on the Empathizing Quotient (M = 45.94 vs. 42.82) and greater Openness to Experience (M = 1.86 vs. 1.32). While these differences do not yet reach statistical significance, they may serve as psychologically relevant covariates in the inferential models to follow. Conversely, the Robot group exhibits a slightly higher mean age and elevated Systemizing Quotient scores, suggesting potential variability in cognitive-affective architecture that may influence responsiveness to synthetic observers. Importantly, these observations remain exploratory at this stage, pending further statistical testing to determine whether they index meaningful shifts in interpretive framing rather than random variance.

\begin{table}[H]
	\label{tab:descriptive-stats}
	\centering
	\includegraphics[width=\textwidth]{tables/descriptive_highlights_table.pdf}
	\caption{Summary of central tendencies for key moral and psychological variables. The Robot condition shows numerically lower donation amounts and empathizing scores, suggesting potential attenuation effects of passive robotic presence.}
\end{table}
%%% END N3

%%%NEW CONTENT N4
\vspace{0.3cm}
\noindent
To assess whether the observed divergence in donation behavior between the Control and Robot conditions constitutes a statistically credible effect, both parametric and nonparametric inferential techniques were employed. A chi-squared test on total donation sums yielded a significant result ($\chi^2 = 4.25$, $p = .039$), affirming the directional claim established in the original hypotesys that:

\statementheader{Conclusion}{Impact of moral refactor}
\label{conc:impact_moral_refactor}
\begin{hypobox}
robotic presence is associated with attenuated charitable behavior at the aggregate level.
\end{hypobox}

\fpcom{It is not associated with attenuated charitable behaviour untill when we link monetary donations in terms of measurable money to charitable bahviour in the dedicated chaper.}

However, a Mann-Whitney U test comparing the full donation distributions between conditions failed to reach statistical significance ($U = 777$, $p = .194$), suggesting that while the central tendencies of the two groups differ, the overall distributions remain substantially overlapping. This implies that the effect of robotic presence, though observable in total group output, does not manifest uniformly or robustly at the level of individual moral acts.

\fpcom{Ask Alessandro. We have never used U test in the other articles. This is just a draft.}

A bootstrapped estimate of the mean donation difference ($\Delta M = £0.71$) further supports the interpretation of a modest directional effect. However, the 95\% confidence interval surrounding this estimate spans zero (CI = [–£0.33, £1.79]), underscoring the inherent variability and epistemic fragility of the observed attenuation. These findings reinforce the reading of robotic presence not as a deterministic suppressor of moral behavior, but as a \textbf{subtle perturbator of collective prosociality}—its influence detectable in the structure of aggregated behavior, yet too diffuse or psychologically mediated to yield reliable individual-level contrasts without more granular stratification.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/statistical_tests_table.pdf}
	\caption{Inferential statistical comparisons of donation behavior across conditions. The chi-squared test shows a significant difference in donation totals, while the Mann-Whitney U and bootstrapped estimates provide converging but non-significant results on distributional shape and directional effect size.}
	
	
	\label{tab:key_variables}
\end{table}

%%% END N4


%%% NEW CONTENT N4.1
\vspace{0.3cm}
\noindent
Inferential statistical testing corroborates the initial descriptive trends, albeit with nuanced gradations in evidential strength. A chi-squared test applied to the aggregate donation sums across experimental conditions yielded a statistically significant divergence ($\chi^2 = 4.25$, $p = .039$), affirming the core hypothesis that the presence of a robotic observer perturbs the inferential pathway from moral salience recognition to prosocial output. 

However, this significance attenuates when subjected to distribution-sensitive analyses: a Mann–Whitney U test failed to detect a reliable shift in the overall distributions of donation amounts ($U = 777$, $p = .194$), suggesting that central tendency shifts are accompanied by substantial individual variability. A bootstrapped estimation of the mean donation difference ($\Delta M = £0.71$) reinforced this pattern of modest directional change, but the 95\% confidence interval [$–£0.33$, £1.79$]$ encompasses the null, underscoring the epistemic fragility of the observed effect.

Together, these results imply that while robotic presence operates as a salient perturbator at the level of group aggregates, its impact at the individual decision level is probabilistic, diffuse, and structurally unstable—an effect coherent with the notion of robots as \textbf{liminal agents} within the moral-evaluative architecture of human cognition. In this reading, the robot is not a causal force imposing behavioral regularities, but a semiotic anomaly: a presence that destabilizes the otherwise inferentially coherent conversion of moral salience into prosocial action.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_effect_by_condition.png}
	\caption{Mean donation amounts by experimental condition, with 95\% bootstrapped confidence intervals. Participants in the control condition donated more on average than those in the robot condition, aligning with the hypothesis that robotic presence attenuates the inferential mapping from moral salience to prosocial action. Confidence intervals reveal substantial overlap, indicating that while the aggregate effect reaches significance in total sums, individual-level variability remains high.}
	\label{fig:age_distribution_by_group}
\end{figure}

%%% END N4.1

%%% NEW CONTENT N7
Beyond establishing the statistical significance of the observed differences, it is epistemically imperative to quantify the magnitude of behavioral perturbation induced by robotic presence. The following analyses introduce both parametric and nonparametric effect size metrics to characterise the structural modulation of moral decision-making.

\subsection{Quantification of Behavioral Modulation: Parametric and Nonparametric Effect Sizes}

To complement the statistical significance analyses, the magnitude of the observed behavioral modulation was quantified using both parametric and nonparametric effect size metrics. Specifically, Cohen’s $d$ and Cliff’s $\Delta$ were employed to capture the standardised and ordinal dimensions of effect magnitude, respectively.

\vspace{0.3cm}
\noindent
The metrics are formally defined as follows:

\vspace{0.2cm}
\noindent
Cohen's \( d \):
\[
d = \frac{\bar{x}_1 - \bar{x}_2}{s_p} \quad\text{where}\quad s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}
\]
where:
\begin{itemize}
	\item \(\bar{x}_1, \bar{x}_2\) are the group means,
	\item \(s_1, s_2\) are the group standard deviations,
	\item \(n_1, n_2\) are the respective group sizes.
\end{itemize}

\vspace{0.3cm}
\noindent
Cliff's Delta \( (\Delta) \):
\[
\Delta = \frac{\#(x>y) - \#(x<y)}{n_x n_y}
\]
where:
\begin{itemize}
	\item $\#(x>y)$ and $\#(x<y)$ represent the number of pairwise comparisons in which an observation in group $x$ exceeds or falls below one in group $y$.
\end{itemize}

\vspace{0.3cm}
\noindent
The results indicate that Cohen’s $d$ for donation amounts between the Control and Robot conditions was approximately $d = 0.30$, corresponding to a small to moderate standardised effect size. In parallel, Cliff’s Delta was estimated at approximately $\Delta = 0.20$, confirming a modest but consistent ordinal shift in prosocial behavior.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_density_by_condition.png}
	\caption{Distribution of donation amounts by experimental condition. Kernel density estimates illustrate the probability density of donation values within each group. The distribution for the control group exhibits a higher central mass and heavier right tail relative to the robot condition, suggesting a directional attenuation of high-value prosocial acts in the presence of the robotic entity.}
	\label{fig:donation_density}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{new_plots/donation_mean_with_se.png}
	\caption{Mean donation amounts with standard error bars by condition. The control group exhibited a higher mean donation (£1.89) compared to the robot group (£1.17), aligning with the hypothesis that robotic presence modulates, rather than eliminates, the human inferential machinery responsible for translating moral salience into actionable generosity.}
	\label{fig:mean_donation}
\end{figure}


These metrics substantiate the interpretation that robotic presence modulates, but does not obliterate, the inferential machinery that governs moral salience conversion. The moral field is not annihilated but refracted; its coherence weakened but not rendered inert. 

Such a pattern resonates with the broader theoretical framing advanced in this work:

\statementheader{Conclusion}{Amplitude of moral refactor}
\label{conc:amplitude_moral_refactor}
\begin{hypobox}
synthetic agents do not operate as binary moral suppressors but rather as \textbf{probabilistic refractors}—entities that modulate the amplitude and directionality of moral cognition without fully displacing its normative orientation.
\end{hypobox}

%%% END CONTENT N7


%%%NEW CONTENT N8
\subsection{Latent Trait Structures and Individual Modulation of Moral Perturbation}

To deepen the analysis of how individual differences condition the moral impact of robotic presence, participants were clustered according to their standardized psychometric profiles. This dimensionality reduction and clustering procedure serves to refine the \(\beta_C\) term in the operational model \(\mathscr{P}(\delta_m) = f(\alpha_E, \beta_C, \gamma_R)\), replacing scalar trait vectors with structurally defined personality constellations.

Seven variables were included in the initial psychometric space: Empathizing, Systemizing, Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Each participant’s score vector was standardized and submitted to a Principal Component Analysis (PCA), yielding two orthogonal principal components that preserved the most informative axes of variance.

The reduced space was then subjected to a $k$-means clustering algorithm with \(k = 3\), producing three psychologically coherent personality clusters. These clusters were visualized in the reduced PCA space (Figure~\ref{fig:personality-clusters-pca}) to confirm interpretability and approximate structural separability.


\fpcom{Here a lot of work was done to produce a mathematical justification to n=3. I did not mention any of it as it seemed to me going outside the scope of the thesis but potentially it needs some level of mention. If we decide to include a justification for n=3 then, the only problem I have is to go back to the jupyther notebook I used to canlulate it as I don't remember if I have it in the old office laptop.}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{new_plots/personality_clusters_pca.png}
	\caption{Participants clustered in PCA-reduced psychometric space, colored by cluster identity and shaped by experimental condition.}
	\label{fig:personality-clusters-pca}
\end{figure}

This framework offers a structural lens through which to examine the interaction between moral perturbation and trait-defined cognitive-affective style. Rather than treating individual differences as additive covariates, this clustering approach models them as latent psychological regimes that modulate the inferential stability of moral salience recognition under robotic presence.

\fpcom{Mathematical justification starts.}

The number of clusters was determined using the elbow method applied to the within-cluster sum of squares (WCSS) in conjunction with the silhouette coefficient, which jointly indicated a stable local optimum at \(k = 3\). This balance point reflects the minimal number of clusters needed to meaningfully partition participants into psychologically distinct subgroups without overfitting idiosyncratic noise. From a conceptual standpoint, this solution aligns with the hypothesis that perturbation effects may be differentially refracted through a small set of discrete cognitive-affective configurations, each constituting a distinct normative filter through which the robotic presence is interpreted.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{new_plots/cluster_elbow_silhouette.png}
	\caption{Participants clustered in PCA-reduced psychometric space, colored by cluster identity and shaped by experimental condition.}
	\label{fig:personality-clusters-pca}
\end{figure}

While the silhouette analysis revealed a local maximum at $k = 9$, this spike is best interpreted as a statistical artifact arising from over-partitioning a relatively small dataset. The high silhouette value at this resolution reflects tightness in small clusters, not psychological coherence. In contrast, $k = 3$ corresponds to the elbow point in the inertia curve and yields clusters of interpretable size and structure. This choice balances model fit with parsimony, ensuring that the derived clusters correspond to meaningful cognitive-affective configurations rather than idiosyncratic separations. Accordingly, we retain $k = 3$ as the optimal number of clusters for both methodological rigor and interpretive validity.

\fpcom{Mathematical justification end.}

Cluster-specific donation behavior further reveals heterogeneous responses to moral cues across subgroups (Figure~\ref{fig:donation-by-cluster}). In Cluster 1, the presence of the robot appears to strongly attenuate donation, while in Clusters 0 and 2, the difference is negligible or weak. These patterns suggest that \(\gamma_R\) does not act uniformly upon all cognitive-affective configurations, but instead interacts with emergent psychological structures in non-linear and potentially regime-dependent ways.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{new_plots/donation_by_cluster_and_condition.png}
	\caption{Mean donation amount by experimental condition within each personality cluster, derived from $k$-means analysis on psychometric trait profiles. Error bars reflect standard deviation. Clusters are indexed from 0 to 2 and represent latent cognitive-affective subgroups. Notably, Cluster 1—which donates less under robotic presence—tends to exhibit higher systemizing and lower empathizing scores. This suggests a diminished susceptibility to affectively encoded moral cues in the presence of ontologically ambiguous agents, consistent with a refracted moral response under $\gamma_R$ perturbation.}
	
	
	\label{fig:donation-by-cluster}
\end{figure}

Such findings deepen the interpretation of robotic presence not as a global suppressor of moral behavior, but as a semiotic agent whose moral salience is differentially refracted through distinct personality configurations. The robot's effect is thus not fixed, but \textbf{contingently realized through latent cognitive structures}—structures now made visible via this clustering framework.

\statementheader{Conclusion}{Contingent structure of cognitive modulation}
\label{conc:clustered_moral_refraction}
\begin{hypobox}
	The moral impact of robotic presence is not globally uniform but emerges through contingent interactions between artificial agents and latent psychological regimes. Personality clustering reveals that synthetic moral perturbation is structurally modulated—its amplitude and valence refracted through cognitive-affective configurations that define the subject’s interpretive topology.
\end{hypobox}



%%%END CONTENT N8

%%%NEW CONTENT N10

\subsection{Cluster-Specific Regression Analysis of Robotic Perturbation}

To examine whether specific cognitive-affective regimes are differentially sensitive to robotic presence, a stratified linear regression analysis was conducted within each personality cluster. Donation amount served as the dependent variable, and experimental condition (Control vs. Robot) was the primary predictor.

In \textbf{Cluster 1}, the robot condition was associated with a substantial reduction in donation (\(\beta = -1.33\)), approaching conventional significance (\(p = .091\)) and accounting for a modest portion of the variance (\(R^2 = 0.087\)). This suggests a pronounced moral perturbation effect within this group. In contrast, \textbf{Clusters 0 and 2} exhibited negligible effects (\(\beta \approx 0\) and \(\beta = -0.28\), respectively; both \(p > .70\)), indicating that robotic co-presence does not uniformly alter moral output across psychological profiles.

These results substantiate the hypothesis that the ethical salience of robotic presence is modulated not solely by the intensity of individual traits, but by the emergent configuration of those traits—what may be conceptualized as \textit{psychological ecologies}: interdependent cognitive-affective structures within which moral salience may either propagate, fragment, or collapse when exposed to artificial co-agents.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{new_plots/cluster_regression_coefficients.png}
	\caption{Regression coefficients for the robot condition within each personality cluster, with 95\% confidence intervals. While Clusters 0 and 2 exhibit near-zero or non-significant effects, Cluster 1 shows a marked negative coefficient, indicating a stronger attenuation of prosocial behavior in the presence of the robot. This pattern supports a differentiated model of moral responsiveness, contingent on latent psychological configuration.}
	\label{fig:cluster-regression}
\end{figure}

\statementheader{Conclusion}{Differentiated moral sensitivity to robotic presence}
\label{conc:regression_cluster_specific}
\begin{hypobox}
	Robotic presence does not exert a uniform moral influence, but interacts differentially with distinct psychological ecologies. Cluster-specific regression analysis reveals that moral attenuation is concentrated within particular cognitive-affective regimes, indicating that the ethical salience of synthetic agents is not globally encoded but \textbf{emerges through structured trait configurations} that define the agent’s evaluative responsiveness.
\end{hypobox}

Yet such analysis still encodes classical assumptions—what happens if we relax them?

%%%END CONTENT N10

%%%NEW CONTENT N11

\subsection{Bayesian Estimation and Epistemic Gradient Framing}

In all preceding analyses, the difference in donation behavior between the Control and Robot conditions was evaluated through classical inference techniques—chi-squared, Mann–Whitney U, and OLS regression. However, each of these techniques imposes strict statistical assumptions (normality, homoscedasticity, independence) and forces evidence into binary categories: significant or not. The limitations here are not merely statistical—they are epistemic. Such frameworks fail to quantify degrees of belief or represent uncertainty as a distributional property of knowledge.

To move beyond these constraints, we applied Bayesian estimation. Bayesian methods are well-suited to small-to-medium datasets (\(N \approx 70\)) and allow for the modeling of uncertainty without arbitrary thresholds. By producing full posterior distributions, they permit statements about effect magnitude and direction that are probabilistically grounded rather than threshold-dependent.

\begin{table}[H]
	\centering
	\includegraphics[width=\textwidth]{tables/demographic_balance_table.pdf}
	\caption{Rationale for employing robust and Bayesian techniques in the analysis of donation behavior. Each method addresses different limitations of frequentist inference, enhancing the epistemic transparency and robustness of the findings.}
	\label{tab:bayesian_methods}
\end{table}


Using a hierarchical Bayesian model, we estimated the posterior distribution of the mean donation difference (\texttt{Control – Robot}). The posterior mean was approximately £0.70 in favor of the Control group, with a 95\% credible interval ranging from –£1.75 to +£0.30. Although the interval includes zero, its density is asymmetrically skewed toward negative values, suggesting directional evidence for an attenuating effect of robotic presence on donation behavior.

Unlike frequentist tests that collapse inferential nuance into p-value dichotomies, Bayesian inference permits epistemically richer conclusions: under our model and priors, the hypothesis that robotic presence reduces prosocial output is \textit{plausible, structured, and quantifiable}—though epistemically fragile. This final step reframes our inquiry: we are not adjudicating rejection versus acceptance but articulating a gradient of moral plausibility, located within a posterior distribution that reflects both uncertainty and structure.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{new_plots/posterior_donation_difference.png}
	\caption{Posterior distribution of the estimated donation difference between the Control and Robot conditions. The density curve skews toward negative values, indicating directional probabilistic evidence for attenuated donation behavior under robotic presence. The vertical dashed line at zero denotes the boundary of no effect. Bayesian inference supports the plausibility of moral salience attenuation, while explicitly representing its uncertainty as an epistemic gradient.}
	\label{fig:posterior-difference}
\end{figure}

\statementheader{Conclusion}{Gradient of belief under uncertainty}
\label{conc:bayesian_epistemics}
\begin{hypobox}
	Bayesian estimation reframes the effect of robotic presence not as a question of significance, but as a continuous epistemic field. Rather than affirming or rejecting, it articulates a structured probability of moral attenuation—anchoring belief not in categorical claims but in the asymmetry and topology of posterior distributions. The moral impact of $\mathscr{R}$ is thus rendered \textbf{plausible, uncertain, and gradient-valued}—a refractor not only of cognition, but of inference itself.
\end{hypobox}

\subsubsection{Clarification for Non-Expert Readers}

While Bayesian inference may appear technically distinct from the classical tests previously discussed, its philosophical value lies in its capacity to express uncertainty as a \textit{graded belief}, rather than as a binary decision. The posterior distribution shown in Figure~\ref{fig:posterior-difference} does not say that robotic presence \textit{definitely} reduces donations. Rather, it says this outcome is \textit{more likely than not}, given the data and model assumptions, and that the magnitude of this effect is plausibly around £0.70, but uncertain.

For readers more familiar with p-values, it’s important to note that some of the classical tests (e.g., Mann–Whitney \(U\)) did not return statistically significant results and became further attenuated under False Discovery Rate (FDR) correction. However, the Bayesian analysis was not intended to override or ‘rescue’ these null findings. Instead, it reframes the question. It asks not whether the data pass a specific threshold, but whether—given the structure and sparsity of our dataset—\textit{a directional pattern exists that is epistemically credible and transparently modeled}.

In this view, the absence of classical significance does not invalidate the Bayesian result; it clarifies the level of caution required in interpreting it. The Bayesian model incorporates that very uncertainty directly into its distribution. Rather than hiding it, it shows it. In doing so, it affirms not a fixed answer but a morally and scientifically meaningful hypothesis that:

\statementheader{Conclusion}{Gradient of the Impact of Moral Refactor}
\label{conc:bayesian_epistemics}
\begin{hypobox}
	the presence of a robot may, in some contexts and for some agents, reduce the likelihood of prosocial behavior—even if our evidence remains epistemically modest and gradated rather than definitive.
\end{hypobox}

The reader is invited to see the concluding statement in Conclusion~\ref{conc:impact_moral_refactor} for comparison against a non-Baesyan version of this conclusion.


%%%END CONTENT N11

%%%NEW CONTENT N9

\subsection{Interpreting Moral Perturbation through Latent Trait Regimes}

\fpcom{This subsection refers to the mathematical justification. If not needed it can be deleted including both comments to facilitate editind the latex file.}

The three personality clusters derived from $k$-means analysis were not only structurally coherent in trait space, but also revealed differential patterns of moral responsivity under robotic presence. Each cluster may be interpreted as a distinct cognitive-affective regime, modulating the transformation function $f(\cdot)$ that converts environmental moral cues $\Sigma$ into moral action $\delta_m$, particularly under perturbation by $\mathscr{R}$.

\textbf{Cluster 0} demonstrates behavioral invariance across experimental conditions, with mean donation amounts stable regardless of robotic presence. This suggests a transformation function in which:
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] \approx \mathbb{E}[f(\Sigma)],
\]
indicating that the inferential machinery responsible for mapping moral salience to action remains functionally stable even under semiotic perturbation. Individuals in this group appear robust to the refractive influence of $\mathscr{R}$, perhaps due to strong normative encoding of $\alpha_E$ that is unaffected by ambient ambiguity.

\textbf{Cluster 1}, by contrast, exhibits a marked attenuation in donation behavior in the Robot condition relative to Control. This cluster is characterized by elevated systemizing and reduced empathizing scores, suggesting a cognitive style less responsive to affectively encoded cues. The transformation function here appears significantly degraded in the presence of $\mathscr{R}$:
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] \ll \mathbb{E}[f(\Sigma)].
\]
This implies a breakdown in the propagation of moral salience through the internal evaluative system—participants perceive the stimulus, but its normative weight is not successfully transduced into action. The robotic presence functions here not merely as noise, but as a semiotic deflector that collapses the affective-moral inference pathway.

\textbf{Cluster 2} reveals a milder attenuation in donation, suggesting a transformation function of intermediate integrity:
\[
\mathbb{E}[f(\Sigma \cup \mathscr{R})] < \mathbb{E}[f(\Sigma)].
\]
Participants in this group are partially susceptible to the refractive presence of $\mathscr{R}$, but retain enough affective sensitivity to maintain baseline levels of moral responsiveness. This configuration suggests a partial encoding of robotic co-presence as a moral cue, one that distorts but does not entirely suppress the moral inferential arc.

Taken together, these three clusters instantiate a spectrum of moral perturbability. They reveal that $\gamma_R$ does not exert a uniformly suppressive force, but acts as a probabilistic refractor whose amplitude and directionality depend on the underlying cognitive-affective topology of the agent. In formal terms, each cluster realizes a distinct mapping from the perturbed moral environment $\Sigma \cup \mathscr{R}$ to prosocial behavior, mediated by the structural properties of $f(\cdot)$ latent within their psychological architecture.

\statementheader{Conclusion}{Cluster-dependent perturbation structure}
\label{conc:cluster_responsive_structure}
\begin{hypobox}
	The moral effect of robotic presence is structurally contingent upon the latent cognitive-affective architecture of the agent. Personality clusters reveal differentiated mappings from perturbed environments to moral action: some remain inferentially stable, others exhibit collapse or partial refractoriness. These findings confirm that $\mathscr{R}$ does not act as a universal moral suppressor but as a \textbf{structure-sensitive modulator}—its influence emerging only through interaction with specific psychological regimes.
\end{hypobox}


%%%END CONTENT N9

\section{Results and Interpretation: Quantifying the Moral Displacement Effect}

If the term experiment is to retain its epistemic dignity within moral psychology, its outputs must be interpretable not merely as statistical artifacts, but as structured signals—signatures of cognitive-affective systems interacting with normatively charged environments. In this section, we present the observed results from the two experimental conditions and propose a theoretical interpretation grounded in moral cognition, embodied social presence, and the normative semiotics of co-located artificial agents.

\subsection{Summary of Quantitative Findings}

The experimental dataset comprises \( N = 73 \) valid cases, distributed as follows:

\begin{itemize}
	\item \textbf{Control group} (no robot): \( n = 37 \)
	\item \textbf{Robot group} (robot present): \( n = 36 \)
\end{itemize}

Let the amount donated by participant \( i \) be denoted \( d_i \), and let \( c_i \in \{C, R\} \) indicate their condition (Control or Robot, respectively). The total donation by group is given by:

\[
D_C = \sum_{i:c_i=C} d_i = £66, \quad D_R = \sum_{i:c_i=R} d_i = £44.35
\]

This yields a \textbf{donation ratio}:

\[
\frac{D_C}{D_R} \approx 1.49
\]

A \textbf{\(\chi^2\) test for distributional difference} between the two groups returns:

\[
\chi^2 = \frac{(D_C - E)^2}{E} + \frac{(D_R - E)^2}{E}, \quad \text{where } E = \frac{D_C + D_R}{2}
\]

yielding a \textbf{p-value = 0.01}, which is statistically significant under conventional thresholds. The robustness of this finding was further confirmed by controlling for potential confounding variables (gender, age, educational background, psychometric scores), none of which yielded significant intergroup variation post-FDR correction.

Hence, the presence of a humanoid robot—non-interactive, passive, and ontologically ambiguous—reliably modulated moral behavior in an otherwise norm-stable context.

%%%NEW CONTENT N12 (CONCLUSIONS)
\subsection{Epistemic Synthesis and Closing Reflections}

This experiment, in its full epistemic arc, does not merely quantify behavior—it exposes the structural vulnerability of moral inference to ontological ambiguity. The presence of the humanoid robot did not suppress generosity in any universal or deterministic sense; rather, it modulated the internal transformation function by which environmental cues—such as the Watching Eye stimulus—are converted into prosocial action.

Crucially, this modulation was not homogeneous. It was contingent upon the psychological architecture of the individual. By clustering participants based on multidimensional trait constellations, rather than treating scalar traits in isolation, we revealed a latent moral topology: a structured evaluative landscape in which only certain cognitive-affective configurations were susceptible to refractive collapse in the presence of $\mathscr{R}$.

What emerges is a reconceptualization of synthetic social influence—not as intrinsically prosocial or antisocial, but as structurally contingent and psychologically asymmetrical. An agent that elicits generosity from one cognitive regime and inertness from another does not instantiate moral agency; it instantiates moral bifurcation. Its presence becomes an operator on moral space, reshaping affordances rather than issuing imperatives.

Methodologically, our progression from chi-squared to Bayesian inference mirrors a deeper philosophical movement: from fixed hypothesis testing to probabilistic epistemology. We have not merely asked “is there a difference?” but rather “how plausible is the difference?”, and more importantly, “for whom does it matter, and under what internal constraints?”

In its final synthesis, the experiment affirms that robotic presence perturbs moral behavior—not through coercion, communication, or mimicry, but through the silent deformation of evaluative inference. It does not alter moral content, but moral process. And in doing so, it invites us to reconsider the epistemic architecture of moral cognition itself.

This is not a study of certainty, but of its limits. And it reminds us that in moral psychology—as in epistemology—certainty is not the culmination of inquiry, but its terminus. A theory that ceases to entertain uncertainty is not resolved, but ossified. In this view, the experiment remains open—not as a failure of closure, but as a triumph of epistemic responsibility.


%%%END CONTENT N12 (CONCLUSIONS)

\subsection{Toward a Theory of Robotic Normative Interference}

To interpret these results as philosophically significant, we propose the following \textit{moral displacement hypothesis}:

\begin{quote}
	\textbf{The presence of a non-sentient yet humanomorphic artificial agent alters the normative topology of the environment such that affective priming cues lose moral traction, resulting in diminished prosocial behavior.}
\end{quote}

This result should not be interpreted as a simple \textit{attenuation} of the Watching Eye effect. Rather, it is a case of \textbf{semiotic interference}: the robotic presence functions as a competing moral symbol—a silent node of ambiguous intentionality—displacing the affective salience of the child's face and thereby weakening the cognitive-affective circuit that leads from perception to moral action.

In cognitive terms, this can be formalized as:

\[
\mathscr{M}_i \;=\; f(\sigma_{WE}, \pi_i, \rho_R)
\]

where:

\begin{itemize}
	\item $\mathscr{M}_i$ denotes the moral salience assigned to a stimulus by participant \( i \),
	\item $\sigma_{WE}$ is the Watching Eye stimulus strength,
	\item $\pi_i$ is the dispositional moral profile (via EQ, SQ, BFI),
	\item and $\rho_R$ is the robotic presence function, modulating \(\sigma_{WE}\) via attentional or interpretive interference.
\end{itemize}

The significant reduction in \(\mathscr{M}_i\) (as inferred from diminished donation) is therefore not explained by \(\pi_i\) (which remains statistically constant across conditions), but by the variation in \(\rho_R\). This substantiates the claim that robotic presence modifies \textit{moral field intensity}—not by providing explicit moral information, but by distorting the interpretive vectors through which existing stimuli are processed.

\section{Normative Implications: Robots as Epistemic Agents of Moral Ambiguity}

What emerges from this empirical configuration is a profound theoretical provocation: that robots can act as second-order moral agents, not by executing decisions, but by modulating the affective and normative infrastructure in which those decisions are made. This reframes the classical position in Machine Ethics—namely, that robots are not moral agents because they lack sentience and autonomy (Floridi \& Sanders, 2004)—as ontologically incomplete.

If the consequential structure of a decision changes due to robotic presence, then even morally neutral robots can become moral catalysts or moral occluders, depending on their semiotic and cognitive profile (Coeckelbergh, 2010; Nyholm, 2020; Gunkel, 2012). This renders inert robotic co-presence a potentially ethically non-neutral design decision in socially situated environments.

To put the matter plainly: designing robots without accounting for their ambient moral influence is epistemically reckless, and risks producing environments in which moral reasoning is systematically deflected or weakened.

